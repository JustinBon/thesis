{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "inner-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from spacy import displacy\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from spacy.tokens import Span, DocBin\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-interpretation",
   "metadata": {},
   "source": [
    "This notebook supplies the spacy ner model with more custom training data. This data is manually labeled by running the showText function. \n",
    "\n",
    "To see if this works better than the rule based matcher, I randomly selected 500 pages and labeled every mention of a ministery. The showTokens function prints out the tokens with its index above to make this a bit easier seeing that spacy requeres a Span of tokens to be added. The span consists of a doc (the current page), a start and end index, and a label (ministery). ShowTokens can also highlight certain words, in this case ministerie, ministeries and all of the abbriviations of all ministeries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "embedded-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinisteries():\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "\n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    return wikis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unusual-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "\n",
    "def showTokens(text, allMinisteries):\n",
    "\n",
    "    try:\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "    except:\n",
    "        pass\n",
    "    doc = nlp(str(text))\n",
    "    # get list of entities as strings\n",
    "    listTokens = [str(ent.text) for ent in doc.ents]\n",
    "\n",
    "    # control variables\n",
    "    nToken = 0\n",
    "    tempTokens = []\n",
    "    lenTemp = 0\n",
    "\n",
    "    # loop over tokens\n",
    "    for i in range(len(doc)):\n",
    "\n",
    "        # get string representation of token and add to a temporary list with token index\n",
    "        t = str(doc[i])\n",
    "        tempTokens.append((t, nToken))\n",
    "\n",
    "        # increase n tokens and length of all strings in tempTokens\n",
    "        nToken += 1\n",
    "        lenTemp += (len(t) + 1)\n",
    "\n",
    "        # once lenght of all strings in tempTokens is 100 or larger, start printing the line\n",
    "        if lenTemp >= 100 or i >= len(doc) - 1:\n",
    "\n",
    "            # print the index for every token\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # if index number is more chars than the token, dont print the index number\n",
    "                if len(word[0]) < len(str(word[1])):\n",
    "                    print(' ' * (len(word[0]) + 1), end='')\n",
    "\n",
    "                # else just print index number plus a number of spaces\n",
    "                else:\n",
    "                    print(word[1], ' ' * (len(word[0]) - len(str(word[1]))), end='')\n",
    "            print('')\n",
    "\n",
    "            # print all tokens\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # highlight tokens in red if named entity\n",
    "                if word[0].lower() in allMinisteries + ['ministerie', 'ministeries']:\n",
    "                    printHilight(word[0])\n",
    "                else:\n",
    "                    print(word[0] + ' ', end='')\n",
    "\n",
    "            # add some space between lines\n",
    "            print('\\n\\n')\n",
    "\n",
    "            # reset control variables\n",
    "            tempTokens = []\n",
    "            lenTemp = 0\n",
    "    \n",
    "    # inputs:\n",
    "    # no input: next page\n",
    "    # q: quit \n",
    "    # remove: remove last label\n",
    "    # number1 number2: adds a label from span number1 to number2\n",
    "    # everything else is invalid, you will be prompted again\n",
    "    \n",
    "    \n",
    "    newEnts = []\n",
    "    while True:\n",
    "        x = input()\n",
    "        if x == '':\n",
    "            break\n",
    "        elif x == 'q':\n",
    "            raise Exception(\"Stopped the program\")\n",
    "        elif x == 'remove':\n",
    "            del newEnts[-1]\n",
    "            print('Removed')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x = x.split()\n",
    "            newEnts.append(Span(doc, int(x[0]), int(x[1]), label='MINISTERIE'))\n",
    "            print(doc[int(x[0]):int(x[1])])\n",
    "        except:\n",
    "            print('Two numbers seperated by a space')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        doc.ents = newEnts\n",
    "        clear_output()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "familiar-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showText(df, n):\n",
    "    \n",
    "    ministeries = getMinisteries()\n",
    "    abrr = [ministeries[x]['Abbriviation'] for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    allMinisteries = list(ministeries.keys()) + abrr\n",
    "    allMinisteries = [x.lower() for x in allMinisteries]\n",
    "    \n",
    "    samples = df.sample(n)\n",
    "    docs = list(samples.text.apply(lambda x: showTokens(x, allMinisteries)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "viral-cursor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 300\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for i in range(30):\n",
    "    docs += showText(df, 10)\n",
    "    print(f'done {i + 1}0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fifteen-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\train2.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\dev2.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "blank-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this in folder with the test and train data files\n",
    "\n",
    "# generate default confic file   \n",
    "# $ python -m spacy init config ./config.cfg --lang nl --pipeline ner\n",
    "\n",
    "# training \n",
    "# python -m spacy train ./config.cfg --output ./output --paths.train train2.spacy --paths.dev dev2.spacy\n",
    "\n",
    "nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developing-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledData():\n",
    "    with open('..\\\\data\\\\ministeries.txt', 'r', encoding='utf-8') as f:\n",
    "        m = f.read()\n",
    "        m = m.split('\\n\\n')\n",
    "        \n",
    "    labeledMinisteries = {}\n",
    "    for file in m:\n",
    "        lines = file.split('\\n')\n",
    "        \n",
    "        labeledMinisteries[lines[0]] = {}\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            line = line.lower()\n",
    "            if line in labeledMinisteries[lines[0]]:\n",
    "                labeledMinisteries[lines[0]][line] += 1\n",
    "            else:\n",
    "                labeledMinisteries[lines[0]][line] = 1\n",
    "\n",
    "\n",
    "    return labeledMinisteries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "elegant-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMinisteries(df, files):\n",
    "    nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "    \n",
    "    found = {}\n",
    "    \n",
    "    for file in files:\n",
    "        found[file] = {}\n",
    "        \n",
    "        with open('..\\\\data\\\\covid wob text without ocr\\\\' + file +'.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(' +', ' ', text)\n",
    "        \n",
    "        doc = nlp1(text)\n",
    "        for ent in doc.ents:\n",
    "            ent = str(ent).lower()\n",
    "            ent = ent.replace('\\n', ' ')\n",
    "            \n",
    "            if 'ministerie van ' in ent:\n",
    "                ent = ent[15:]\n",
    "            \n",
    "            if ent in found[file]:\n",
    "                found[file][ent] += 1\n",
    "            else:\n",
    "                found[file][ent] = 1\n",
    "        \n",
    "    \n",
    "    return found\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "considered-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = extractMinisteries(df, getLabeledData().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "blind-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df, found=None):\n",
    "    labeled = getLabeledData()\n",
    "    \n",
    "    if not found:\n",
    "        found = extractMinisteries(df, labeled.keys())\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    fpList = []\n",
    "    fnList = []\n",
    "    \n",
    "    \n",
    "    for file in found:\n",
    "        \n",
    "        for ministerie in found[file]:\n",
    "            if ministerie not in labeled[file]:\n",
    "                fp += found[file][ministerie]\n",
    "                fpList.append(ministerie)\n",
    "                \n",
    "            elif found[file][ministerie] == labeled[file][ministerie]:\n",
    "                tp += found[file][ministerie]\n",
    "                \n",
    "            elif found[file][ministerie] > labeled[file][ministerie]:\n",
    "                tp += labeled[file][ministerie]\n",
    "                fp += found[file][ministerie] - labeled[file][ministerie]\n",
    "                fpList.append(ministerie)\n",
    "                \n",
    "            elif found[file][ministerie] < labeled[file][ministerie]:\n",
    "                tp += found[file][ministerie]\n",
    "                fn += labeled[file][ministerie] - found[file][ministerie]\n",
    "                fnList.append(ministerie)\n",
    "    \n",
    "        for ministerie in labeled[file]:\n",
    "            if ministerie not in found:\n",
    "                fn += labeled[file][ministerie]\n",
    "                fnList.append(ministerie)\n",
    "                \n",
    "    \n",
    "    for file in found.keys():\n",
    "        print(file)\n",
    "        pprint.pprint(found[file])\n",
    "        print('')\n",
    "        pprint.pprint(labeled[file])\n",
    "        print('')\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "    print(tp, fp, fn)\n",
    "    print('recall', tp / (tp + fn))\n",
    "    print('precision', tp / (tp + fp))\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "decreased-submission",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0068ed0b40cca6270f857d2614cc63c0_besluit.pdf\n",
      "{'financiën': 1, 'szw': 1, 'vws': 2}\n",
      "\n",
      "{'economische zaken en klimaat': 1,\n",
      " 'ezk': 1,\n",
      " 'financiën': 1,\n",
      " 'ienw': 1,\n",
      " 'infrastructuur en w aterstaat': 1,\n",
      " 'sociale zaken en werkgelegenheid ': 1,\n",
      " 'szw': 1,\n",
      " 'volksgezondheid, welzijn en sport': 1,\n",
      " 'vws': 1}\n",
      "\n",
      "\n",
      "\n",
      "0068ed0b40cca6270f857d2614cc63c0_document.pdf\n",
      "{'financiën': 1}\n",
      "\n",
      "{'financiën': 1}\n",
      "\n",
      "\n",
      "\n",
      "0335b3f498dbbd7c537ad23abe8c08dc_deelbesluit-1-wob-verzoek-dd-11-augustus-2021-inzake-het-europees-herstelfonds.pdf\n",
      "{'defensie': 1}\n",
      "\n",
      "{'buitenlandse zaken': 2, 'economische zaken en klimaat': 1, 'financiën': 2}\n",
      "\n",
      "\n",
      "\n",
      "07e2b274045cb5b4f54371a3c905cae9_wobverzoek-mccb-catshuis.pdf\n",
      "{'az': 1,\n",
      " 'bz': 1,\n",
      " 'bzk': 5,\n",
      " 'ezk': 5,\n",
      " 'fin': 4,\n",
      " 'i&w': 1,\n",
      " 'j&v': 1,\n",
      " 'jenv': 2,\n",
      " 'justitie en veiligheid': 1,\n",
      " 'ocw': 8,\n",
      " 'sociale zaken en werkgelegenheiddirectie': 1,\n",
      " 'szw': 46,\n",
      " 'vws': 17}\n",
      "\n",
      "{'ezk': 1,\n",
      " 'justitie en veiligheid': 1,\n",
      " 'sociale zaken en werkgelegenheid': 6,\n",
      " 'szw': 1}\n",
      "\n",
      "\n",
      "\n",
      "17967f10340f6de2a79ba984209b4a2c_besluit.pdf\n",
      "{'financiën': 1, 'volksgezondheid': 1, 'vws': 3}\n",
      "\n",
      "{'financiën': 1, 'volksgezondheid, welzijn en sport': 1, 'vws': 1}\n",
      "\n",
      "\n",
      "\n",
      "2c4079ccaad78e8d2cb494681ae79928_stukken-bij-besluit-wob-verzoek-notities-besluitvorming-coronacrisis-20.pdf\n",
      "{'-vws': 1,\n",
      " 'az': 1,\n",
      " 'bz': 4,\n",
      " 'bzk': 4,\n",
      " 'def': 1,\n",
      " 'ezk': 1,\n",
      " 'fin': 2,\n",
      " 'financiën': 1,\n",
      " 'i&w': 2,\n",
      " 'jenv': 2,\n",
      " 'lnv': 4,\n",
      " 'ocw': 3,\n",
      " 'szw': 4,\n",
      " 'vws': 25}\n",
      "\n",
      "{'financiën': 1}\n",
      "\n",
      "\n",
      "\n",
      "3ee644b02189ec45794dd998ac5da5b5_besluit.pdf\n",
      "{'financiën': 2}\n",
      "\n",
      "{'financiën': 2}\n",
      "\n",
      "\n",
      "\n",
      "40f5564f839324b9af20c295dd261007_inventarislijst-eerste-deelbesluit.pdf\n",
      "{'algemene zaken': 1,\n",
      " 'az': 3,\n",
      " 'binnenlandse zaken': 1,\n",
      " 'bzk': 11,\n",
      " 'economische zaken': 1,\n",
      " 'ezk': 1,\n",
      " 'ienw': 1,\n",
      " 'infrastructuur en waterstaat': 1,\n",
      " 'jenv': 2,\n",
      " 'justitie en veiligheid': 1,\n",
      " 'minezk': 2,\n",
      " 'volksgezondheid': 1,\n",
      " 'vws': 1}\n",
      "\n",
      "{'algemene zaken': 1,\n",
      " 'binnenlandse zaken en koninkrijksrelaties': 1,\n",
      " 'economische zaken': 1,\n",
      " 'infrastructuur en waterstaat': 1,\n",
      " 'justitie en veiligheid': 1,\n",
      " 'volksgezondheid, welzijn en sport': 1}\n",
      "\n",
      "\n",
      "\n",
      "41fedde7ea03bdd9ac5cd92c7f7cfd43_documenten.pdf\n",
      "{'justitie en veiligheid': 1}\n",
      "\n",
      "{'justitie en veiligheid': 1}\n",
      "\n",
      "\n",
      "\n",
      "439285231fa523a74c430da4a2704fab_deels-openbare-documenten.pdf\n",
      "{'bzk': 4,\n",
      " 'i&w': 4,\n",
      " 'infrastructuur en waterstaat': 5,\n",
      " 'infrastructuur en waterstaatrijnstraat': 3,\n",
      " 'jenv': 8,\n",
      " 'justitie en veiligheid': 18,\n",
      " 'justitie en veiligheiddirectie wetgeving en juridische zaken sector juridische zaken': 1,\n",
      " 'vws': 27}\n",
      "\n",
      "{'bzk': 2,\n",
      " 'infrastructuur en waterstaat': 20,\n",
      " 'jenv': 2,\n",
      " 'justitie en veiligheid': 19,\n",
      " 'vws': 1}\n",
      "\n",
      "\n",
      "\n",
      "4991dc6aed6e1369f58e6bc1996d5e2d_paginas-van-samengevoegd-document-850-paginas-met-uitgestelde-verstrekking-deel-1.pdf\n",
      "{'economische zakenen klimaat. ik zou': 1}\n",
      "\n",
      "{'economische zaken en klimaat': 2,\n",
      " 'economische zakenen klimaat': 1,\n",
      " 'ezk': 2,\n",
      " 'financiën': 2,\n",
      " 'sociale zaken en  werkgelegenheid': 2}\n",
      "\n",
      "\n",
      "\n",
      "4991dc6aed6e1369f58e6bc1996d5e2d_paginas-van-samengevoegd-document-850-paginas-met-uitgestelde-verstrekking-deel-2.pdf\n",
      "{'buitenlandse zaken': 1, 'economische zaken': 1, 'i&w': 1, 'landbouw': 2}\n",
      "\n",
      "{'buitenlandse zaken': 1,\n",
      " 'economische zaken en klimaat': 10,\n",
      " 'ezk': 1,\n",
      " 'financiën': 1,\n",
      " 'landbouw,  natuur en  voedselkwaliteit': 1,\n",
      " 'landbouw, natuur en voedselkwaliteit': 1,\n",
      " 'szw': 1}\n",
      "\n",
      "\n",
      "\n",
      "4991dc6aed6e1369f58e6bc1996d5e2d_paginas-van-samengevoegd-document-850-paginas-met-uitgestelde-verstrekking-deel-3.pdf\n",
      "{}\n",
      "\n",
      "{'economische zaken en klimaat': 4}\n",
      "\n",
      "\n",
      "\n",
      "4991dc6aed6e1369f58e6bc1996d5e2d_paginas-van-samengevoegd-document-850-paginas-met-uitgestelde-verstrekking-deel-4.pdf\n",
      "{'economische zaken': 1,\n",
      " 'ezk zou hoeven te geschieden , maar dat u': 1,\n",
      " 'infrastructuur en waterstaat': 3,\n",
      " 'jenv': 1,\n",
      " 'ministerie lnv': 1,\n",
      " 'vws': 3}\n",
      "\n",
      "{'economische zaken en klimaat': 2,\n",
      " 'ezk': 2,\n",
      " 'infrastructuur en waterstaat': 2,\n",
      " 'lnv': 1}\n",
      "\n",
      "\n",
      "\n",
      "48 227 133\n",
      "recall 0.26519337016574585\n",
      "precision 0.17454545454545456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(df, found = found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-necessity",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hmmm these results are a bit weird. The calculated recall and precision are thrash but looking at the results in the output of the cell above is a bit more promising (the first dict after the file name is the results of the model, the second is the labeled data). There are definitely some big mistakes in the updated spacy model but it also found a lot more than I actually labeled (shows how good my labeling skills are...). \n",
    "\n",
    "The thing that the model does well compared to my rule based matcher is when there is a list of ministries like \"de ministeries van ezk, vws en szw\". These abbriviations are also caught a lot more with this model than with the rule based matcher. Take a look at the results for the file \"stukken-bij-besluit-wob-verzoek-notities-besluitvorming-coronacrisis-20.pdf\" for a good example of this.\n",
    "\n",
    "Then the things that the model doesn't do well. Sometimes it recognizes a ministery as \"ministerie van binnenlandse zaken\" in stead of \"ministerie van binnenlandse zaken en koninkrijksrelaties\". It doesn't take extract the whole name of the ministery. A second place where it messes up is when it does the oppisite. In the last file in the cell above, it extracts \"ezk zou hoeven te geschieden , maar dat u\" while it only should have gotten \"ezk\".\n",
    "\n",
    "In short, what the model finds seems to be mostly correct albeit not completely correct. \n",
    "\n",
    "I did not run the model on the ocr'ed text because the labeled data came from the text that was extracted using pypdf2. I did this to keep everything consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
