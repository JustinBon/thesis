{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developed-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from spacy import displacy\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from spacy.tokens import Span, DocBin\n",
    "import random\n",
    "\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-window",
   "metadata": {},
   "source": [
    "This notebook supplies the spacy ner model with more custom training data. This data is manually labeled by running the showText function. \n",
    "\n",
    "To see if this works better than the rule based matcher, I randomly selected 500 pages and labeled every mention of a ministery. The showTokens function prints out the tokens with its index above to make this a bit easier seeing that spacy requeres a Span of tokens to be added. The span consists of a doc (the current page), a start and end index, and a label (ministery). ShowTokens can also highlight certain words, in this case ministerie, ministeries and all of the abbriviations of all ministeries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romance-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinisteries():\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "\n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    return wikis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stone-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "\n",
    "def showTokens(text, allMinisteries):\n",
    "\n",
    "    try:\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "    except:\n",
    "        pass\n",
    "    doc = nlp(str(text))\n",
    "    # get list of entities as strings\n",
    "    listTokens = [str(ent.text) for ent in doc.ents]\n",
    "\n",
    "    # control variables\n",
    "    nToken = 0\n",
    "    tempTokens = []\n",
    "    lenTemp = 0\n",
    "\n",
    "    # loop over tokens\n",
    "    for i in range(len(doc)):\n",
    "\n",
    "        # get string representation of token and add to a temporary list with token index\n",
    "        t = str(doc[i])\n",
    "        tempTokens.append((t, nToken))\n",
    "\n",
    "        # increase n tokens and length of all strings in tempTokens\n",
    "        nToken += 1\n",
    "        lenTemp += (len(t) + 1)\n",
    "\n",
    "        # once lenght of all strings in tempTokens is 100 or larger, start printing the line\n",
    "        if lenTemp >= 100 or i >= len(doc) - 1:\n",
    "\n",
    "            # print the index for every token\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # if index number is more chars than the token, dont print the index number\n",
    "                if len(word[0]) < len(str(word[1])):\n",
    "                    print(' ' * (len(word[0]) + 1), end='')\n",
    "\n",
    "                # else just print index number plus a number of spaces\n",
    "                else:\n",
    "                    print(word[1], ' ' * (len(word[0]) - len(str(word[1]))), end='')\n",
    "            print('')\n",
    "\n",
    "            # print all tokens\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # highlight tokens in red if named entity\n",
    "                if word[0].lower() in allMinisteries + ['ministerie', 'ministeries']:\n",
    "                    printHilight(word[0])\n",
    "                else:\n",
    "                    print(word[0] + ' ', end='')\n",
    "\n",
    "            # add some space between lines\n",
    "            print('\\n\\n')\n",
    "\n",
    "            # reset control variables\n",
    "            tempTokens = []\n",
    "            lenTemp = 0\n",
    "    \n",
    "    # inputs:\n",
    "    # no input: next page\n",
    "    # q: quit \n",
    "    # remove: remove last label\n",
    "    # number1 number2: adds a label from span number1 to number2\n",
    "    # everything else is invalid, you will be prompted again\n",
    "    \n",
    "    \n",
    "    newEnts = []\n",
    "    while True:\n",
    "        x = input()\n",
    "        if x == '':\n",
    "            break\n",
    "        elif x == 'q':\n",
    "            raise Exception(\"Stopped the program\")\n",
    "        elif x == 'remove':\n",
    "            del newEnts[-1]\n",
    "            print('Removed')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x = x.split()\n",
    "            newEnts.append(Span(doc, int(x[0]), int(x[1]), label='MINISTERIE'))\n",
    "            print(doc[int(x[0]):int(x[1])])\n",
    "        except:\n",
    "            print('Two numbers seperated by a space')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        doc.ents = newEnts\n",
    "        clear_output()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "catholic-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showText(df, n):\n",
    "    \n",
    "    ministeries = getMinisteries()\n",
    "    abrr = [ministeries[x]['Abbriviation'] for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    allMinisteries = list(ministeries.keys()) + abrr\n",
    "    allMinisteries = [x.lower() for x in allMinisteries]\n",
    "    \n",
    "    samples = df.sample(n)\n",
    "    docs = list(samples.text.apply(lambda x: showTokens(x, allMinisteries)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "functioning-major",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 300\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for i in range(30):\n",
    "    docs += showText(df, 10)\n",
    "    print(f'done {i + 1}0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "advanced-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\train2.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\dev2.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "urban-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this in folder with the test and train data files\n",
    "\n",
    "# generate default confic file   \n",
    "# $ python -m spacy init config ./config.cfg --lang nl --pipeline ner\n",
    "\n",
    "# training \n",
    "# python -m spacy train ./config.cfg --output ./output --paths.train train2.spacy --paths.dev dev2.spacy\n",
    "\n",
    "nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "gross-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate():\n",
    "    nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
