{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separated-basement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "C:\\Users\\justin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forward-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ministries = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-musical",
   "metadata": {},
   "source": [
    "# dates\n",
    "This first part will be the evalutation for the base dates extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "       'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "sent = ['datum', 'verzonden', 'sent', 'date', 'received']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    \n",
    "def showMatches(doc, matches, regexMatches):   \n",
    "    indexOfMatches = []\n",
    "    for matchid, start, end in matches:\n",
    "        for i in range(start, end):\n",
    "            indexOfMatches.append(i)\n",
    "            \n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "        else:\n",
    "            if token.text in regexMatches:\n",
    "                printHilight(str(token.text))\n",
    "            else:\n",
    "                print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "golden-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dates, sep, pat):\n",
    "    goodDates = []\n",
    "    for date in dates:\n",
    "        date = date.replace(' ', '')\n",
    "        try:\n",
    "            date = date.replace(sep, ' ')\n",
    "            datetime.strptime(date, pat)\n",
    "            goodDates.append(date.replace(' ', sep))\n",
    "        except:\n",
    "            try:\n",
    "                if len(date.split(' ')) == 3 and len(date.split(' ')[2]) == 2:\n",
    "                    datetime.strptime(date, '%d %m %y')\n",
    "                    goodDates.append(date.replace(' ', sep))\n",
    "            except:\n",
    "                pass     \n",
    "    return goodDates\n",
    "            \n",
    "\n",
    "def regexMatcher(text):\n",
    "    results = []\n",
    "    \n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]', text), '/', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]\\/[0-9]{2,4}', text), '/', '%d %m %Y')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]', text), '-', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]-[0-9]{2,4}', text), '-', '%d %m %Y')\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acoustic-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'exit':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, exit or nothing\")\n",
    "        \n",
    "\n",
    "def evaluate(nlp):\n",
    "    \n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "           'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
    "           'ma', 'di', 'wo', 'woe', 'do', 'vrij', 'za', 'zat', 'zo', 'vr']\n",
    "    \n",
    "    datesPattern = [{\"LOWER\" : {\"IN\" : days}, \"OP\" : \"?\"}, \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True, \"OP\" : \"?\"}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "    \n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that isnt a mactch\n",
    "    if os.path.isfile('..\\\\data\\\\results\\\\dates_results,json'):\n",
    "        with open('..\\\\data\\\\results\\\\dates_results,json') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'america' :0,\n",
    "            'ocr':0,\n",
    "            'oneoff':0,\n",
    "            'other':0\n",
    "        }\n",
    "    \n",
    "    print(results)\n",
    "    cases = ['correct','incorrect','missing','spurious','america','ocr','oneoff','other']\n",
    "    documents = 0\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            print(sum([results[x] for x in results.keys() if x in cases[:4]]), results['documents'])\n",
    "            sample = df.sample(1)\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            doc = nlp(text)\n",
    "            matches = matcher(doc)\n",
    "\n",
    "            regexMatches = regexMatcher(text)\n",
    "\n",
    "            showMatches(doc, matches, regexMatches)\n",
    "\n",
    "            for case in cases:\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    results['documents'] += documents\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "            documents += 1\n",
    "            clear_output()\n",
    "        except:\n",
    "            print('error')\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "center-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResults(r):\n",
    "    precision = r['correct'] / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = r['correct'] / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quantitative-decimal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 227\n",
      "d8d9c5015c9ceb952052f29e1a27ed1f_openbaar-te-maken-documenten-deel-2 108 \n",
      "\n",
      "\n",
      "Departementaal vertrouwelijk \n",
      " Crisisteam \u001b[1;31m9 \u001b[0m\u001b[1;31mapril \u001b[0m\u001b[1;31m2020 \u001b[0m, 10:00 uur Notulist : Joyce Corver \n",
      " \n",
      " \n",
      " Akkoord : Johan Gro _ \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 35 . | Versoepeling \u001b[1;31m24/7 \u001b[0mbeleid aanwezigheid arts/verpleegkundige bij besmette gedetineerden c Afgerond \n",
      " < 70jr , \u001b[1;31m23-03-2020 \u001b[0m\n",
      " . | e e . O O hoeveel justitiabelen er Afgerond \n",
      " per plek aanwezig zijn , zodat het mogelijk wordt naar rato beschermingsmiddelen te \u001b[1;31m23-03-2020 \u001b[0m\n",
      " leveren . \n",
      " 37 . | e e t de procedure uit hoe het werkt met het verkrijgen van bijstand van Afgerond \n",
      " Defensie . \u001b[1;31m23-03-2020 \u001b[0m\n",
      " DGS&B doet de aanvraag . Er is een standaard format voor . \n",
      " 3 . | e t u t de factsheets die op intranet staan , t.b.v. justitiabelen naar Afgerond \n",
      " EE n n Zij u caten de factsheets . \u001b[1;31m23-03-2020 \u001b[0m\n",
      " \n",
      " \n",
      " 3 . | e e een voorstel voor het Beraad van komende Afgerond \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " maandag , \u001b[1;31m23 \u001b[0m\u001b[1;31mmaart \u001b[0m, over Penitentiaire programma en artikelplaatsingen ( extramuraal \u001b[1;31m23-03-2020 \u001b[0m\n",
      " verblijf } \n",
      " Beraad heeft gevraagd om dit voorstel in het Beleidsteam te brengen ( \u001b[1;31m23 \u001b[0m\u001b[1;31mmaart \u001b[0m) \n",
      " 40 . | Corporate Communicatie vertaalt de Q&A's voor personeel om te informeren over de Afgerond \n",
      " mogelijkheid om ander werk te moeten verrichten . Voorleggen ter arbeidsjuridische \u001b[1;31m23-03-2020 \u001b[0m\n",
      " toetsing . \n",
      " 41 . | B ontroleert of de ROAZ DJI-inrichtingen erkent als zorgaanbieder en meldt dit Afgerond \n",
      " voor overleg met de minister . \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 42 . De afgekondigde maatregelen gelden tot \u001b[1;31m6 \u001b[0m\u001b[1;31mapril \u001b[0m\u001b[1;31m. \u001b[0mCorporate Communicatie vermeldt dit in Afgerond \n",
      " de O&A's . \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 43 . | B controleert of de lijn die nu is uitgezet voor het vignet voor België ook geldt Afgerond \n",
      " om Nederland in te komen . ‚ \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 44 . DPMO maakt een voorstel over het uitbetalen van overwerk en het schuiven van verlof . Afgerond \n",
      " Het voorstel ligt ter besluit in het Beraad van \u001b[1;31m26 \u001b[0m\u001b[1;31mmaart \u001b[0m\u001b[1;31m. \u001b[0m\u001b[1;31m25-03-2020 \u001b[0m\n",
      " 45 . De DV&O werkt een nota uit over de extra te kopen enkelbanden en de systemen . Afgerond \n",
      " \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 46 . | DPMO heeft de Memo “ inzet personeel D ! naar inrichtingen/Covid- 19 \" afgerond . y Afgerond \n",
      " G zeeft akkoord . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 47 . Corporate Communicatie brengt de vragen van advocaten in beeld . Indien nodig worden Afgerond \n",
      " er Q8A's gemaakt . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " DIZ en CC schakelen over welke info.er is en wat gevraagd wordt . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 48 . e everi bij SSm een lijst aan , waarop staat waar de problemen met de Afgerond \n",
      " ROAZ zich voordoen . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " E neemt contact op met VWS . \n",
      " 49 . | GW meldt de inrichtingen dat de GGD gevraagd moet worden voor coronatests . Afgerond \n",
      " Morgen terugkoppeling of dit is gebeurd . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 50 . | Meeft door aan VWS dat de GGD alle DJI-inrichtingen gelijkstelt aan een Afgerond \n",
      " zorginstelling , in het licht van de coronatests . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 51 . | e evert tekst en getallen met de uitleg over de extra enkelbanden aan Afgerond \n",
      " _ S e behoeve van de minister . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 52 . S e emt contact op met de Reclassering ( S o te vragen of zij Afgerond \n",
      " meegaan in de ontwikkelingen van uitbreiden enkelbandmonitoring . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 53 . | Erworden Q&A's op internet gezet voor advocaten die vragen hebben over Afgerond \n",
      " detentieongeschiktheid \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 54 . | De brief van - \" de bonden m.b.t. het meerpersoonscelgebruik is in de Afgerond \n",
      " \n",
      " \n",
      " \n",
      " Pagina 5 van 7 \n",
      " correct1\n",
      "incorrectexit\n",
      "precision = 0.9506726457399103, recall = 0.8706365503080082, f1 = 0.9088960342979635\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(nlp)\n",
    "results = calcResults(results)\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\dates_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rapid-newport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 424,\n",
       " 'incorrect': 9,\n",
       " 'missing': 54,\n",
       " 'spurious': 13,\n",
       " 'america': 30,\n",
       " 'ocr': 19,\n",
       " 'oneoff': 12,\n",
       " 'other': 15,\n",
       " 'documents': 265,\n",
       " 'precision': 0.9506726457399103,\n",
       " 'recall': 0.8706365503080082,\n",
       " 'f1': 0.9088960342979635}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-bernard",
   "metadata": {},
   "source": [
    "removed all access newlines and spaces\n",
    "\n",
    "Part of the reason not all dates were caught was an oversight with american date formats. To validate the if a found match is actually a date, it needs a date format to compare the match to. Only the dd-mm-yyyy format was checked and not the american format of mm-dd-yyyy with the month before the year. This means it excludes dates like 04-14-2022 as this cannot be a date in the dd-mm-yyyy format because there obviously isn't a 14th month. In the american system this is just april 14th 2022.\n",
    "\n",
    "Another reason for not finding some dates is OCR mistakes. For example, in one case the date that was supposed to be found was \"5/12/2021\" but in the OCR process that string was read as \"542/2021\" where the \"/\" and \"1\" were seen as one character, a 4.\n",
    "\n",
    "Missed dates in file names were excluded from the evaluation because these are not the dates your looking for\n",
    "\n",
    "precision = 0.9197860962566845, recall = 0.8911917098445595, f1 = 0.9052631578947369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-torture",
   "metadata": {},
   "source": [
    "# Ministries\n",
    "This next part is the evalutation of the ministries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "radio-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinisteries():\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "\n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    minList = list(wikis.keys()) + [wikis[x]['Abbriviation'] for x in wikis.keys()]\n",
    "    for x in minList:\n",
    "        temp+=x.replace(',', '').split(' ')\n",
    "    \n",
    "    return [x.lower() for x in temp if x != 'en' and x != ''] + ['ezk']\n",
    "minList = getMinisteries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "valued-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "\n",
    "def printHilightUnderline(string):\n",
    "    print('\\033[4m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    \n",
    "def showMatchesMinistries(doc, minList):   \n",
    "    indexOfMatches = []\n",
    "    for ent in doc.ents:\n",
    "        for i in range(int(ent.start), int(ent.end)):\n",
    "            indexOfMatches.append(i)\n",
    "\n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "\n",
    "    for token in doc:\n",
    "        flag = False\n",
    "        for mini in minList:\n",
    "            if token.text.lower() == mini:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "            \n",
    "        elif flag:\n",
    "            printHilightUnderline(str(token.text))\n",
    "            \n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "simplified-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'q':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, exit or nothing\")\n",
    "\n",
    "def evaluateMinistries(nlp, minList):\n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that\n",
    "    # isnt a mactch, \n",
    "    if os.path.isfile('..\\\\data\\\\results\\\\ministries_results,json'):\n",
    "        with open('..\\\\data\\\\results\\\\ministries_results,json') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'partial':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'abbr' :0,\n",
    "            'ocr':0,\n",
    "            'whitespace':0,\n",
    "            'minister':0,\n",
    "            'other':0\n",
    "        }\n",
    "    \n",
    "    minList = set(minList)\n",
    "    while(True):\n",
    "        try:\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(sum(list(results.values())[:4] ))\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            showMatchesMinistries(doc, minList)\n",
    "\n",
    "            for case in list(results.keys())[:10]:\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "\n",
    "            clear_output()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "viral-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResultsMinistries(r):\n",
    "    precision = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daily-internet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "b752f2f8f1fc2db6fe146785b4b521a3_04-iao-mei 895 \n",
      "\n",
      "\n",
      "7 \n",
      " Ë \n",
      " Actuele onderzoeken ( T ) - stevige \u001b[4meconomische \u001b[0mkrimp verwacht \n",
      " Verschillende instanties hebben ramingen gegeven voor de \u001b[4meconomische \u001b[0mgroei . De \n",
      " omvang loopt uiteen , maar er is consensus dat de Nederlandse economie een \n",
      " stevige krimp staat te wachten in 2020 . \n",
      " Het CPB publiceerde in maart 4 scenario's . In het meest sombere scenario krimpt de \n",
      " economie in 2020 met 7,7% . De eerstvolgende raming van het CPB verschijnt op 16 juni . \n",
      " \n",
      " De Europese Commissie ( mei 2020 ) verwacht dat de Nederlandse economie dit jaar \n",
      " krimpt met 6,8% en in 2021 weer met 5% groeit . \n",
      " Het IMF ( april 2020 ) verwacht dat de NLse economie in 2020 circa 7,5% zal krimpen , en \n",
      " een groei van 3% in 2021 . \n",
      " De \u001b[4meconomische \u001b[0mbureaus van de Rabobank en ING gaan ook uit van een forse krimp in \n",
      " 2020 . ING verwacht een krimp van 6 tot 8 procent als het virus snel onder controle komt \n",
      " ( wat zeer onzeker is ) . Rabobank verwacht in hun basisscenario een krimp van 5 ® % . \n",
      " 17 \n",
      " 24-5-2020 \n",
      " correctq\n",
      "precision = 0.96875, recall = 0.6549295774647887, f1 = 0.7815126050420168\n",
      "{'correct': 178, 'incorrect': 0, 'partial': 16, 'missing': 106, 'spurious': 14, 'abbr': 18, 'ocr': 0, 'whitespace': 5, 'minister': 6, 'other': 96, 'precision': 0.96875, 'recall': 0.6549295774647887, 'f1': 0.7815126050420168}\n"
     ]
    }
   ],
   "source": [
    "results = evaluateMinistries(nlp_ministries, minList)\n",
    "\n",
    "results = calcResultsMinistries(results)\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\ministries_results,json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hollywood-pressure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wvc',\n",
       " 'algemene',\n",
       " 'zaken',\n",
       " 'binnenlandse',\n",
       " 'zaken',\n",
       " 'koninkrijksrelaties',\n",
       " 'buitenlandse',\n",
       " 'zaken',\n",
       " 'defensie',\n",
       " 'economische',\n",
       " 'zaken',\n",
       " 'klimaat',\n",
       " 'financiën',\n",
       " 'infrastructuur',\n",
       " 'waterstaat',\n",
       " 'justitie',\n",
       " 'veiligheid',\n",
       " 'landbouw',\n",
       " 'natuur',\n",
       " 'voedselkwaliteit',\n",
       " 'onderwijs',\n",
       " 'cultuur',\n",
       " 'wetenschap',\n",
       " 'sociale',\n",
       " 'zaken',\n",
       " 'werkgelegenheid',\n",
       " 'volksgezondheid',\n",
       " 'welzijn',\n",
       " 'sport',\n",
       " 'az',\n",
       " 'bzk',\n",
       " 'bz',\n",
       " 'def',\n",
       " 'ez',\n",
       " 'fin',\n",
       " 'i&w',\n",
       " 'j&v',\n",
       " 'lnv',\n",
       " 'ocw',\n",
       " 'szw',\n",
       " 'vws',\n",
       " 'ezk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-jamaica",
   "metadata": {},
   "source": [
    "Missing things in situations like this: RVO/LNV. LNV should be have been caught here. This might be caused by the fact that spacy works with tokens/words in stead of individual letters. Places with a lot of extra newlines or spaces within the name of a ministry will also trip up the model. No context also doesnt help\n",
    "\n",
    "when training I specificly excluded the minister variants of the ministries. Therefore these aren't counted as missing if the model doesn't find them but will be counted as spurious if the the model does find them.\n",
    "\n",
    "The data the model was trained on means that it easily recoginizes ministries that have a lot to do with covid like volksgezondheid, but less so for others like landbouw. EZK (economics and climate) specifically doesn't want to be captured\n",
    "\n",
    "The type of text doesnt lend itself very well to NER. A lot of times the abbriviations of the ministries are listed without any contex to what it is. NER uses information about the surrounding words to predict what the word is. If there are no surrounding words, no context, predictions are going to be difficult\n",
    "\n",
    "ministerie SZW en EZK was counted as two partially correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "worldwide-microphone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3\n",
    "x += -4\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-edinburgh",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n",
    "This next part is the spacy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incredible-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGroundTruth():\n",
    "    with open('..\\\\data\\\\ner labeled data\\\\test.conllu', 'r', encoding='utf8') as f:\n",
    "        ground = f.read()\n",
    "        ground = ground.split('\\n')\n",
    "        ground = [x.split('\\t') for x in ground]\n",
    "    \n",
    "    text = []\n",
    "    for word in ground:\n",
    "        try:\n",
    "            text.append(word[1])\n",
    "        except IndexError:\n",
    "            text.append('\\n')\n",
    "    \n",
    "\n",
    "    text = ' '.join(text)\n",
    "    return text, ground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "provincial-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function makes sure that an index refers to the same token in both the ground truth and the doc\n",
    "# its not a robust approach but it works\n",
    "def sync(ground, doc):\n",
    "    \n",
    "    # newlines in ground truth are given as an empty string in a list\n",
    "    # spacy doesnt do tokens for newlines so these can be removed\n",
    "    ground = [x for x in ground if x != ['']]\n",
    "    \n",
    "    groundIndex, docIndex = 0, 0\n",
    "    docNew, groundNew = [], []\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex+2 > len(ground) or docIndex+2 > len(doc):\n",
    "            return groundNew, docNew\n",
    "        \n",
    "        \n",
    "        if str(doc[docIndex].text) != ground[groundIndex][1]:\n",
    "            # see if next token in ground truth equals current doc token\n",
    "            if str(doc[docIndex].text) == ground[groundIndex + 1][1]:\n",
    "                groundIndex +=1\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex + 2][1]:\n",
    "                groundIndex +=2\n",
    "\n",
    "            # see if next token in doc equals current ground truth token\n",
    "            elif str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=1\n",
    "            elif str(doc[docIndex + 2].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=2\n",
    "\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex += 2\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex ][1] + ground[groundIndex + 1][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 2\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex+1].text)+ str(doc[docIndex+2].text) == ground[groundIndex][1]:\n",
    "                docIndex += 3\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex][1] + ground[groundIndex+1][1] + ground[groundIndex+2][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 3\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(str(doc[docIndex].text), str(doc[docIndex+1].text),str(doc[docIndex+2].text)) \n",
    "                print(ground[groundIndex][1],ground[groundIndex+1][1],ground[groundIndex+2][1])\n",
    "                return groundNew, docNew\n",
    "        \n",
    "        # add good tokens to new lists\n",
    "        groundNew.append((ground[groundIndex][1], ground[groundIndex][2]))\n",
    "        docNew.append((str(doc[docIndex].text), doc[docIndex].ent_type_))\n",
    "            \n",
    "        groundIndex += 1\n",
    "        docIndex += 1\n",
    "\n",
    "def testSync(ground, doc):\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i][0] != ground[i][0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pediatric-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets the span of all entities in ground truth\n",
    "def getSpans(ground):\n",
    "    span = []\n",
    "    i = 0\n",
    "    flag = False\n",
    "    begin = 0\n",
    "    \n",
    "    while i < len(ground):\n",
    "        \n",
    "        # base case: no continuation of entiy or start of entity\n",
    "        if ground[i][1] == 'O' and flag == False:\n",
    "            pass\n",
    "        \n",
    "        # end of entity span no new entity, reset flag and add entity\n",
    "        elif ground[i][1] == 'O' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            flag = False\n",
    "        \n",
    "        # end of entity span, new entity starts. Reset flag, add entity, and start new entity\n",
    "        elif ground[i][1][0] == 'B' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            begin = i\n",
    "        \n",
    "        # start of new entity, set flag and begin\n",
    "        elif ground[i][1][0] == 'B' and flag == False:\n",
    "            flag = True\n",
    "            begin = i\n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "continued-saturn",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "def savePredictions(predictions):\n",
    "    begin = [x[0][0] for x in predictions]\n",
    "    end = [x[0][1] for x in predictions]\n",
    "    entType = [x[1] for x in predictions]\n",
    "    ent = [x[2] for x in predictions]\n",
    "    pred = pd.DataFrame.from_dict({'begin':begin, 'end':end, 'entType':entType, 'ent':ent})\n",
    "    pred.to_csv('..\\\\data\\\\predictions.csv')\n",
    "\n",
    "# get entities found by spacy in correct format\n",
    "def getPredictionSpans(doc, tokens):\n",
    "    spans = []\n",
    "    \n",
    "    i = 0\n",
    "    entityIndex = 0\n",
    "    \n",
    "    # check for every entity\n",
    "    while i < len(tokens):\n",
    "        try:\n",
    "            \n",
    "            # if current token has an entity label\n",
    "            if tokens[i][1] != '':\n",
    "                \n",
    "                # get entity type and string representation\n",
    "                entity = doc.ents[entityIndex]\n",
    "                entityType = doc[entity.start].ent_type_\n",
    "                entity = str(entity)\n",
    "                \n",
    "                # get number of tokens in entity and add to list\n",
    "                nTokens = len(entity.split(' '))\n",
    "                spans.append(((i, i + nTokens), entityType, entity))\n",
    "                \n",
    "                # increase token index by number of tokens in current entity\n",
    "                i += nTokens\n",
    "                \n",
    "                # set entity index to next\n",
    "                entityIndex += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "        except IndexError:\n",
    "            savePredictions(spans)\n",
    "            return\n",
    "        \n",
    "    \n",
    "    savePredictions(spans)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bizarre-ozone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load predictions from csv\n",
    "def getPredictions():\n",
    "    predictions = pd.read_csv('..\\\\data\\\\predictions.csv')\n",
    "    begin = list(predictions.begin)\n",
    "    end = list(predictions.end)\n",
    "    entType = list(predictions.entType)\n",
    "    ent = list(predictions.ent)\n",
    "    \n",
    "    predictionsList = []\n",
    "    for i in range(len(begin)):\n",
    "        predictionsList.append(((begin[i], end[i]), entType[i], ent[i]))\n",
    "        \n",
    "    return predictionsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "loved-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = full text as string\n",
    "# groundOld = tokens with ground truth labels\n",
    "# doc = spacy doc of full text\n",
    "# ground = list of tokens with ground truth labels in sync with docList\n",
    "# docList = list of tokens from spacy with predicted labels in sync with ground\n",
    "# predictions = list of predicted entities by spacy with begin, end, type and text\n",
    "# span = list of ground truth entites with begin, end, type\n",
    "\n",
    "\n",
    "text, groundOld = loadGroundTruth()\n",
    "doc = nlp(text[:1000000])\n",
    "ground, docList = sync(groundOld, doc)\n",
    "predictions = getPredictions()\n",
    "span = getSpans(ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "specialized-plymouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170685"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groundOld)\n",
    "len(docList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-combination",
   "metadata": {},
   "source": [
    "## SpaCy eval normal (Doenst work, is not used)\n",
    "also very bad and doesnt even do what it is supposed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fewer-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1(tp, tn, fp, fn):\n",
    "    print(tp, tn, fp, fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "    return recall, precision, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "swedish-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNER(ground, doc):\n",
    "    \n",
    "    groundIndex, docIndex = 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    nerCats = ['FAC', 'PERSON', 'ORG', 'GPE', 'LOC']\n",
    "    groundCats = ['ORG', 'LOC', 'PER']\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            if str(doc[docIndex].text) != ground[groundIndex][1]:\n",
    "                \n",
    "                # checks for if the words are still in sync\n",
    "                if str(doc[docIndex + 1].text) != ground[groundIndex][1]:\n",
    "                    docIndex += 1\n",
    "                    continue\n",
    "                \n",
    "                elif str(doc[docIndex].text) != ground[groundIndex + 1][1]:\n",
    "                    groundIndex += 1\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    for j in range(docIndex-3,docIndex+3):\n",
    "                        print(doc[j], ground[j])\n",
    "                \n",
    "            else:\n",
    "                if doc[docIndex].ent_type_ == '' and ground[groundIndex][2] == 'O':\n",
    "                    tn += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ in nerCats and ground[groundIndex][2] == 'O':\n",
    "                    fp += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ in nerCats and ground[groundIndex][2] != 'O':\n",
    "                    tp += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ == '' and ground[groundIndex][2] != 'O':\n",
    "                    fn += 1\n",
    "        \n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        groundIndex +=1\n",
    "        docIndex +=1\n",
    "        \n",
    "        if groundIndex > len(ground) and docIndex > len(doc):\n",
    "            print('good')\n",
    "            return calcF1(tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "renewable-movie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "1240 10349 47 185\n",
      "recall 0.8701754385964913\n",
      "precision 0.9634809634809635\n",
      "f1 0.9144542772861357\n"
     ]
    }
   ],
   "source": [
    "recall, precision, f1 = evalNER(ground, doc)\n",
    "print('recall', recall)\n",
    "print('precision', precision)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-store",
   "metadata": {},
   "source": [
    "## SpaCy eval strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "beautiful-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1Strict(cor, inc, spu, mis):\n",
    "    print(cor,inc,spu,mis)\n",
    "    recall = cor / (cor+inc+mis)\n",
    "    precision = cor / (cor+inc+spu)\n",
    "    f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "    print('recall', recall)\n",
    "    print('precision', precision)\n",
    "    print('f1', f1)\n",
    "    results = {'total':cor + inc + spu + mis,\n",
    "              'correct':cor,\n",
    "              'incorrect':inc,\n",
    "              'missing':mis,\n",
    "              'spurious':spu,\n",
    "              'precision':precision,\n",
    "              'recall':recall,\n",
    "              'f1':f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ranking-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNER(ground, pred, method):\n",
    "    correct, incorrect, spurious, missing = 0, 0, 0, 0\n",
    "    \n",
    "    spacyBanList = ['CARDINAL','DATE','LAW','MONEY','ORDINAL','PERCENT','QUANTITY','TIME']\n",
    "    cats = {'B-ORG': ['ORG'],\n",
    "           'B-PER': ['PERSON'],\n",
    "           'B-LOC': ['FAC', 'GPE', 'LOC'],\n",
    "           'B-ORG': ['ORG']}\n",
    "    \n",
    "    groundIndex = 0\n",
    "    predIndex = 0\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex >= len(ground) or predIndex >= len(pred):\n",
    "            return calcF1Strict(correct, incorrect, spurious, missing)\n",
    "        \n",
    "        # set current tokens \n",
    "        groundEnt = ground[groundIndex]\n",
    "        predEnt = pred[predIndex]\n",
    "        \n",
    "        # correct span\n",
    "        if groundEnt[0] == predEnt[0]:\n",
    "            if method == 'exact':\n",
    "                correct += 1\n",
    "            \n",
    "            else:\n",
    "                # correct type\n",
    "                if groundEnt[1] == 'B-MISC':\n",
    "                    correct += 1\n",
    "\n",
    "                # also correct type\n",
    "                elif predEnt[1] in cats[groundEnt[1]]:\n",
    "                    correct += 1\n",
    "\n",
    "                # not correct type\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                \n",
    "            groundIndex += 1\n",
    "            predIndex += 1\n",
    "                \n",
    "        # no overlap between spans\n",
    "        elif groundEnt[0][0] > predEnt[0][1]:\n",
    "            # ground is higher, increase predEnt\n",
    "            # spurious\n",
    "            # check ents: some do not count\n",
    "            if predEnt[1] not in spacyBanList:\n",
    "                spurious += 1\n",
    "            \n",
    "            predIndex += 1\n",
    "\n",
    "        elif groundEnt[0][1] < predEnt[0][0]:\n",
    "            # ground is lower, increase ground\n",
    "            # missing\n",
    "            groundIndex += 1\n",
    "            missing += 1\n",
    "        \n",
    "        # overlap between spans\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            groundIndex += 1\n",
    "            predIndex += 1   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "modified-affiliation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10359 1935 1093 1029\n",
      "recall 0.7775275838775051\n",
      "precision 0.7738104130873236\n",
      "f1 0.7756645451141895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10359,\n",
       " 'incorrect': 1935,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.7738104130873236,\n",
       " 'recall': 0.7775275838775051,\n",
       " 'f1': 0.7756645451141895}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'strict')\n",
    "with open('..\\\\data\\\\results\\\\ner_strict_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-status",
   "metadata": {},
   "source": [
    "## SpaCy eval exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "otherwise-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976 1318 1093 1029\n",
      "recall 0.8238384748179839\n",
      "precision 0.8198999028908642\n",
      "f1 0.8218644702358667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10976,\n",
       " 'incorrect': 1318,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.8198999028908642,\n",
       " 'recall': 0.8238384748179839,\n",
       " 'f1': 0.8218644702358667}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'exact')\n",
    "with open('..\\\\data\\\\results\\\\ner_exact_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-montreal",
   "metadata": {},
   "source": [
    "These are really good scores for the model, especially the exact one. But we need to keep in mind that the permorance of a NER model can depend on the sort of text. It could be that the spaCy model was trained on documents that have a very high resemblance to this test set. The performance on the WOB document can be lower than these results suggest.\n",
    "\n",
    "For the evaluation, spacy has a lot more things it looks for, like monotary values or percentages. These were skipped, so if spacy found an entity that the ground truth didnt have and the label was one of the banned labels, it was not considered a spurious match. The law label was also ignored because that will be done better by hand (hopefully)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python36464bite3e9fea8e7cf4572b612ccf79ec495cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
