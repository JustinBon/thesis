{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reduced-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "C:\\Users\\justin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "armed-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ministries = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-incident",
   "metadata": {},
   "source": [
    "# dates\n",
    "This first part will be the evalutation for the base dates extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attempted-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "       'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "sent = ['datum', 'verzonden', 'sent', 'date', 'received']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enhanced-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    \n",
    "def showMatches(doc, matches, regexMatches):   \n",
    "    indexOfMatches = []\n",
    "    for matchid, start, end in matches:\n",
    "        for i in range(start, end):\n",
    "            indexOfMatches.append(i)\n",
    "            \n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "        else:\n",
    "            if token.text in regexMatches:\n",
    "                printHilight(str(token.text))\n",
    "            else:\n",
    "                print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mathematical-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dates, sep, pat):\n",
    "    goodDates = []\n",
    "    for date in dates:\n",
    "        date = date.replace(' ', '')\n",
    "        try:\n",
    "            date = date.replace(sep, ' ')\n",
    "            datetime.strptime(date, pat)\n",
    "            goodDates.append(date.replace(' ', sep))\n",
    "        except:\n",
    "            try:\n",
    "                if len(date.split(' ')) == 3 and len(date.split(' ')[2]) == 2:\n",
    "                    datetime.strptime(date, '%d %m %y')\n",
    "                    goodDates.append(date.replace(' ', sep))\n",
    "            except:\n",
    "                pass     \n",
    "    return goodDates\n",
    "            \n",
    "\n",
    "def regexMatcher(text):\n",
    "    results = []\n",
    "    \n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]', text), '/', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]\\/[0-9]{2,4}', text), '/', '%d %m %Y')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]', text), '-', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]-[0-9]{2,4}', text), '-', '%d %m %Y')\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interesting-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'exit':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, exit or nothing\")\n",
    "        \n",
    "\n",
    "def evaluate(nlp):\n",
    "    \n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "           'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
    "           'ma', 'di', 'wo', 'woe', 'do', 'vrij', 'za', 'zat', 'zo', 'vr']\n",
    "    \n",
    "    datesPattern = [{\"LOWER\" : {\"IN\" : days}, \"OP\" : \"?\"}, \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True, \"OP\" : \"?\"}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "    \n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that isnt a mactch\n",
    "    if os.path.isfile('..\\\\data\\\\results\\\\dates_results,json'):\n",
    "        with open('..\\\\data\\\\results\\\\dates_results,json') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'america' :0,\n",
    "            'ocr':0,\n",
    "            'oneoff':0,\n",
    "            'other':0\n",
    "        }\n",
    "    \n",
    "    print(results)\n",
    "    cases = ['correct','incorrect','missing','spurious','america','ocr','oneoff','other']\n",
    "    documents = 0\n",
    "    \n",
    "    while(True):\n",
    "        try:\n",
    "            print(sum([results[x] for x in results.keys() if x in cases[:4]]), results['documents'])\n",
    "            sample = df.sample(1)\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            doc = nlp(text)\n",
    "            matches = matcher(doc)\n",
    "\n",
    "            regexMatches = regexMatcher(text)\n",
    "\n",
    "            showMatches(doc, matches, regexMatches)\n",
    "\n",
    "            for case in cases:\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    results['documents'] += documents\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "            documents += 1\n",
    "            clear_output()\n",
    "        except:\n",
    "            print('error')\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "scientific-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResults(r):\n",
    "    precision = r['correct'] / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = r['correct'] / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-kenya",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 227\n",
      "d8d9c5015c9ceb952052f29e1a27ed1f_openbaar-te-maken-documenten-deel-2 108 \n",
      "\n",
      "\n",
      "Departementaal vertrouwelijk \n",
      " Crisisteam \u001b[1;31m9 \u001b[0m\u001b[1;31mapril \u001b[0m\u001b[1;31m2020 \u001b[0m, 10:00 uur Notulist : Joyce Corver \n",
      " \n",
      " \n",
      " Akkoord : Johan Gro _ \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 35 . | Versoepeling \u001b[1;31m24/7 \u001b[0mbeleid aanwezigheid arts/verpleegkundige bij besmette gedetineerden c Afgerond \n",
      " < 70jr , \u001b[1;31m23-03-2020 \u001b[0m\n",
      " . | e e . O O hoeveel justitiabelen er Afgerond \n",
      " per plek aanwezig zijn , zodat het mogelijk wordt naar rato beschermingsmiddelen te \u001b[1;31m23-03-2020 \u001b[0m\n",
      " leveren . \n",
      " 37 . | e e t de procedure uit hoe het werkt met het verkrijgen van bijstand van Afgerond \n",
      " Defensie . \u001b[1;31m23-03-2020 \u001b[0m\n",
      " DGS&B doet de aanvraag . Er is een standaard format voor . \n",
      " 3 . | e t u t de factsheets die op intranet staan , t.b.v. justitiabelen naar Afgerond \n",
      " EE n n Zij u caten de factsheets . \u001b[1;31m23-03-2020 \u001b[0m\n",
      " \n",
      " \n",
      " 3 . | e e een voorstel voor het Beraad van komende Afgerond \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " maandag , \u001b[1;31m23 \u001b[0m\u001b[1;31mmaart \u001b[0m, over Penitentiaire programma en artikelplaatsingen ( extramuraal \u001b[1;31m23-03-2020 \u001b[0m\n",
      " verblijf } \n",
      " Beraad heeft gevraagd om dit voorstel in het Beleidsteam te brengen ( \u001b[1;31m23 \u001b[0m\u001b[1;31mmaart \u001b[0m) \n",
      " 40 . | Corporate Communicatie vertaalt de Q&A's voor personeel om te informeren over de Afgerond \n",
      " mogelijkheid om ander werk te moeten verrichten . Voorleggen ter arbeidsjuridische \u001b[1;31m23-03-2020 \u001b[0m\n",
      " toetsing . \n",
      " 41 . | B ontroleert of de ROAZ DJI-inrichtingen erkent als zorgaanbieder en meldt dit Afgerond \n",
      " voor overleg met de minister . \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 42 . De afgekondigde maatregelen gelden tot \u001b[1;31m6 \u001b[0m\u001b[1;31mapril \u001b[0m\u001b[1;31m. \u001b[0mCorporate Communicatie vermeldt dit in Afgerond \n",
      " de O&A's . \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 43 . | B controleert of de lijn die nu is uitgezet voor het vignet voor België ook geldt Afgerond \n",
      " om Nederland in te komen . ‚ \u001b[1;31m24-03-2020 \u001b[0m\n",
      " 44 . DPMO maakt een voorstel over het uitbetalen van overwerk en het schuiven van verlof . Afgerond \n",
      " Het voorstel ligt ter besluit in het Beraad van \u001b[1;31m26 \u001b[0m\u001b[1;31mmaart \u001b[0m\u001b[1;31m. \u001b[0m\u001b[1;31m25-03-2020 \u001b[0m\n",
      " 45 . De DV&O werkt een nota uit over de extra te kopen enkelbanden en de systemen . Afgerond \n",
      " \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 46 . | DPMO heeft de Memo “ inzet personeel D ! naar inrichtingen/Covid- 19 \" afgerond . y Afgerond \n",
      " G zeeft akkoord . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 47 . Corporate Communicatie brengt de vragen van advocaten in beeld . Indien nodig worden Afgerond \n",
      " er Q8A's gemaakt . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " DIZ en CC schakelen over welke info.er is en wat gevraagd wordt . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 48 . e everi bij SSm een lijst aan , waarop staat waar de problemen met de Afgerond \n",
      " ROAZ zich voordoen . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " E neemt contact op met VWS . \n",
      " 49 . | GW meldt de inrichtingen dat de GGD gevraagd moet worden voor coronatests . Afgerond \n",
      " Morgen terugkoppeling of dit is gebeurd . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 50 . | Meeft door aan VWS dat de GGD alle DJI-inrichtingen gelijkstelt aan een Afgerond \n",
      " zorginstelling , in het licht van de coronatests . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 51 . | e evert tekst en getallen met de uitleg over de extra enkelbanden aan Afgerond \n",
      " _ S e behoeve van de minister . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 52 . S e emt contact op met de Reclassering ( S o te vragen of zij Afgerond \n",
      " meegaan in de ontwikkelingen van uitbreiden enkelbandmonitoring . \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 53 . | Erworden Q&A's op internet gezet voor advocaten die vragen hebben over Afgerond \n",
      " detentieongeschiktheid \u001b[1;31m25-03-2020 \u001b[0m\n",
      " 54 . | De brief van - \" de bonden m.b.t. het meerpersoonscelgebruik is in de Afgerond \n",
      " \n",
      " \n",
      " \n",
      " Pagina 5 van 7 \n",
      " correct1\n",
      "incorrectexit\n",
      "precision = 0.9506726457399103, recall = 0.8706365503080082, f1 = 0.9088960342979635\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(nlp)\n",
    "results = calcResults(results)\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\dates_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-fantasy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 424,\n",
       " 'incorrect': 9,\n",
       " 'missing': 54,\n",
       " 'spurious': 13,\n",
       " 'america': 30,\n",
       " 'ocr': 19,\n",
       " 'oneoff': 12,\n",
       " 'other': 15,\n",
       " 'documents': 265,\n",
       " 'precision': 0.9506726457399103,\n",
       " 'recall': 0.8706365503080082,\n",
       " 'f1': 0.9088960342979635}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-mentor",
   "metadata": {},
   "source": [
    "removed all access newlines and spaces\n",
    "\n",
    "Part of the reason not all dates were caught was an oversight with american date formats. To validate the if a found match is actually a date, it needs a date format to compare the match to. Only the dd-mm-yyyy format was checked and not the american format of mm-dd-yyyy with the month before the year. This means it excludes dates like 04-14-2022 as this cannot be a date in the dd-mm-yyyy format because there obviously isn't a 14th month. In the american system this is just april 14th 2022.\n",
    "\n",
    "Another reason for not finding some dates is OCR mistakes. For example, in one case the date that was supposed to be found was \"5/12/2021\" but in the OCR process that string was read as \"542/2021\" where the \"/\" and \"1\" were seen as one character, a 4.\n",
    "\n",
    "Missed dates in file names were excluded from the evaluation because these are not the dates your looking for\n",
    "\n",
    "precision = 0.9197860962566845, recall = 0.8911917098445595, f1 = 0.9052631578947369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-october",
   "metadata": {},
   "source": [
    "# Ministries\n",
    "This next part is the evalutation of the ministries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caring-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinisteries():\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "\n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    minList = list(wikis.keys()) + [wikis[x]['Abbriviation'] for x in wikis.keys()]\n",
    "    for x in minList:\n",
    "        temp+=x.replace(',', '').split(' ')\n",
    "    \n",
    "    return [x.lower() for x in temp if x != 'en' and x != '']\n",
    "minList = getMinisteries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "conceptual-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "\n",
    "def printHilightUnderline(string):\n",
    "    print('\\033[4m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    \n",
    "def showMatchesMinistries(doc, minList):   \n",
    "    indexOfMatches = []\n",
    "    for ent in doc.ents:\n",
    "        for i in range(int(ent.start), int(ent.end)):\n",
    "            indexOfMatches.append(i)\n",
    "\n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "\n",
    "    for token in doc:\n",
    "        flag = False\n",
    "        for mini in minList:\n",
    "            if token.text.lower() == mini:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "            \n",
    "        elif flag:\n",
    "            printHilightUnderline(str(token.text))\n",
    "            \n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "japanese-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'q':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, exit or nothing\")\n",
    "\n",
    "def evaluateMinistries(nlp, minList):\n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that\n",
    "    # isnt a mactch, \n",
    "    results = {\n",
    "        'correct':0,\n",
    "        'incorrect':0,\n",
    "        'missing':0,\n",
    "        'spurious':0,\n",
    "        'partial':0\n",
    "    }\n",
    "    \n",
    "    minList = set(minList)\n",
    "    while(True):\n",
    "        try:\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    print(m, 'AHHHHHHHHHHHHHHHH')\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(sum(results.values()))\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            showMatchesMinistries(doc, minList)\n",
    "\n",
    "            for case in results.keys():\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "\n",
    "            clear_output()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "increased-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResults(r):\n",
    "    precision = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "stuck-surveillance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zaken AHHHHHHHHHHHHHHHH\n",
      "96\n",
      "6762214604a58986abf9cc852b4202e9_bijlagen-deel-7-bij-besluit-wob-verzoek-over-covid-19 59 \n",
      "\n",
      "\n",
      "529309 \n",
      " \n",
      " \n",
      " | Crisiscoördinator DCC \u001b[1;31mVWS \u001b[0m| \n",
      " Ministerie van Volksgezondheld , \u001b[4mWelzijn \u001b[0men \u001b[4mSport \u001b[0m| Directie Publieke Gezondheid | \n",
      " Afdeling Crisisbeheersing en Infectieziekten | Etage : 8 flex \n",
      " 102 _ G \n",
      " Parnassusplein 5| 2511 VX | Den Haag | \n",
      " Postbus 20350 | 2500 EJ | Den Haag \n",
      " \n",
      " \n",
      " \n",
      " Van : NCC - NCTV \n",
      " Verzonden : woensdag 29 januari 2020 14:54:01 ( UTC+01:00 ) Amsterdam , Berlijn , Bern , Rome , Stockholm , Wenen \n",
      " Onderwerp : RECTIFICATIE uitnodiging IAO maandag 3 februari om 14.00 uur met betrekking tot de stand van \u001b[4mzaken \u001b[0mrondom de \n",
      " uitbraak van het Corona virus in China \n",
      " RECTIFICATIE : 14:00 uur i.p.v. 13.30 uur \n",
      " Geachte heer/mevrouw , \n",
      " Op verzoek van \u001b[1;31mVWS \u001b[0men NCTV nodig ik u uit voor een Interdepartementaal Afstemmings Overleg ( IAO ) op maandag 3 \n",
      " februari om 14.00 uur met betrekking tot de stand van \u001b[4mzaken \u001b[0mrondom de uitbraak van het Corona virus in China . \n",
      " De bijeenkomst vindt plaats bij het \u001b[1;31mMinisterie \u001b[0m\u001b[1;31mvan \u001b[0m\u001b[1;31mJustitie \u001b[0m\u001b[1;31men \u001b[0m\u001b[1;31mVeiligheid \u001b[0m, Turfmarkt 147 , 7e etage . NCTV , MCCb/ICCb- \n",
      " zaal . \n",
      " Eventuele stukken voor de vergadering worden ter plekke uitgedeeld . \n",
      " Graag ontvangen wij een bevestiging van uw aanwezigheid via onderstaand emailadres van het NCC . Vergeet niet om uw \n",
      " legitimatiebewijs mee te nemen . \n",
      " Buiten kantoortijden kunt zich melden bij de 24-uurs ingang . Deze is gelegen aan de Schedeldoekshaven 552 , aan de \n",
      " achterzijde van het gebouw , naast de parkeergarage . \n",
      " Met vriendelijke groet , \n",
      " Nationaal CrisisCentrum ( NCC ) \n",
      " E @nctv.minjenv.nl \n",
      " ( 10){2e ) ( algemeen nummer ) \n",
      " E ( ncident nummer ) \n",
      " \n",
      " ( 10)(2e ) \n",
      " \n",
      " Dit bericht kan informatie bevatten die niet voor u is bestemd . Indien u niet de geadresseerde bent of dit bericht \n",
      " abusievelijk aan u is toegezonden , wordt u verzocht dat aan de afzender te melden en het bericht te verwijderen . De Staat \n",
      " aanvaardt geen aansprakelijkheid voor schade , van welke aard ook , die verband houdt met risico's verbonden aan het \n",
      " elektronisch verzenden van berichten . \n",
      " \u001b[1;31mMinisterie \u001b[0m\u001b[1;31mvan \u001b[0m\u001b[1;31mJustitie \u001b[0m\u001b[1;31men \u001b[0m\u001b[1;31mVeiligheid \u001b[0m\n",
      " This message may contain information that is not intended for you . If you are not the addressee or if this message was sent \n",
      " to you by mistake , you are requested to inform the sender and delete the message . The State accepts no liability for \n",
      " damage of any kind resulting from the risks inherent in the electronic transmission of messages . \n",
      " test 4\n",
      "correct4\n",
      "incorrect\n",
      "missing1\n",
      "spurious\n",
      "partialexit\n",
      "input number, exit or nothing\n",
      "partialq\n",
      "precision = 0.96, recall = 0.7912087912087912, f1 = 0.8674698795180723\n"
     ]
    }
   ],
   "source": [
    "results = evaluateMinistries(nlp_ministries, minList)\n",
    "calcResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aging-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "t = nlp_ministries('het Ministerie is een beetje dom')\n",
    "tl = ['ministerie']\n",
    "for w in t:\n",
    "    print(str(w.text).lower() in tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "generous-major",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wvc',\n",
       " 'algemene',\n",
       " 'zaken',\n",
       " 'binnenlandse',\n",
       " 'zaken',\n",
       " 'koninkrijksrelaties',\n",
       " 'buitenlandse',\n",
       " 'zaken',\n",
       " 'defensie',\n",
       " 'economische',\n",
       " 'zaken',\n",
       " 'klimaat',\n",
       " 'financiën',\n",
       " 'infrastructuur',\n",
       " 'waterstaat',\n",
       " 'justitie',\n",
       " 'veiligheid',\n",
       " 'landbouw',\n",
       " 'natuur',\n",
       " 'voedselkwaliteit',\n",
       " 'onderwijs',\n",
       " 'cultuur',\n",
       " 'wetenschap',\n",
       " 'sociale',\n",
       " 'zaken',\n",
       " 'werkgelegenheid',\n",
       " 'volksgezondheid',\n",
       " 'welzijn',\n",
       " 'sport',\n",
       " 'az',\n",
       " 'bzk',\n",
       " 'bz',\n",
       " 'def',\n",
       " 'ez',\n",
       " 'fin',\n",
       " 'i&w',\n",
       " 'j&v',\n",
       " 'lnv',\n",
       " 'ocw',\n",
       " 'szw',\n",
       " 'vws']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-burns",
   "metadata": {},
   "source": [
    "Missing things in situations like this: RVO/LNV. LNV should be have been caught here. Places with a lot of extra newlines or spaces within the name of a ministry will also trip up the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-electron",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n",
    "This next part is the spacy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGroundTruth():\n",
    "    with open('..\\\\data\\\\ner labeled data\\\\test.conllu', 'r', encoding='utf8') as f:\n",
    "        ground = f.read()\n",
    "        ground = ground.split('\\n')\n",
    "        ground = [x.split('\\t') for x in ground]\n",
    "    \n",
    "    text = []\n",
    "    for word in ground:\n",
    "        try:\n",
    "            text.append(word[1])\n",
    "        except IndexError:\n",
    "            text.append('\\n')\n",
    "    \n",
    "\n",
    "    text = ' '.join(text)\n",
    "    return text, ground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "willing-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function makes sure that an index refers to the same token in both the ground truth and the doc\n",
    "def sync(ground, doc):\n",
    "    \n",
    "    # newlines in ground truth are given as an empty string in a list\n",
    "    # spacy doesnt do tokens for newlines so these can be removed\n",
    "    ground = [x for x in ground if x != ['']]\n",
    "    \n",
    "    groundIndex, docIndex = 0, 0\n",
    "    docNew, groundNew = [], []\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex+2 > len(ground) or docIndex+2 > len(doc):\n",
    "            return groundNew, docNew\n",
    "        \n",
    "        \n",
    "        if str(doc[docIndex].text) != ground[groundIndex][1]:\n",
    "            # see if next token in ground truth equals current doc token\n",
    "            if str(doc[docIndex].text) == ground[groundIndex + 1][1]:\n",
    "                groundIndex +=1\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex + 2][1]:\n",
    "                groundIndex +=2\n",
    "\n",
    "            # see if next token in doc equals current ground truth token\n",
    "            elif str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=1\n",
    "            elif str(doc[docIndex + 2].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=2\n",
    "\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex += 2\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex ][1] + ground[groundIndex + 1][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 2\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex+1].text)+ str(doc[docIndex+2].text) == ground[groundIndex][1]:\n",
    "                docIndex += 3\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex][1] + ground[groundIndex+1][1] + ground[groundIndex+2][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 3\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(str(doc[docIndex].text), str(doc[docIndex+1].text),str(doc[docIndex+2].text)) \n",
    "                print(ground[groundIndex][1],ground[groundIndex+1][1],ground[groundIndex+2][1])\n",
    "                return groundNew, docNew\n",
    "        \n",
    "        # add good tokens to new lists\n",
    "        groundNew.append((ground[groundIndex][1], ground[groundIndex][2]))\n",
    "        docNew.append((str(doc[docIndex].text), doc[docIndex].ent_type_))\n",
    "            \n",
    "        groundIndex += 1\n",
    "        docIndex += 1\n",
    "\n",
    "def testSync(ground, doc):\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i][0] != ground[i][0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exceptional-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets the span of all entities in ground truth\n",
    "def getSpans(ground):\n",
    "    span = []\n",
    "    i = 0\n",
    "    flag = False\n",
    "    begin = 0\n",
    "    \n",
    "    while i < len(ground):\n",
    "        \n",
    "        # base case: no continuation of entiy or start of entity\n",
    "        if ground[i][1] == 'O' and flag == False:\n",
    "            pass\n",
    "        \n",
    "        # end of entity span no new entity, reset flag and add entity\n",
    "        elif ground[i][1] == 'O' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            flag = False\n",
    "        \n",
    "        # end of entity span, new entity starts. Reset flag, add entity, and start new entity\n",
    "        elif ground[i][1][0] == 'B' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            begin = i\n",
    "        \n",
    "        # start of new entity, set flag and begin\n",
    "        elif ground[i][1][0] == 'B' and flag == False:\n",
    "            flag = True\n",
    "            begin = i\n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "annoying-intermediate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "def savePredictions(predictions):\n",
    "    begin = [x[0][0] for x in predictions]\n",
    "    end = [x[0][1] for x in predictions]\n",
    "    entType = [x[1] for x in predictions]\n",
    "    ent = [x[2] for x in predictions]\n",
    "    pred = pd.DataFrame.from_dict({'begin':begin, 'end':end, 'entType':entType, 'ent':ent})\n",
    "    pred.to_csv('..\\\\data\\\\predictions.csv')\n",
    "\n",
    "# get entities found by spacy in correct format\n",
    "def getPredictionSpans(doc, tokens):\n",
    "    spans = []\n",
    "    \n",
    "    i = 0\n",
    "    entityIndex = 0\n",
    "    \n",
    "    # check for every entity\n",
    "    while i < len(tokens):\n",
    "        try:\n",
    "            \n",
    "            # if current token has an entity label\n",
    "            if tokens[i][1] != '':\n",
    "                \n",
    "                # get entity type and string representation\n",
    "                entity = doc.ents[entityIndex]\n",
    "                entityType = doc[entity.start].ent_type_\n",
    "                entity = str(entity)\n",
    "                \n",
    "                # get number of tokens in entity and add to list\n",
    "                nTokens = len(entity.split(' '))\n",
    "                spans.append(((i, i + nTokens), entityType, entity))\n",
    "                \n",
    "                # increase token index by number of tokens in current entity\n",
    "                i += nTokens\n",
    "                \n",
    "                # set entity index to next\n",
    "                entityIndex += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "        except IndexError:\n",
    "            savePredictions(spans)\n",
    "            return\n",
    "        \n",
    "    \n",
    "    savePredictions(spans)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "intensive-shipping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load predictions from csv\n",
    "def getPredictions():\n",
    "    predictions = pd.read_csv('..\\\\data\\\\predictions.csv')\n",
    "    begin = list(predictions.begin)\n",
    "    end = list(predictions.end)\n",
    "    entType = list(predictions.entType)\n",
    "    ent = list(predictions.ent)\n",
    "    \n",
    "    predictionsList = []\n",
    "    for i in range(len(begin)):\n",
    "        predictionsList.append(((begin[i], end[i]), entType[i], ent[i]))\n",
    "        \n",
    "    return predictionsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "vocational-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = full text as string\n",
    "# groundOld = tokens with ground truth labels\n",
    "# doc = spacy doc of full text\n",
    "# ground = list of tokens with ground truth labels in sync with docList\n",
    "# docList = list of tokens from spacy with predicted labels in sync with ground\n",
    "# predictions = list of predicted entities by spacy with begin, end, type and text\n",
    "# span = list of ground truth entites with begin, end, type\n",
    "\n",
    "\n",
    "text, groundOld = loadGroundTruth()\n",
    "doc = nlp(text[:1000000])\n",
    "ground, docList = sync(groundOld, doc)\n",
    "predictions = getPredictions()\n",
    "span = getSpans(ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "diagnostic-calgary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170685"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groundOld)\n",
    "len(docList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-radar",
   "metadata": {},
   "source": [
    "## SpaCy eval normal (Doenst work, is not used)\n",
    "also very bad and doesnt even do what it is supposed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "personalized-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1(tp, tn, fp, fn):\n",
    "    print(tp, tn, fp, fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "    return recall, precision, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "requested-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNER(ground, doc):\n",
    "    \n",
    "    groundIndex, docIndex = 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    nerCats = ['FAC', 'PERSON', 'ORG', 'GPE', 'LOC']\n",
    "    groundCats = ['ORG', 'LOC', 'PER']\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            if str(doc[docIndex].text) != ground[groundIndex][1]:\n",
    "                \n",
    "                # checks for if the words are still in sync\n",
    "                if str(doc[docIndex + 1].text) != ground[groundIndex][1]:\n",
    "                    docIndex += 1\n",
    "                    continue\n",
    "                \n",
    "                elif str(doc[docIndex].text) != ground[groundIndex + 1][1]:\n",
    "                    groundIndex += 1\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    for j in range(docIndex-3,docIndex+3):\n",
    "                        print(doc[j], ground[j])\n",
    "                \n",
    "            else:\n",
    "                if doc[docIndex].ent_type_ == '' and ground[groundIndex][2] == 'O':\n",
    "                    tn += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ in nerCats and ground[groundIndex][2] == 'O':\n",
    "                    fp += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ in nerCats and ground[groundIndex][2] != 'O':\n",
    "                    tp += 1\n",
    "                    \n",
    "                elif doc[docIndex].ent_type_ == '' and ground[groundIndex][2] != 'O':\n",
    "                    fn += 1\n",
    "        \n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        groundIndex +=1\n",
    "        docIndex +=1\n",
    "        \n",
    "        if groundIndex > len(ground) and docIndex > len(doc):\n",
    "            print('good')\n",
    "            return calcF1(tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "experienced-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "1240 10349 47 185\n",
      "recall 0.8701754385964913\n",
      "precision 0.9634809634809635\n",
      "f1 0.9144542772861357\n"
     ]
    }
   ],
   "source": [
    "recall, precision, f1 = evalNER(ground, doc)\n",
    "print('recall', recall)\n",
    "print('precision', precision)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-thing",
   "metadata": {},
   "source": [
    "## SpaCy eval strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "previous-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1Strict(cor, inc, spu, mis):\n",
    "    print(cor,inc,spu,mis)\n",
    "    recall = cor / (cor+inc+mis)\n",
    "    precision = cor / (cor+inc+spu)\n",
    "    f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "    print('recall', recall)\n",
    "    print('precision', precision)\n",
    "    print('f1', f1)\n",
    "    results = {'total':cor + inc + spu + mis,\n",
    "              'correct':cor,\n",
    "              'incorrect':inc,\n",
    "              'missing':mis,\n",
    "              'spurious':spu,\n",
    "              'precision':precision,\n",
    "              'recall':recall,\n",
    "              'f1':f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "express-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNER(ground, pred, method):\n",
    "    correct, incorrect, spurious, missing = 0, 0, 0, 0\n",
    "    \n",
    "    spacyBanList = ['CARDINAL','DATE','LAW','MONEY','ORDINAL','PERCENT','QUANTITY','TIME']\n",
    "    cats = {'B-ORG': ['ORG'],\n",
    "           'B-PER': ['PERSON'],\n",
    "           'B-LOC': ['FAC', 'GPE', 'LOC'],\n",
    "           'B-ORG': ['ORG']}\n",
    "    \n",
    "    groundIndex = 0\n",
    "    predIndex = 0\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex >= len(ground) or predIndex >= len(pred):\n",
    "            return calcF1Strict(correct, incorrect, spurious, missing)\n",
    "        \n",
    "        # set current tokens \n",
    "        groundEnt = ground[groundIndex]\n",
    "        predEnt = pred[predIndex]\n",
    "        \n",
    "        # correct span\n",
    "        if groundEnt[0] == predEnt[0]:\n",
    "            if method == 'exact':\n",
    "                correct += 1\n",
    "            \n",
    "            else:\n",
    "                # correct type\n",
    "                if groundEnt[1] == 'B-MISC':\n",
    "                    correct += 1\n",
    "\n",
    "                # also correct type\n",
    "                elif predEnt[1] in cats[groundEnt[1]]:\n",
    "                    correct += 1\n",
    "\n",
    "                # not correct type\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                \n",
    "            groundIndex += 1\n",
    "            predIndex += 1\n",
    "                \n",
    "        # no overlap between spans\n",
    "        elif groundEnt[0][0] > predEnt[0][1]:\n",
    "            # ground is higher, increase predEnt\n",
    "            # spurious\n",
    "            # check ents: some do not count\n",
    "            if predEnt[1] not in spacyBanList:\n",
    "                spurious += 1\n",
    "            \n",
    "            predIndex += 1\n",
    "\n",
    "        elif groundEnt[0][1] < predEnt[0][0]:\n",
    "            # ground is lower, increase ground\n",
    "            # missing\n",
    "            groundIndex += 1\n",
    "            missing += 1\n",
    "        \n",
    "        # overlap between spans\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            groundIndex += 1\n",
    "            predIndex += 1   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "productive-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10359 1935 1093 1029\n",
      "recall 0.7775275838775051\n",
      "precision 0.7738104130873236\n",
      "f1 0.7756645451141895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10359,\n",
       " 'incorrect': 1935,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.7738104130873236,\n",
       " 'recall': 0.7775275838775051,\n",
       " 'f1': 0.7756645451141895}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'strict')\n",
    "with open('..\\\\data\\\\results\\\\ner_strict_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-cambridge",
   "metadata": {},
   "source": [
    "## SpaCy eval exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "greenhouse-grace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976 1318 1093 1029\n",
      "recall 0.8238384748179839\n",
      "precision 0.8198999028908642\n",
      "f1 0.8218644702358667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10976,\n",
       " 'incorrect': 1318,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.8198999028908642,\n",
       " 'recall': 0.8238384748179839,\n",
       " 'f1': 0.8218644702358667}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'exact')\n",
    "with open('..\\\\data\\\\results\\\\ner_exact_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-cycle",
   "metadata": {},
   "source": [
    "These are really good scores for the model, especially the exact one. But we need to keep in mind that the permorance of a NER model can depend on the sort of text. It could be that the spaCy model was trained on documents that have a very high resemblance to this test set. The performance on the WOB document can be lower than these results suggest.\n",
    "\n",
    "For the evaluation, spacy has a lot more things it looks for, like monotary values or percentages. These were skipped, so if spacy found an entity that the ground truth didnt have and the label was one of the banned labels, it was not considered a spurious match. The law label was also ignored because that will be done better by hand (hopefully)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python36464bite3e9fea8e7cf4572b612ccf79ec495cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
