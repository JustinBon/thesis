{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-algeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stretch-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "empty-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ministries = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-sixth",
   "metadata": {},
   "source": [
    "This notebook is for rq 2.2.2\n",
    "\n",
    "There were a lot of itterations for this extractor. These will be first. At the end, the extractor that was used is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cubic-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string, extraSpace = True):\n",
    "    if extraSpace:\n",
    "        print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    else:\n",
    "        print('\\x1b[1;31m'+string + '\\x1b[0m', end='')\n",
    "\n",
    "def printHilightUnderline(string):\n",
    "    print('\\033[4m\\033[96m'+string  +'\\033[96m\\x1b[0m' + ' ', end='')\n",
    "    \n",
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'q':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, q or nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "combined-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates f1, precision and recall with the partial method\n",
    "def calcResultsMinistries(r):\n",
    "    precision = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    r['precision'] = precision\n",
    "    r['recall'] = recall\n",
    "    r['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    print(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brazilian-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of current dutch ministries and their abbriviations\n",
    "def getMinisteries(getList):\n",
    "    \n",
    "    # get html of the wiki page of dutch ministries\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "    \n",
    "    # get all links in the html\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "    \n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    if getList:\n",
    "        minList = list(wikis.keys()) + [wikis[x]['Abbriviation'] for x in wikis.keys()]\n",
    "        for x in minList:\n",
    "            temp+=x.replace(',', '').split(' ')\n",
    "\n",
    "        return [x.lower() for x in temp if x != 'en' and x != ''] + ['ezk']\n",
    "    else:\n",
    "        return wikis\n",
    "    \n",
    "minList = getMinisteries(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlikely-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showMatchesOld(text, matches, minList):   \n",
    "    charIndex = 0\n",
    "    mIndex = 0\n",
    "\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.split(' ')\n",
    "    flag = False\n",
    "\n",
    "\n",
    "    for word in text:\n",
    "        startMatch = matches[mIndex][0]\n",
    "        endMatch = matches[mIndex][1]\n",
    "        # continuation of match\n",
    "        if flag:\n",
    "\n",
    "            # if end of match\n",
    "            if endMatch <= charIndex + len(word):\n",
    "                printHilight(word[:endMatch - charIndex], False)\n",
    "                print(word[endMatch - charIndex:], end=' ')\n",
    "\n",
    "                flag = False\n",
    "                mIndex += 1\n",
    "            # if end of match\n",
    "            else:\n",
    "                printHilight(word)\n",
    "\n",
    "        # match\n",
    "        elif startMatch == charIndex and endMatch == charIndex + len(word):\n",
    "            printHilight(word)\n",
    "            mIndex += 1\n",
    "\n",
    "        # first part of a match\n",
    "        elif startMatch == charIndex:\n",
    "            printHilight(word)\n",
    "            flag = True\n",
    "\n",
    "        # partial match\n",
    "        elif startMatch < charIndex + len(word):\n",
    "\n",
    "            print(word[:startMatch - charIndex], end='')\n",
    "            if endMatch <= charIndex + len(word):\n",
    "                printHilight(word[startMatch - charIndex:endMatch - charIndex], False)\n",
    "                print(word[endMatch - charIndex:], end=' ')\n",
    "                mIndex += 1\n",
    "            else:\n",
    "                printHilight(word[startMatch - charIndex:endMatch - charIndex])\n",
    "                flag = True\n",
    "\n",
    "\n",
    "        # potential uncaught match\n",
    "        elif word.lower() in minList:\n",
    "            printHilightUnderline(word)\n",
    "\n",
    "        # normal word\n",
    "        else:\n",
    "            print(word, end=' ')\n",
    "\n",
    "        charIndex += len(word) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moving-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinisteries(minList):\n",
    "    \n",
    "    # gets gazetteers for ministries\n",
    "    ministeries = getMinisteries(False)\n",
    "    abrr = [ministeries[x]['Abbriviation'].lower() for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    mins = [x.lower() for x in list(ministeries.keys())]\n",
    "    print(mins)\n",
    "    \n",
    "    # variants of the extractor\n",
    "    variants = [('full', True, True), ('broad', True, False), ('no_prefix', False, False)]\n",
    "    output = {}\n",
    "\n",
    "    # if results already exist, this will load them\n",
    "    # otherwise it will create a new results set\n",
    "    for v in variants:\n",
    "        if os.path.isfile('..\\\\data\\\\results\\\\min_old_' + v[0] + '.json'):\n",
    "            with open('..\\\\data\\\\results\\\\min_old_' + v[0] + '.json') as f:\n",
    "                output[v[0]] = json.load(f)\n",
    "            print('Loaded previous output for ', v[0])\n",
    "        else:\n",
    "            print('new output for ' + v[0])\n",
    "            output[v[0]] = {\n",
    "                'abbr':v[1],\n",
    "                'lookForMin':v[2],\n",
    "                'correct':0,\n",
    "                'partial':0,\n",
    "                'missing':0,\n",
    "                'spurious':0,\n",
    "            }\n",
    "    \n",
    "\n",
    "    minList = set(getMinisteries(True))\n",
    "\n",
    "    # one loop is one document\n",
    "    while(True):\n",
    "        try:\n",
    "\n",
    "            # sample a random document from the df\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "\n",
    "            # remove exces newlines and spaces\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # loop for all extractors\n",
    "            for i in output:\n",
    "\n",
    "                # set result to current extractor\n",
    "                results = output[i]\n",
    "\n",
    "                # print some info\n",
    "                print('version', i)\n",
    "                print(sum(list(results.values())[2:6] ))\n",
    "                print(sample.name.values[0], sample.page.values[0])\n",
    "\n",
    "                # settings for extractor\n",
    "                if results['abbr']:\n",
    "                    allMinisteries = mins + abrr\n",
    "                else:\n",
    "                    allMinisteries = mins\n",
    "\n",
    "                matches = []\n",
    "\n",
    "                # look for all ministries\n",
    "                for ministerie in allMinisteries:\n",
    "                    \n",
    "                    # more settings for extractor\n",
    "                    if results['lookForMin']:        \n",
    "                        matches += [(x.start(), x.end()) for x in re.finditer('ministerie van ' + ministerie.lower(),text.lower())]       \n",
    "                    else:\n",
    "                        matches += [(x.start(), x.end()) for x in re.finditer(ministerie.lower(),text.lower())]    \n",
    "\n",
    "                # get number of matches and pring\n",
    "                print('n matches: ', len(matches), '\\n\\n')\n",
    "\n",
    "                # sort matches and show document\n",
    "                matches = sorted(matches, key=lambda tup: tup[0])\n",
    "                matches.append((len(text), len(text) + 1))\n",
    "                showMatchesOld(text, matches, minList)\n",
    "\n",
    "                # inputs for performance\n",
    "                for case in list(results.keys())[2:6]:\n",
    "                    result = inputHandling(case)\n",
    "\n",
    "                    # if input function returns -1, the program stops\n",
    "                    if result == -1:\n",
    "                        output[i] = results\n",
    "                        return output\n",
    "                    else:\n",
    "                        results[case] += result\n",
    "\n",
    "                # clear output for next loop\n",
    "                clear_output()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "described-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version no_prefix\n",
      "108\n",
      "07e2b274045cb5b4f54371a3c905cae9_wobverzoek-mccb-catshuis 2\n",
      "n matches:  0 \n",
      "\n",
      "\n",
      "document 02 Sitrap \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m COVID-19, nummer 1   Van: Departementaal Crisiscentrum (DCC) Aan: leden Bewindspersonenstaf, leden Bestuursraad, directeuren \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m Datum: 1 maart, 17.00 Departementaal vertrouwelijk   Dit is een eerste sitrap, op basis van de thans bij het DCC bekende informatie. Het beeld zal de komende tijd telkens worden aangevuld met actuele informatie. Zo lang relevant, wordt voor elke bewindspersonenstaf en BR een actuele sitrap rondgestuurd. Aanvullingen of wijzigingen kunnen worden aangeleverd bij dcc@minszw.nl of 1022 @minszw.nl   Crisisstructuur SZW:   De NCTV en \u001b[4m\u001b[96mVWS\u001b[96m\u001b[0m zijn leidend in de interdepartementale crisisstructuur. DCC \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m coördineert de aanpak binnen SZW, is de verbindingsschakel van de interdepartementale crisisstructuur naar \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m en zorgt voor het informeren van de politieke en ambtelijke leiding SZW. De directeur BO is hoofd DCC. De directies blijven verantwoordelijk voor activiteiten die op hun terrein liggen. Verzoek is om voor alle \u001b[4m\u001b[96mzaken\u001b[96m\u001b[0m die het coronavirus betreffen het DCC te informeren. DCC zorgt voor de voorbereiding van de overleggen in de interdepartementale crisisstructuur: de hoogambtelijke Interdepartementale Commissie Crisisbeheersing (ICCb, pSG neemt deel) en het Interdepartementaal Afstemmingsoverleg (IAO; DCC en G&VW nemen deel). Communicatie neemt deel aan het interdepartementale Nationaal Kernteam Crisiscommunicatie (NKC). Het NKC zorgt voor rijksbreed afgestemde communicatie. De communicatie vanuit het NKC wordt gecoördineerd door VWS. Binnen \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m is er een afstemmingsgroep van de betrokken directies (G&VW, UAW, AV, SV, KO, Inspectie, C, BO/DCC). Werktijdverkorting (wtv):   Het aantal aanvragen wtv neemt nog steeds toe. Op dit moment heeft UAW 87 aanvragen toegewezen (910 werknemers), 22 aanvragen afgewezen (141 werknemers), 70 \u001b[4m\u001b[96mzaken\u001b[96m\u001b[0m in behandeling en 36 nieuwe zaken, die nog niet in behandeling zijn. Het type bedrijven loopt erg uiteen. Kwantificeren per sector is niet mogelijk. Het is moeilijk te controleren of alle aanvragen terecht zijn. FEZ is gevraagd daarvoor een risicosessie te organiseren. In media en vanuit bedrijven is veel aandacht voor werktijdverkorting. Dagelijks vragen media naar de actuele stand van het aantal aanvragen en toekenningen. Deze wordt dagelijks door woordvoering van \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m verstrekt. Voornemen is deze week vanuit \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m een nieuwsbericht aan werktijdverkorting te wijden. Vanwege overbelasting is het reguliere telefonische informatienummer van de directie bij \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m geschrapt. Telefonisch contact verloopt nu via het \u001b[4m\u001b[96malgemene\u001b[96m\u001b[0m informatienummer van het Rijk. Echter wordt door de telefonisten aangegeven dat zij hier niet over gaan. Het \u001b[4m\u001b[96malgemene\u001b[96m\u001b[0m nummer wordt van de site gehaald met informatie over wtv. Er wordt gezocht naar een oplossing (zie hieronder). Als de werkgever van \u001b[4m\u001b[96mSZW\u001b[96m\u001b[0m een vergunning voor werktijdverkorting heeft, kan hij de (gedeeltelijke) WW-uitkeringen aanvragen bij UWV. Deze worden afgehandeld door een kleine afdeling bij het UWV, dus men houdt ook hier een vinger aan de pols. Er is contact tussen \u001b[4m\u001b[96mEZK\u001b[96m\u001b[0m en AV tot stand gebracht zodat de brede \u001b[4m\u001b[96meconomische\u001b[96m\u001b[0m \u001b[1;31m \u001b[0mcorrect\n",
      "partial\n",
      "missing11\n",
      "spuriousq\n",
      "precision = 1.0, recall = 0.102803738317757, f1 = 0.1864406779661017\n",
      "{'abbr': True, 'lookForMin': True, 'correct': 11, 'partial': 0, 'missing': 96, 'spurious': 0, 'incorrect': 0, 'precision': 1.0, 'recall': 0.102803738317757, 'f1': 0.1864406779661017}\n",
      "______________________________\n",
      "precision = 0.23697916666666666, recall = 0.9381443298969072, f1 = 0.37837837837837834\n",
      "{'abbr': True, 'lookForMin': False, 'correct': 90, 'partial': 2, 'missing': 7, 'spurious': 294, 'incorrect': 0, 'precision': 0.23697916666666666, 'recall': 0.9381443298969072, 'f1': 0.37837837837837834}\n",
      "______________________________\n",
      "precision = 0.5454545454545454, recall = 0.11009174311926606, f1 = 0.183206106870229\n",
      "{'abbr': False, 'lookForMin': False, 'correct': 12, 'partial': 0, 'missing': 97, 'spurious': 10, 'incorrect': 0, 'precision': 0.5454545454545454, 'recall': 0.11009174311926606, 'f1': 0.183206106870229}\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "results = findMinisteries(minList)\n",
    "\n",
    "for variant in results:\n",
    "    output = calcResultsMinistries(results[variant])\n",
    "    with open('..\\\\data\\\\results\\\\min_old_' + variant + '.json', 'w') as f:\n",
    "        json.dump(output, f)\n",
    "    print('______________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "biblical-scope",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wvc',\n",
       " 'algemene',\n",
       " 'zaken',\n",
       " 'binnenlandse',\n",
       " 'zaken',\n",
       " 'koninkrijksrelaties',\n",
       " 'buitenlandse',\n",
       " 'zaken',\n",
       " 'defensie',\n",
       " 'economische',\n",
       " 'zaken',\n",
       " 'klimaat',\n",
       " 'financiën',\n",
       " 'infrastructuur',\n",
       " 'waterstaat',\n",
       " 'justitie',\n",
       " 'veiligheid',\n",
       " 'landbouw',\n",
       " 'natuur',\n",
       " 'voedselkwaliteit',\n",
       " 'onderwijs',\n",
       " 'cultuur',\n",
       " 'wetenschap',\n",
       " 'sociale',\n",
       " 'zaken',\n",
       " 'werkgelegenheid',\n",
       " 'volksgezondheid',\n",
       " 'welzijn',\n",
       " 'sport',\n",
       " 'az',\n",
       " 'bzk',\n",
       " 'bz',\n",
       " 'def',\n",
       " 'ez',\n",
       " 'fin',\n",
       " 'i&w',\n",
       " 'j&v',\n",
       " 'lnv',\n",
       " 'ocw',\n",
       " 'szw',\n",
       " 'vws',\n",
       " 'ezk']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-danish",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It is easy to find correct matches for ministeries when using gazetteers. The precision of this test is 0.882. Most all of the matches that were found are actually ministeries. However the recall of the matcher is very bad at 0.377. The matcher doesnt even find half of all the ministeries that were in the text. The reason for this is clear. It has to do with summations as abbriviations. \n",
    "\n",
    "Within the texts, a lot of summations of ministeries are used. For example: \"de ministeries van VWSS, JenV, en BZK\". In this case none of these will abbreviations of ministeries will be matched. The matcher looks for \"ministerie van {name of ministerie}\". VWSS will not be matched because ministeries is plural in stead of singular and JenV and BZK will not be matched because these are not preceded by \"ministerie van \". A solution for this could be to not look for the preceding \"ministerie van \", however, if this is done, all of those abbriviations will be matched outside of context. The abbriviation of the ministery of defence for example, is \"def\" so the matcher will find all occurences of the three letters \"def\". With this modification the recall will actually increase to a whopping 0.458 but the precision will decrease to a meager 0.122. So this is not a solution. \n",
    "\n",
    "I also tested if the it would help to at least find the VWSS in the example by also looking at the plural of ministerie but that decreased the precision more than it increased the recall. \n",
    "\n",
    "The last solution is the \"best\". This completely ignores the abbriviations and just looks for the names if the ministeries without the preceding \"ministerie van \". This actually increases the recall to 0.435 but it also decreases the precision to 0.732. This is because it will also find random mentions of the words \"financien\" and \"defensie\" and others. I don't think that that is worth it. In this case I would rather have more confidance in what I what I find to be correct than finding everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-minnesota",
   "metadata": {},
   "source": [
    "Now for the best extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-mistress",
   "metadata": {},
   "source": [
    "## Labeling new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amended-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function allows a user to label new data\n",
    "def showTokens(text, allMinisteries):\n",
    "\n",
    "    try:\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "    except:\n",
    "        pass\n",
    "    doc = nlp(str(text))\n",
    "    # get list of entities as strings\n",
    "    listTokens = [str(ent.text) for ent in doc.ents]\n",
    "\n",
    "    # control variables\n",
    "    nToken = 0\n",
    "    tempTokens = []\n",
    "    lenTemp = 0\n",
    "\n",
    "    # loop over tokens\n",
    "    for i in range(len(doc)):\n",
    "\n",
    "        # get string representation of token and add to a temporary list with token index\n",
    "        t = str(doc[i])\n",
    "        tempTokens.append((t, nToken))\n",
    "\n",
    "        # increase n tokens and length of all strings in tempTokens\n",
    "        nToken += 1\n",
    "        lenTemp += (len(t) + 1)\n",
    "\n",
    "        # once lenght of all strings in tempTokens is 100 or larger, start printing the line\n",
    "        if lenTemp >= 100 or i >= len(doc) - 1:\n",
    "\n",
    "            # print the index for every token\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # if index number is more chars than the token, dont print the index number\n",
    "                if len(word[0]) < len(str(word[1])):\n",
    "                    print(' ' * (len(word[0]) + 1), end='')\n",
    "\n",
    "                # else just print index number plus a number of spaces\n",
    "                else:\n",
    "                    print(word[1], ' ' * (len(word[0]) - len(str(word[1]))), end='')\n",
    "            print('')\n",
    "\n",
    "            # print all tokens\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # highlight tokens in red if named entity\n",
    "                if word[0].lower() in allMinisteries + ['ministerie', 'ministeries']:\n",
    "                    printHilight(word[0])\n",
    "                else:\n",
    "                    print(word[0] + ' ', end='')\n",
    "\n",
    "            # add some space between lines\n",
    "            print('\\n\\n')\n",
    "\n",
    "            # reset control variables\n",
    "            tempTokens = []\n",
    "            lenTemp = 0\n",
    "    \n",
    "    # inputs:\n",
    "    # no input: next page\n",
    "    # q: quit \n",
    "    # remove: remove last label\n",
    "    # number1 number2: adds a label from span number1 to number2\n",
    "    # everything else is invalid, you will be prompted again\n",
    "    \n",
    "    \n",
    "    newEnts = []\n",
    "    while True:\n",
    "        x = input()\n",
    "        if x == '':\n",
    "            break\n",
    "        elif x == 'q':\n",
    "            raise Exception(\"Stopped the program\")\n",
    "        elif x == 'remove':\n",
    "            del newEnts[-1]\n",
    "            print('Removed')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x = x.split()\n",
    "            newEnts.append(Span(doc, int(x[0]), int(x[1]), label='MINISTERIE'))\n",
    "            print(doc[int(x[0]):int(x[1])])\n",
    "        except:\n",
    "            print('Two numbers seperated by a space')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        doc.ents = newEnts\n",
    "        clear_output()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statewide-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showText(df, n):\n",
    "    \n",
    "    ministeries = getMinisteries()\n",
    "    abrr = [ministeries[x]['Abbriviation'] for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    allMinisteries = list(ministeries.keys()) + abrr\n",
    "    allMinisteries = [x.lower() for x in allMinisteries]\n",
    "    \n",
    "    samples = df.sample(n)\n",
    "    docs = list(samples.text.apply(lambda x: showTokens(x, allMinisteries)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for i in range(30):\n",
    "    docs += showText(df, 10)\n",
    "    print(f'done {i + 1}0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\train2.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\dev2.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "planned-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-costs",
   "metadata": {},
   "source": [
    "## Evaluating new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "french-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMatchesMinistries(doc, minList):   \n",
    "    indexOfMatches = []\n",
    "    for ent in doc.ents:\n",
    "        for i in range(int(ent.start), int(ent.end)):\n",
    "            indexOfMatches.append(i)\n",
    "\n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "\n",
    "    for token in doc:\n",
    "        flag = False\n",
    "        for mini in minList:\n",
    "            if token.text.lower() == mini:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "            \n",
    "        elif flag:\n",
    "            printHilightUnderline(str(token.text))\n",
    "            \n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "varying-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateMinistries(nlp):\n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that\n",
    "    # isnt a mactch, \n",
    "    if os.path.isfile('..\\\\data\\\\results\\\\ministries_results,json'):\n",
    "        with open('..\\\\data\\\\results\\\\ministries_results,json') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'partial':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'abbr' :0,\n",
    "            'ocr':0,\n",
    "            'whitespace':0,\n",
    "            'minister':0,\n",
    "            'other':0\n",
    "        }\n",
    "    \n",
    "    minList = set(getMinisteries(True))\n",
    "    while(True):\n",
    "        try:\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(sum(list(results.values())[:4] ))\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            showMatchesMinistries(doc, minList)\n",
    "\n",
    "            for case in list(results.keys())[:10]:\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "\n",
    "            clear_output()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "available-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "306d4b337a5d407eb00c8845884c82fd_rivm-maart-2020-documenten 2674 \n",
      "\n",
      "\n",
      "( instabiliteit , onveiligheid , \u001b[4m\u001b[96meconomische\u001b[96m\u001b[0m verstrengeling met elkaar , toegang tot markten in de toekomst ) moeten we ons ook \n",
      " bezig houden met een internationale aanpak . We moeten dit nu internationaal regelen met het IMF , Wereldbank en de EU , \n",
      " dan zullen we nog heel lang de gevolgen hiervan plukken . \n",
      " * “ Binnen het budget dat ik heb , kijken we naar andere middelen . We kijken ook juist naar hoe het op een Marshall-achtige \n",
      " fund aangepakt kan worden door het IMF . ” \n",
      " « Social distancing is een basisconcept dat in vluchtelingenkampen en favela's wegvalt . Structurele armoede en slecht bestuur \n",
      " is een giftige mengeling . \n",
      " s De cyclus van een aanpak van corona , morgen misschien een ander virus , is ook afhankelijk van de aanpak in de zwakste \n",
      " schakel in de keten . Wervenheid van reizen , werken , denken en mensen , moet je ook aan de zwakste schakel werken . Groot \n",
      " punt van zorg . Ook delen van lessen en ervaringen met ebola is belangrijk . Ergens in de uitzending noemde ze ook het belang \n",
      " van delen van data en informatie . \n",
      " « Wat kan NL doen ? Gaat om primaire gezondheidszorg : moet je denken aan verplegers , gemeenschapsinformatie ( veel \n",
      " goede maatschappelijke organisaties uit NL doen aan bevolkingsvoorlichting - was je handen etc ) en blijven kijken naar andere \n",
      " noden . Groot punt van zorg is dat de hiv-patiënten aan de kant worden geschoven doordat de financiering er alleen maar naar \n",
      " bestrijding van corona gaat . Dus we moeten blijven kijken naar de kwetsbaarheid van de mens en zorg dat de economieën niet \n",
      " alleen draaiende blijven , maar armoede aanpakt . \n",
      " Wie beslist er ? \n",
      " Dan nog korte uitleg over de organen die de adviezen voorbereiden en nemen . Of kijk dit filmpje . ‘ De Lijn ’ volgens het Nationaal \n",
      " Handboek Crisisbesluitvorming is daarin leidend . De rolverdeling is namelijk : deskundigen adviseren , ambtenaren formuleren \n",
      " mogelijke maatregelen en politici besluiten welke genomen worden . Het is een drietrapsraket , zodat artsen niet gaan over beleid en \n",
      " politici niet gaan shoppen in wetenschappelijke gegevens . \n",
      " Volgorde : Outbreak Management Team ( medisch advies ) 3 Interdepartementale Commissie Crisisbeheersing ( maatregelen en \n",
      " beleidsadvies ) > Ministeriële Commissie Crisisbeheersing ( knopen doorhakken ) \n",
      " 1 . Het Outbreak Management Team \n",
      " « Het OMT komt bij elkaar op het moment dat ‘ bestaande draaiboeken te weinig houvast bieden ’ bij de uitbraak van een \n",
      " infectieziekte . De vorige keer dat dat gebeurde , was in 2009 bij de Mexicaanse griep . Vanwege Covid-19 riep \n",
      " de directeur van het Centrum Infectieziektebestrijding van het RIVM , het OMT op 24 januari voor het eerst bij elkaar . \n",
      " « Het overleg is vertrouwelijk , leden mogen niet uit de school klappen . Of er pittige discussies waren , of er leden zijn die \n",
      " twijfelen aan het advies : dat blijft binnenskamers . Hoewel de leden zijn uitgenodigd omdat ze bestuurder of expert zijn bij een \n",
      " medische vereniging of wetenschappelijk instituut , spreken ze op persoonlijke titel . Zo kunnen ze snel beslissen . Alleen het \n",
      " advies wordt openbaar . \n",
      " * Slechts twee van de leden van het team zijn veel in de media te zien : Van Dissel is voorzitter . Secretaris is UJES , hoofd \n",
      " van een belangrijk onderdeel van dat centrum . \n",
      " * Rondom de andere leden is enige geheimzinnigheid . Hun namen ontbreken op openbare stukken . Een van vaste leden is in \n",
      " elk geval viroloog van het Leids Universitair Medisch Centrum , en voorzitter van de Nederlandse Vereniging voor \n",
      " Medische Microbiologie . Zij komt ook op tv : ze schoof al eens aan bij Jinek om uitleg te geven over het virus , en deed mee aan \n",
      " een van de live-uitzendingen van de NOS . Ze doet onder andere onderzoek naar het herpesvirus . \n",
      " * Annelies Verbon , hoogleraar infectieziekten bij Erasmus MC , is lid namens de Nederlandse Vereniging voor Internist- \n",
      " Infectiologen . Zij is onder meer gespecialiseerd in het hiv-virus en het gebruik van antibiotica , en treedt weinig op in de media . \n",
      " * Van andere vaste deelnemers is alleen bekend namens welke organisatie ze aanschuiven . Volgens een woordvoerder van \n",
      " het Nederlands Huisartsen Genootschap is het ‘ vertrouwelijk ’ wie namens hen in het OMT zit . Andere vaste deelnemers \n",
      " komen van het Nederlands Centrum voor Beroepsziekten van het Amsterdam UMC en het Landelijk Overleg \n",
      " Infectieziektebestrijding van de GGD's . \n",
      " * Naast deze vaste leden doen bij iedere vergadering andere specialisten mee , afhankelijk van het onderwerp . Hoe verder de \n",
      " crisis zich ontwikkelt , des te groter wordt de groep , zo blijkt uit de adviezen die het OMT uitbrengt . Bij het vorige overleg zaten \n",
      " naast de vaste leden ook longartsen , intensive-care-artsen , ouderengeneeskundigen , een arts-microbioloog , iemand van een \n",
      " laboratorium en verschillende andere GGD'ers , RIVM-medewerkers en specialisten . Dat overleg vindt nu grotendeels via \n",
      " inbellen plaats . Zo kunnen ook de leden van het OMT voldoen aan het eigen advies : houdt minstens 1,5 meter afstand van \n",
      " elkaar . \n",
      " « Het RIVM krijgt haar info van huisartsenpraktijken en zorgverleners die verplicht zijn om alle coronagevallen te melden in \n",
      " het systeem . \n",
      " 2 . Interdepartementale Commissie Crisisbeheersing \n",
      " 371919 \n",
      " precision = 0.96875, recall = 0.6549295774647887, f1 = 0.7815126050420168\n",
      "{'correct': 178, 'incorrect': 0, 'partial': 16, 'missing': 106, 'spurious': 14, 'abbr': 18, 'ocr': 0, 'whitespace': 5, 'minister': 6, 'other': 91, 'precision': 0.96875, 'recall': 0.6549295774647887, 'f1': 0.7815126050420168}\n"
     ]
    }
   ],
   "source": [
    "results = evaluateMinistries(nlp_ministries)\n",
    "\n",
    "results = calcResultsMinistries(results)\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\ministries_results,json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-consultancy",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Missing things in situations like this: RVO/LNV. LNV should be have been caught here. This might be caused by the fact that spacy works with tokens/words in stead of individual letters. Places with a lot of extra newlines or spaces within the name of a ministry will also trip up the model. No context also doesnt help\n",
    "\n",
    "when training I specificly excluded the minister variants of the ministries. Therefore these aren't counted as missing if the model doesn't find them but will be counted as spurious if the the model does find them.\n",
    "\n",
    "The data the model was trained on means that it easily recoginizes ministries that have a lot to do with covid like volksgezondheid, but less so for others like landbouw. EZK (economics and climate) specifically doesn't want to be captured\n",
    "\n",
    "The type of text doesnt lend itself very well to NER. A lot of times the abbriviations of the ministries are listed without any contex to what it is. NER uses information about the surrounding words to predict what the word is. If there are no surrounding words, no context, predictions are going to be difficult\n",
    "\n",
    "ministerie SZW en EZK was counted as two partially correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-provincial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
