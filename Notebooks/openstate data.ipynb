{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hindu-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "import PyPDF2\n",
    "import tabula\n",
    "\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reduced-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-intro",
   "metadata": {},
   "source": [
    "## Request metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-vacation",
   "metadata": {},
   "source": [
    "Metadata about wob requests can be usefull to collect. Luckily, when the government agency to which the request was send completes the request, it also gives a decision document and a inventory list in addition to the documents that were requested. With these two documents the reason for a request, the date on which the request was received and completed, the number of documents concidered, the number of documents (partially) released, and the number of documents not released can be extracted.\n",
    "\n",
    "# Reason for request\n",
    "The reason for the wob request can be found in the decision document. In this document it states in one sentence a summary of what has been requested. This summary is what needs to be extracted. This summary is usually indicated by a keyword or keywords. These keywords come down to dutch versions of \"requested\", \"information about\", or \"publication of\". What follows is a list of all keywords used in dutch:\n",
    "- verzocht\n",
    "- u verzoekt\n",
    "- om informatie over\n",
    "- uw verzoek ziet\n",
    "- om openbaarmaking van\n",
    "- more to come\n",
    "When one of these keywords are found, all following text is extracted until a the next period occurs. To do this, a regular expression was used. \n",
    "\n",
    "keyword([^.]+?)\\\\.\n",
    "\n",
    "Where keyword is one of the words or phrases listed above. The expression first finds one of these keywords and then matches any alpha-numerical character until the first period is found. Before the regular expression can be used however, the text needs some preprocessing. First, exessive newlines are removed. Second, all letters are converted to lowercase letters. This is done so that the regular expression will match the keywords even though in the text the the keywords are written with capitalization. This has to be done as regular expressions are case sensitive. Last, for any word or abbreviation in the text that includes a period where the period does not indicate the end of a sentence, said period have to be removed otherwise it will trip up the regular expression. After this, the neccessary information can be extracted.\n",
    "\n",
    "After the regular expressions have been run, the matches need to be checked for duplicates. This can happen when the decision document states for example \"the request if for information about [...]\". In this case the same sentence would be matched for the keyword \"request\" and \"information about\". This would match the same sentence twice so only one of these is needed and the other is removed.\n",
    "\n",
    "To evaluate the extractor, a different method than the other extractors was used: the Intersection over Union (IoU). With this metric precision, recall, and F1 scores are still calculated, but with different method. It uses the overlap between the ground truth and the prediction to measure the performance of a model. First the intersect and union of the ground truth and prediction are calculated. The intersect is the overlap between the set of words in the extraction and the set of words in the ground truth. The union is the combination of the two. With this the IoU metric is calculated. Using the panoptic segmentation metric \\cite{Mechea_2019}, IoU can then be compared to a threshold value. If the IoU is higher then the treshold it counts as a true positive, if it is lower it counts as both a false negative and a false positive. Presicion, recall and F1 score can then be calculated.\n",
    "\n",
    "# Relevant dates\n",
    "There are two relevant dates that can be extracted from the decision document: the date on which the request was received and the date on which the request was completed. The decision document has these two dates at the beginning. The completion date is same date as when the document was made so that is always the very first date in the document. The date of request is always in the first sentence of the document as they all start with as follows: \"In your letter of 01 januari 2022\". This means that the first and second date that are found in the document are the relevant dates to extract. Sometimes the date a request was send is not the same as the date the request was received. This happens when the request is send by post. In this case the decision document states: \"in your letter of 01 januari 2022, received on 05 januari 2022\". The following regular expression was used to check if this is the case:\n",
    "\n",
    "'ontvangen op ([^.]+?)\\,'\n",
    "\n",
    " In this case the first and third date are the relevant dates. To actually extract the dates, the dates extractor described here TODO[REF HERE] was used. These dates can then also be used to check how long the request took to fulfull and if it was done in the time that they have. \n",
    "\n",
    "\n",
    "# Number of documents\n",
    "The inventory lists contain an overview of all documents that were found that fall in the scope of the request. The list also has information about which documents were made public, which were made partially public, which were not made public and also the documents that were already public. \"Openbaar\" or \"volledig openbaar\" for public, \"deels openbaar\" or \"gedeeltelijk openbaar\" for partially public, and \"niet openbaar\", \"reeds openbaar\", or \"geweigerd\" for not public. The sum of these can be used to find the total number of documents considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-concern",
   "metadata": {},
   "source": [
    "## Results\n",
    "The request metadata extractors in \\hyperref[tab:table7]{table 7} show very varying results. All the extractors that had to do with dates (Date received, date fulfilled, number of days taken, and completed in time) all show good performance in the F\\textsubscript{1} score. However it is worth to mention that this is for the most part due to the recall for all of these being 1. This means that the extractor never found a match when there was no ground truth match to be found. The precision is a lot lower meaning that it wasn't correct all of the time. \n",
    "\n",
    "The extractors that had to do with the number of documents preformed really low. These are: Documents considered, days taken per document, Number of public documents, Number of partially public documents, and Number of not public documents. This is mostly because of the fact that the inventory lists are necessary to calculate these and of the 1045 requests in the dataset only 256 had an one. Besides that, the way the data was stored doesn't lend itself well to extraction. Its stored in tables within a PDF documents. There are methods to retrieve the tables with Python (like Tabula used in this thesis) however these methods are far from foolproof and don't always work. Even if the table is retrieved, there is no consistency between ministries on how to make these tables which adds another layer of complexity.\n",
    "\n",
    "The reason for request extractor does show good results with a precision of 0.722 it correctly identified almost three quarters of request reasons. The recall is higher at 0.878. These figures give an F\\textsubscript{1} score of 0.793\n",
    "\n",
    "\n",
    "One problem the used approach is sensetive to is mistakes made in the OCR process. The regular expressions look some keywords and, when those are found, the end of the sentence. If in the OCR process a mistake was made in on of the keywords or if the period at the end of the sentence wasn't recognized as a period, the regular expression won't match it even though it should. \n",
    "\n",
    "Another problem is the generalizibility. These extractors were made specificly for desicion documents coming from wob requests to dutch government ministries. They will not work for wob request to provinces or muninipalities. They use different standards which do not work with the current extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simplified-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the dirs id wob request as 1, 2,3 and some do it as 1.0, 2.0, 3.0\n",
    "# this makes all of the dirs use the first method\n",
    "def fixNamingSceme():\n",
    "    base = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "    for dir in os.listdir(base):\n",
    "        for folder in os.listdir(base + dir):\n",
    "            if os.path.isdir(base + dir + '\\\\' + folder):\n",
    "                os.rename(base + dir + '\\\\' + folder, base + dir + '\\\\' + folder.split('.')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sexual-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates matcher\n",
    "def getDates(text, nlp):\n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'okt']\n",
    "    days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "       'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "\n",
    "    datesPattern = [ \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = removeKenmerkDate(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    return [doc[start:end].text for match_id, start, end in matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beautiful-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the extractor used periods as indicators, abbreviations like N.V.T. need to be removed\n",
    "# this function converts it to N V T while keeping the periods at the end of sentences\n",
    "def removeAbbreviation(text):\n",
    "    sentence = ''\n",
    "\n",
    "    # split sentence in words\n",
    "    for word in text.split(' '):\n",
    "\n",
    "        # if there is no period in the word, add it back to sentence\n",
    "        if word.count('.') == 0 or '\\n' in word:\n",
    "            sentence += word + ' '\n",
    "            continue\n",
    "        \n",
    "        # when there is one period in word\n",
    "        elif word.count('.') == 1:\n",
    "\n",
    "            # if it is at the end, keep it and add word to sentence\n",
    "            if word[-1] == '.':\n",
    "                sentence += word + ' '\n",
    "            \n",
    "            # if its in the middle replace it with a space\n",
    "            else:\n",
    "                word = word.replace('.', ' ')\n",
    "                sentence += word + ' '\n",
    "        \n",
    "        # if there are  more than 1 periods, replace them all\n",
    "        else:\n",
    "            word = word.replace('.', ' ')\n",
    "            sentence += word + ' '\n",
    "            \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "welcome-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRequestReason(text, removeAbbr = True):\n",
    "\n",
    "    # text = rawText.replace('\\n', ' ')\n",
    "    text = text.lower()\n",
    "    if removeAbbr:\n",
    "        text = removeAbbreviation(text)\n",
    "\n",
    "    patterns = ['verzocht([^.]+?)\\.', \n",
    "                'u verzoekt([^.]+?)\\.', \n",
    "                'om informatie over([^.]+?)\\.', \n",
    "                'uw verzoek ziet([^.]+?)\\.', \n",
    "                'om openbaarmaking van([^.]+?)\\.']\n",
    "    matches = []\n",
    "\n",
    "    # get matches for all keywords\n",
    "    for pattern in patterns:\n",
    "        matches += re.findall(pattern, text)\n",
    "\n",
    "    uniqueMatches = []\n",
    "\n",
    "    # check all matches against eachother\n",
    "    for i in range(len(matches)):\n",
    "        for j in range(len(matches)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # if there is a duplicate we do not add it to uniqueMatches\n",
    "            if matches[i] in matches[j]:\n",
    "                break\n",
    "\n",
    "        # add match to uniqueMatches if the j loop completes\n",
    "        else:\n",
    "            uniqueMatches.append(matches[i])\n",
    "\n",
    "    return matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "knowing-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets a random decision doc from a given ministry\n",
    "def getRandomDecisionDoc(ministrie):\n",
    "\n",
    "    # set paths \n",
    "    baseDFPath = '..\\\\data\\\\openstate data\\\\'\n",
    "    basePDFPath = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "\n",
    "    # get df of ministry\n",
    "    for f in os.listdir(baseDFPath):\n",
    "        if ministrie in f.lower():\n",
    "            file = f\n",
    "            break\n",
    "\n",
    "    # get dir of ministry \n",
    "    for d in os.listdir(basePDFPath):\n",
    "        if ministrie in d.lower():\n",
    "            dir = basePDFPath + d + '\\\\'\n",
    "            break\n",
    "    \n",
    "    requests = [x for x in os.listdir(dir) if x != '.DS_Store']\n",
    "    requestNr = int(random.choice(requests))\n",
    "    \n",
    "    # load in dataframe of ministry and get random sample\n",
    "    testDf = pd.read_excel(baseDFPath + file)\n",
    "    testDf.columns = [x.replace('\\n', '') for x in testDf.columns]    \n",
    "    s = testDf[testDf['WOB Verzoek'] == requestNr]\n",
    "\n",
    "    if not os.path.exists(dir + str(requestNr)):\n",
    "        return None, None, None\n",
    "\n",
    "    # find the desicion document\n",
    "    for p in os.listdir(dir + str(requestNr)):\n",
    "\n",
    "        # if found, save it in pdfPath\n",
    "        if 'besluit' in p.lower() and 'bijlage' not in p.lower() and p.endswith('.pdf'):\n",
    "            pdfPath = p\n",
    "            break\n",
    "    \n",
    "    # if there is no desicion document, try again\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "    return s, dir + str(requestNr) + '\\\\', pdfPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grateful-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDate(date, timestamp=False):\n",
    "    months = {\n",
    "        'januari':'01','jan':'01',\n",
    "        'februari':'02','feb':'02',\n",
    "        'maart':'03','mrt':'03',\n",
    "        'april':'04','apr':'04',\n",
    "        'mei':'05','mei':'05',\n",
    "        'juni':'06','jun':'06',\n",
    "        'juli':'07','jul':'07',\n",
    "        'augustus':'08','aug':'08',\n",
    "        'september':'09','sep':'09',\n",
    "        'oktober':'10','okt':'10',\n",
    "        'november':'11','nov':'11',\n",
    "        'december':'12','dec':'12'\n",
    "    }\n",
    "\n",
    "\n",
    "    date = date.split(' ')\n",
    "    date[1] = months[date[1].lower()]\n",
    "\n",
    "    if timestamp:\n",
    "        return pd.Timestamp(year=int(date[2]), month=int(date[1]), day=int(date[0]))\n",
    "    if len(date[0]) == 1:\n",
    "        date[0] = '0' + date[0]\n",
    "    return date[2] + '-' + date[1] + '-' + date[0]\n",
    "\n",
    "\n",
    "def days_between(d1, d2):\n",
    "    d1 = convertDate(d1)\n",
    "    d2 = convertDate(d2)\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return abs((d2 - d1).days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "violent-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textExtract(dir, name, startPage = 0, endPage = 10):\n",
    "    txtName = '.'.join(name.split('.')[0:-1])\n",
    "    txtName = dir + txtName + f'-pages{startPage}-{endPage}' + '.txt'\n",
    "\n",
    "    # extract text\n",
    "    os.system(f'pdftotext -f {startPage} -l {endPage} -raw \"{dir}{name}\" \"{txtName}\"')\n",
    "    \n",
    "    if not os.path.exists(txtName): \n",
    "        return None\n",
    "    \n",
    "    # open file and return content\n",
    "    with open(txtName, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "critical-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateInformation(text):        \n",
    "    # find dates in text with date extractor\n",
    "    matches = getDates(text, nlp)\n",
    "\n",
    "    # first found date is date when document was written\n",
    "    # the date the wob request was completed\n",
    "    if len(matches) < 3:\n",
    "        return None, None, None, None\n",
    "    completedDate = matches[0]\n",
    "\n",
    "    # check if request was received on a different date then when it was send\n",
    "    # this is the case if it states \"ontvangen op\"\n",
    "    receivedDate = re.findall('ontvangen op ([^.]+?)\\,', text)\n",
    "    if not receivedDate:\n",
    "        receivedDate = matches[1]\n",
    "    else:\n",
    "        receivedDate = matches[2]\n",
    "\n",
    "    # calculate days between request and completion\n",
    "    daysTaken = days_between(completedDate, receivedDate)\n",
    "    \n",
    "    # converts to yyyy-mm-dd\n",
    "    start = convertDate(receivedDate)\n",
    "    end = convertDate(completedDate)\n",
    "    print(start, end)\n",
    "    \n",
    "    # converts to np.datetime \n",
    "    start = np.datetime64(start, format='Y%-%m-%d')\n",
    "    end = np.datetime64(end, format='Y%-%m-%d')\n",
    "    \n",
    "    # check if request was fulfilled wihtin 42 business days\n",
    "    businessDaysTaken = np.busday_count(start, end)\n",
    "    inTime = businessDaysTaken <= 42\n",
    "    return receivedDate, completedDate, daysTaken, inTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wanted-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDocs(text):\n",
    "\n",
    "    # look for the first mention of a number of documents\n",
    "    nDocuments = re.findall('[0-9]+? documenten[a-z]{2} aangetroffen', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        nDocuments = re.findall('[0-9]+? document[a-z]{2}', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        if len(re.findall('één document', text)) == 1:\n",
    "            return 1\n",
    "\n",
    "    # return the first if found, else return None\n",
    "    if type(nDocuments) == list:\n",
    "        if len(nDocuments) == 0:\n",
    "            return None\n",
    "        return int(nDocuments[0].split(' ')[0])\n",
    "    elif type(nDocuments) == int:\n",
    "        return nDocuments\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "capital-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocByNumber(ministry, number):\n",
    "    base = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "    for d in os.listdir(base):\n",
    "        if ministry in d.lower():\n",
    "            ministryDir = d\n",
    "            break\n",
    "\n",
    "    dir = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\' + ministryDir + '\\\\' + number + '\\\\'\n",
    "    \n",
    "    for p in os.listdir(dir):\n",
    "\n",
    "        # if found, save it in pdfPath\n",
    "        if 'besluit' in p.lower() and 'bijlage' not in p.lower():\n",
    "            return textExtract(dir, p, 11, 13)\n",
    "    \n",
    "    # if there is no desicion document, return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prostate-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of pages in pdf documents\n",
    "def getNumberOfPages(path):\n",
    "    nPages = 0\n",
    "\n",
    "    # gets list of pdf files in a directory\n",
    "    pdfs = [x for x in os.listdir(path) if x.endswith('.pdf')]\n",
    "    \n",
    "    # open all files and count pages\n",
    "    for file in pdfs:\n",
    "        try:\n",
    "            with open(path + file, 'rb') as f:\n",
    "                pdf = PyPDF2.PdfFileReader(f, strict=False)\n",
    "                nPages += pdf.numPages\n",
    "        except:\n",
    "            return None\n",
    "    return nPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demonstrated-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory(path, pdf):\n",
    "\n",
    "    # words to look for\n",
    "    rating = {'deels openbaar':0, 'niet openbaar':0, 'openbaar':0, 'reeds openbaar':0, \n",
    "            'geweigerd':0, 'gedeeltelijk openbaar':0,\n",
    "            'volledig openbaar': 0}\n",
    "    \n",
    "    # if an inventory document exists, use that\n",
    "    for file in os.listdir(path):\n",
    "        if 'inventaris' in file.lower():\n",
    "            rating = inventoryListToDataframe(path + file, rating)\n",
    "            break\n",
    "    \n",
    "    # else try to find inventory table in decision doc\n",
    "    if not rating:\n",
    "        rating = inventoryListToDataframe(path + pdf, rating)\n",
    "\n",
    "    if not rating:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # combine categories\n",
    "    notPublic = rating['niet openbaar'] + rating['geweigerd'] + rating['reeds openbaar']\n",
    "    partialPublic = rating['deels openbaar'] + rating['gedeeltelijk openbaar']\n",
    "    public = rating['openbaar'] - rating['niet openbaar'] - rating['reeds openbaar'] - partialPublic\n",
    "    total = public + notPublic + partialPublic\n",
    "    print(rating)\n",
    "    if total == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    return public, notPublic, partialPublic, public + notPublic + partialPublic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accurate-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeKenmerkDate(text):\n",
    "    text = text.split('\\n')\n",
    "    \n",
    "    toRemove = None\n",
    "    for i in range(len(text)):\n",
    "        if '(kenmerk)' in text[i] and 'ons' not in text[i]:\n",
    "            toRemove = [i, i+1]\n",
    "            break\n",
    "            \n",
    "    if toRemove:\n",
    "        text.remove(text[toRemove[0]])\n",
    "        text.remove(text[toRemove[1]])\n",
    "\n",
    "    text = '\\n'.join(text)                        \n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "working-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'F:\\Data files\\Master thesis\\verzoeken\\WOB-verzoeken OCW\\2\\\\'\n",
    "# pdf = 'Bij-Besluit+Wob-verzoek+inzake+documentaire+Sigrid+Kaag+van+Beiroet+tot+Binnenhof+1.pdf'\n",
    "# inventory(path, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    " def inventoryListToDataframe(pdf, rating):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(pdf, pages='all')\n",
    "\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            return None\n",
    "\n",
    "        for table in tables:\n",
    "            for col in table.columns:\n",
    "                col = list(table[col])\n",
    "                col = [str(x).lower() for x in col]\n",
    "                for key in rating:\n",
    "                    for value in col:\n",
    "                        if key in value:\n",
    "                            rating[key] += 1\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "authentic-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds intersection between two strings\n",
    "def findLongestCommenSubstring(string1, string2):\n",
    "    longestSubstring = ''\n",
    "    nChecked = 0\n",
    "    if len(string1) > len(string2):\n",
    "        base = string1\n",
    "        query = string2\n",
    "    else:\n",
    "        base = string2\n",
    "        query = string1\n",
    "        \n",
    "    for i in range(len(query)):\n",
    "        for j in range(len(query) - i):\n",
    "            substring = query[i:len(query) - j]\n",
    "            \n",
    "            if len(substring) < len(longestSubstring):\n",
    "                break\n",
    "            \n",
    "            if substring in base:\n",
    "                longestSubstring = substring\n",
    "                break\n",
    " \n",
    "    return longestSubstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divided-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIoU(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "\n",
    "    intersection = 0\n",
    "\n",
    "    for value in a:\n",
    "        if value in b:\n",
    "            intersection += 1\n",
    "\n",
    "    union = len(a) + len(b) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "north-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts description of n documents to int\n",
    "def getSumOfNumbersFromString(string):\n",
    "    try:\n",
    "        if type(string) == str:\n",
    "            return sum([int(s) for s in string.split() if s.isdigit()])\n",
    "        elif type(string) == float or type(string) == int:\n",
    "            return int(string)\n",
    "    except:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "polish-observer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws', 'corona']\n",
    "    for min in mins:\n",
    "        text = ''\n",
    "\n",
    "        # find a document with at least 20 chars\n",
    "        while len(text) < 20:\n",
    "            clear_output()\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(min)\n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')\n",
    "\n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "        \n",
    "        reason = getRequestReason(text)\n",
    "        receivedDate, completedDate, daysTaken, inTime = dateInformation(text)\n",
    "        nDocuments = nDocs(text)\n",
    "        nPages = getNumberOfPages(pdfPath)\n",
    "        public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "        \n",
    "        if daysTaken and nDocuments:\n",
    "            daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "        elif total and daysTaken:\n",
    "            daysPerDoc = round(daysTaken / total, 2)\n",
    "        else:\n",
    "            daysPerDoc = None\n",
    "\n",
    "        desc = groundTruth['Soort aanvraag'].values[0]\n",
    "        received = groundTruth['Datum van binnenkomst'].values[0]\n",
    "        awnser = groundTruth['Datum van antwoord'].values[0]\n",
    "        daysTakenGT = groundTruth['Aantal dagen in behandeling'].values[0]\n",
    "        inTimeGT = groundTruth['Binnen de termijn afgehandeld'].values[0]\n",
    "        nDocsConsidered = groundTruth['Aantal overwogen documenten'].values[0]\n",
    "        daysPerDocGT = groundTruth['Aantal dagen nodig gehad per document'].values[0]\n",
    "        nPagesGT = groundTruth[\"Omvang document (aantal pagina's)\"].values[0]\n",
    "        publicGT = groundTruth[\"Volledig verstrekte documenten\"].values[0]\n",
    "        notPublicGT = groundTruth[\"Niet verstrekte documenten\"].values[0]\n",
    "        partialPublicGT = groundTruth[\"Deels verstrekte documenten\"].values[0]\n",
    "\n",
    "        print(f'Ground truth description: {desc}')\n",
    "        print(f'Extracted description: {reason}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth received: {received}')\n",
    "        print(f'Extracted received: {receivedDate}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth awnser: {awnser}')\n",
    "        print(f'Extracted awnser: {completedDate}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth days taken: {daysTakenGT}')\n",
    "        print(f'Extracted days taken: {daysTaken}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth in Time: {inTimeGT}')\n",
    "        print(f'Extracted in time: {inTime}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth n docs considered: {nDocsConsidered}')\n",
    "        print(f'Extracted n docs considered (decision doc): {nDocuments}')\n",
    "        print(f'Extracted n docs considered (inventory doc): {total}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth days per doc: {daysPerDocGT}')\n",
    "        print(f'Extracted days per doc: {daysPerDoc}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth pages: {nPagesGT}')\n",
    "        print(f'Extracted pages: {nPages}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth public docs: {publicGT}')\n",
    "        print(f'Extracted public docs: {public}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth partial docs: {partialPublicGT}')\n",
    "        print(f'Extracted partial docs: {partialPublic}')\n",
    "        print('\\n______________________________')\n",
    "        print(f'Ground truth not public docs: {notPublicGT}')\n",
    "        print(f'Extracted not public docs: {notPublic}')\n",
    "        print('\\n______________________________')\n",
    "        print('\\n\\n\\n')\n",
    "        # print(text)\n",
    "\n",
    "        x = input()\n",
    "        if x == 'q':\n",
    "            break\n",
    "        else:\n",
    "            clear_output()\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smoking-velvet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# eval with correct, spurious, and missing\n",
    "def evaluate():\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws']\n",
    "    \n",
    "    resultsDir = 'C:\\\\Users\\\\justin\\\\OneDrive - UvA\\\\Studie\\\\Data Science\\\\Thesis\\\\Knowledge extraction\\\\data\\\\results\\\\'\n",
    "    \n",
    "    # get results doc\n",
    "    if 'openstateV2.json' in os.listdir(resultsDir):\n",
    "        with open(resultsDir + 'openstateV2.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "            docsChecked = sum([results['received'][key] for key in results['received']])\n",
    "    # if theres no results doc yet, create new one\n",
    "    else:\n",
    "        docsChecked = 0\n",
    "        results = {}\n",
    "        cats = ['received', 'awnser', 'days taken', 'in time', 'docs considered', 'days per doc', 'n pages', 'public', 'not public', 'partial']\n",
    "        for cat in cats:\n",
    "            results[cat] = {'correct' : 0, 'spurious' : 0, 'missing' : 0}\n",
    "        results['reason'] = {'true positive' : 0, 'false negative' : 0, 'false positive' : 0,\n",
    "                             'correct' : 0, 'spurious' : 0, 'missing' : 0, 'partial' : 0}\n",
    "    \n",
    "\n",
    "    while docsChecked < 50:\n",
    "\n",
    "        # this selects a request with good text and retrieves the ground truth from the openstate files\n",
    "        while True:\n",
    "            ministry = random.choice(mins)\n",
    "            clear_output()\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(ministry)\n",
    "            \n",
    "            if pdfPath == None:\n",
    "                continue\n",
    "            \n",
    "            # get request number \n",
    "            requestNumber = groundTruth['WOB Verzoek'].values[0]\n",
    "            \n",
    "            # if there are NaN's in the ground truth, get a new one\n",
    "#             if groundTruth.isnull().values.any():\n",
    "#                 continue\n",
    "            \n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')   \n",
    "            \n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "            if text == None:\n",
    "                continue\n",
    "            print(text)\n",
    "            \n",
    "            # check if the text actually contains the decision document\n",
    "            goodText = input('Good text? ')\n",
    "            if goodText.lower() != 'n':\n",
    "                break\n",
    "            if goodText == 'q':\n",
    "                with open('..\\\\data\\\\results\\\\openstateV2.json', 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "                return\n",
    "            \n",
    "        docsChecked += 1\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # get reason for request\n",
    "        reason = getRequestReason(text)\n",
    "        if len(reason) == 0:\n",
    "            reason = ''\n",
    "            \n",
    "        # remove unneccesary newlines from extracted text\n",
    "        else:\n",
    "            reason = reason[0].replace('\\n', ' ')\n",
    "            \n",
    "        # extract metadata\n",
    "        receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "        nDocuments = nDocs(text)\n",
    "        nPages = getNumberOfPages(pdfPath)\n",
    "        public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "        \n",
    "        # tabula has a lot of output, this clears it\n",
    "        clear_output()\n",
    "\n",
    "        # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "        try:\n",
    "            if receivedDate:\n",
    "                receivedDate = convertDate(receivedDate, True)\n",
    "        except:\n",
    "            receivedDate = None\n",
    "        try:\n",
    "            if completedDate:\n",
    "                completedDate = convertDate(completedDate, True)\n",
    "        except:\n",
    "            completedDate = None\n",
    "\n",
    "        # if number of documents was not found in text, use total docs from inventory list\n",
    "        if not nDocuments and total:\n",
    "            nDocuments = total        \n",
    "\n",
    "        # calculate days per doc\n",
    "        if daysTaken and nDocuments:\n",
    "            daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "        elif total and daysTaken:\n",
    "            daysPerDoc = round(daysTaken / total, 2)\n",
    "        else:\n",
    "            daysPerDoc = None\n",
    "        \n",
    "        # ground truth data\n",
    "        desc = groundTruth['Soort aanvraag'].values[0]\n",
    "        received = groundTruth['Datum van binnenkomst'].values[0]\n",
    "        awnser = groundTruth['Datum van antwoord'].values[0]\n",
    "        daysTakenGT = groundTruth['Aantal dagen in behandeling'].values[0]\n",
    "        inTimeGT = groundTruth['Binnen de termijn afgehandeld'].values[0]\n",
    "        nDocsConsidered = groundTruth['Aantal overwogen documenten'].values[0]\n",
    "        daysPerDocGT = groundTruth['Aantal dagen nodig gehad per document'].values[0]\n",
    "        nPagesGT = groundTruth[\"Omvang document (aantal pagina's)\"].values[0]\n",
    "        publicGT = groundTruth[\"Volledig verstrekte documenten\"].values[0]\n",
    "        notPublicGT = groundTruth[\"Niet verstrekte documenten\"].values[0]\n",
    "        partialPublicGT = groundTruth[\"Deels verstrekte documenten\"].values[0]\n",
    "\n",
    "        # extract n docs from ground truth values\n",
    "        publicGT = getSumOfNumbersFromString(publicGT)\n",
    "        notPublicGT = getSumOfNumbersFromString(notPublicGT)\n",
    "        partialPublicGT = getSumOfNumbersFromString(partialPublicGT)\n",
    "        \n",
    "        if nDocsConsidered == 0:\n",
    "            nDocsConsidered = None\n",
    "\n",
    "        # get inTimeGT to correct true/false format\n",
    "        if inTimeGT == 'Ja':\n",
    "            inTimeGT = True\n",
    "        elif inTimeGT == 'Nee':\n",
    "            inTimeGT = False\n",
    "        else:\n",
    "            inTimeGT = None\n",
    "\n",
    "\n",
    "        values = {\n",
    "            'received' : [pd.Timestamp(received), receivedDate], \n",
    "            'awnser' : [pd.Timestamp(awnser), completedDate], \n",
    "            'days taken' : [daysTakenGT, daysTaken], \n",
    "            'in time' : [inTimeGT, inTime], \n",
    "            'docs considered' : [nDocsConsidered, nDocuments], \n",
    "            'days per doc' : [round(daysPerDocGT, 2), daysPerDoc], \n",
    "            'n pages' : [nPagesGT, nPages], \n",
    "            'public' : [publicGT, public], \n",
    "            'not public' : [notPublicGT, notPublic], \n",
    "            'partial' : [partialPublicGT, partialPublic]\n",
    "        }\n",
    "        \n",
    "        for key in values:\n",
    "            if type(values[key][0]) == float  or type(values[key][0]) == np.float64:\n",
    "                if np.isnan(values[key][0]):\n",
    "                    values[key][0] = None\n",
    "\n",
    "        print(f'Path: {pdfPath}{pdfName}')\n",
    "        print('\\n______________________________')\n",
    "        \n",
    "        # compare all extracted data to ground truth values\n",
    "        for cat in values:\n",
    "            print(f'Ground truth {cat}: {values[cat][0]}')\n",
    "            print(f'extracted {cat}: {values[cat][1]}')\n",
    "            \n",
    "            # if ground truth and extractor are equal, its correct\n",
    "            if values[cat][1] == values[cat][0]:\n",
    "                print('Correct')\n",
    "                results[cat]['correct'] += 1\n",
    "            \n",
    "            # if extractor didnt find anything, its missing\n",
    "            elif values[cat][1] == None:\n",
    "                print('Missing')\n",
    "                results[cat]['missing'] += 1\n",
    "                \n",
    "            # if they do not equal, its spurious\n",
    "            else:\n",
    "                print('Spurious')\n",
    "                results[cat]['spurious'] += 1\n",
    "            \n",
    "            print('\\n______________________________')\n",
    "        \n",
    "        # description uses different eval\n",
    "        desc = desc.replace(\"\\n\", \" \")\n",
    "        print(f'Ground truth description: {desc}')\n",
    "        print(f'Extracted description: {reason}')\n",
    "        \n",
    "        # finds the largest overlap between ground truth and extracted\n",
    "        overlap = findLongestCommenSubstring(desc.lower(), reason)\n",
    "        print(f'Overlap: {overlap}')\n",
    "        \n",
    "        # intersection is the total length of largest overlap\n",
    "        intersection = len(overlap)\n",
    "        \n",
    "        # combination is the combo between ground truth and extracted where the intersection part counts only once\n",
    "        combination = len(desc) + len(reason) - intersection\n",
    "        IoU = intersection / combination\n",
    "\n",
    "        if IoU > .2:\n",
    "            print('above threshold')\n",
    "            results['reason']['true positive'] += 1\n",
    "        else:\n",
    "            print('below threshold')\n",
    "            results['reason']['false positive'] += 1\n",
    "            results['reason']['false negative'] += 1\n",
    "\n",
    "        print('\\n______________________________')\n",
    "        print('Number of documents checked: ', docsChecked)\n",
    "        \n",
    "        flag = False\n",
    "        while True:\n",
    "            inputs = input('c for correct, i for incorrect, p for partial, m for missing, q for quit? ')\n",
    "            if inputs == 'q':\n",
    "                flag = True\n",
    "                break\n",
    "            elif inputs == 'c':\n",
    "                results['reason']['correct'] += 1\n",
    "                break\n",
    "            elif inputs == 'i':\n",
    "                results['reason']['spurious'] += 1\n",
    "                break\n",
    "            elif inputs == 'm':\n",
    "                results['reason']['missing'] += 1\n",
    "                break\n",
    "            elif inputs == 'p':\n",
    "                results['reason']['partial'] += 1\n",
    "                break\n",
    "                \n",
    "        \n",
    "        with open('..\\\\data\\\\results\\\\openstateV2.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "        if flag:\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "# evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wound-occupation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# eval with tp, fn, fp\n",
    "def evaluateV2():\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws']\n",
    "    \n",
    "    resultsDir = 'C:\\\\Users\\\\justin\\\\OneDrive - UvA\\\\Studie\\\\Data Science\\\\Thesis\\\\Knowledge extraction\\\\data\\\\results\\\\'\n",
    "    \n",
    "    # get results doc\n",
    "    if 'openstateV2.json' in os.listdir(resultsDir):\n",
    "        with open(resultsDir + 'openstateV2.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "            docsChecked = sum([results['received'][key] for key in results['received']])\n",
    "    # if theres no results doc yet, create new one\n",
    "    else:\n",
    "        docsChecked = 0\n",
    "        results = {}\n",
    "        cats = ['received', 'awnser', 'days taken', 'in time', 'docs considered', 'days per doc', 'n pages', 'public', 'not public', 'partial']\n",
    "        for cat in cats:\n",
    "            results[cat] = {'tp' : 0, 'fn' : 0, 'fp' : 0}\n",
    "    \n",
    "\n",
    "    while docsChecked < 50:\n",
    "\n",
    "        # this selects a request with good text and retrieves the ground truth from the openstate files\n",
    "        while True:\n",
    "            ministry = random.choice(mins)\n",
    "            clear_output()\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(ministry)\n",
    "            \n",
    "            if pdfPath == None:\n",
    "                continue\n",
    "            \n",
    "            # get request number \n",
    "            requestNumber = groundTruth['WOB Verzoek'].values[0]\n",
    "            \n",
    "            # if there are NaN's in the ground truth, get a new one\n",
    "#             if groundTruth.isnull().values.any():\n",
    "#                 continue\n",
    "            \n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')   \n",
    "            \n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "            if text == None:\n",
    "                continue\n",
    "            print(text)\n",
    "            print(docsChecked)\n",
    "            # check if the text actually contains the decision document\n",
    "            goodText = input('Good text? ')\n",
    "            if goodText.lower() != 'n':\n",
    "                break\n",
    "            if goodText == 'q':\n",
    "                with open('..\\\\data\\\\results\\\\openstateV2.json', 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "                return\n",
    "            \n",
    "        docsChecked += 1\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "            \n",
    "        # extract metadata\n",
    "        receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "        nDocuments = nDocs(text)\n",
    "        nPages = getNumberOfPages(pdfPath)\n",
    "        public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "        \n",
    "        # tabula has a lot of output, this clears it\n",
    "        clear_output()\n",
    "\n",
    "        # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "        try:\n",
    "            if receivedDate:\n",
    "                receivedDate = convertDate(receivedDate, True)\n",
    "        except:\n",
    "            receivedDate = None\n",
    "        try:\n",
    "            if completedDate:\n",
    "                completedDate = convertDate(completedDate, True)\n",
    "        except:\n",
    "            completedDate = None\n",
    "\n",
    "        # if number of documents was not found in text, use total docs from inventory list\n",
    "        if not nDocuments and total:\n",
    "            nDocuments = total        \n",
    "\n",
    "        # calculate days per doc\n",
    "        if daysTaken and nDocuments:\n",
    "            daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "        elif total and daysTaken:\n",
    "            daysPerDoc = round(daysTaken / total, 2)\n",
    "        else:\n",
    "            daysPerDoc = None\n",
    "        \n",
    "        # ground truth data\n",
    "        desc = groundTruth['Soort aanvraag'].values[0]\n",
    "        received = groundTruth['Datum van binnenkomst'].values[0]\n",
    "        awnser = groundTruth['Datum van antwoord'].values[0]\n",
    "        daysTakenGT = groundTruth['Aantal dagen in behandeling'].values[0]\n",
    "        inTimeGT = groundTruth['Binnen de termijn afgehandeld'].values[0]\n",
    "        nDocsConsidered = groundTruth['Aantal overwogen documenten'].values[0]\n",
    "        daysPerDocGT = groundTruth['Aantal dagen nodig gehad per document'].values[0]\n",
    "        nPagesGT = groundTruth[\"Omvang document (aantal pagina's)\"].values[0]\n",
    "        publicGT = groundTruth[\"Volledig verstrekte documenten\"].values[0]\n",
    "        notPublicGT = groundTruth[\"Niet verstrekte documenten\"].values[0]\n",
    "        partialPublicGT = groundTruth[\"Deels verstrekte documenten\"].values[0]\n",
    "\n",
    "        # extract n docs from ground truth values\n",
    "        publicGT = getSumOfNumbersFromString(publicGT)\n",
    "        notPublicGT = getSumOfNumbersFromString(notPublicGT)\n",
    "        partialPublicGT = getSumOfNumbersFromString(partialPublicGT)\n",
    "        \n",
    "        if nDocsConsidered == 0:\n",
    "            nDocsConsidered = None\n",
    "\n",
    "        # get inTimeGT to correct true/false format\n",
    "        if inTimeGT == 'Ja':\n",
    "            inTimeGT = True\n",
    "        elif inTimeGT == 'Nee':\n",
    "            inTimeGT = False\n",
    "        else:\n",
    "            inTimeGT = None\n",
    "\n",
    "\n",
    "        values = {\n",
    "            'received' : [pd.Timestamp(received), receivedDate], \n",
    "            'awnser' : [pd.Timestamp(awnser), completedDate], \n",
    "            'days taken' : [daysTakenGT, daysTaken], \n",
    "            'in time' : [inTimeGT, inTime], \n",
    "            'docs considered' : [nDocsConsidered, nDocuments], \n",
    "            'days per doc' : [round(daysPerDocGT, 2), daysPerDoc], \n",
    "            'n pages' : [nPagesGT, nPages], \n",
    "            'public' : [publicGT, public], \n",
    "            'not public' : [notPublicGT, notPublic], \n",
    "            'partial' : [partialPublicGT, partialPublic]\n",
    "        }\n",
    "        \n",
    "        for key in values:\n",
    "            if type(values[key][0]) == float  or type(values[key][0]) == np.float64:\n",
    "                if np.isnan(values[key][0]):\n",
    "                    values[key][0] = None\n",
    "\n",
    "        print(f'Path: {pdfPath}{pdfName}')\n",
    "        print('\\n______________________________')\n",
    "        \n",
    "        # compare all extracted data to ground truth values\n",
    "        for cat in values:\n",
    "            \n",
    "            # if ground truth and extractor are equal but not both none, its tp else tn\n",
    "            if values[cat][1] == values[cat][0]:\n",
    "                if values[cat][1] != None and values[cat][0] != None:\n",
    "                    results[cat]['tp'] += 1\n",
    "            \n",
    "            # if extractor didnt find anything, its missing\n",
    "            elif values[cat][1] == None:\n",
    "                results[cat]['fn'] += 1\n",
    "                \n",
    "            # if they do not equal, its spurious\n",
    "            else:\n",
    "                results[cat]['fp'] += 1\n",
    "            \n",
    "                \n",
    "        \n",
    "        with open('..\\\\data\\\\results\\\\openstateV2.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "\n",
    "\n",
    "# evaluateV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "occupational-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true positive': 19, 'false negative': 31, 'false positive': 31, 'correct': 30, 'spurious': 10, 'missing': 2, 'partial': 5}\n",
      "{'precision': 0.722, 'recall': 0.878, 'f1': 0.793}\n"
     ]
    }
   ],
   "source": [
    "def performanceReason(results, save):\n",
    "    precision = (results['correct'] + (.5* results['partial'])) / (results['correct'] + results['spurious'] + results['partial'])\n",
    "    recall    = (results['correct'] + (.5* results['partial'])) / (results['correct'] + results['missing'] + results['partial'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    save['reason'] = {'precision':round(precision,3),'recall':round(recall,3),'f1':round(f1,3)}\n",
    "    print(results)\n",
    "    print(save['reason'])\n",
    "    return save\n",
    "\n",
    "def calcPerformance(r):\n",
    "    performance = {}\n",
    "    \n",
    "    for item in r:\n",
    "        if item == 'reason':\n",
    "            continue\n",
    "        \n",
    "        precision = (r[item]['correct']) / (r[item]['correct']+r[item]['spurious'])\n",
    "        recall = (r[item]['correct']) / (r[item]['correct']+r[item]['missing'])\n",
    "        f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "        performance[item] = {}\n",
    "        performance[item]['precision'] = round(precision,3)\n",
    "        performance[item]['recall'] = round(recall,3)\n",
    "        performance[item]['f1'] = round(f1,3)\n",
    "\n",
    "    return performance\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\openstate.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "    save = calcPerformance(results)\n",
    "    save = performanceReason(results['reason'], save)\n",
    "with open('..\\\\data\\\\results\\\\openstate performance.json', 'w') as f:\n",
    "    json.dump(save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "coordinate-avatar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calcPerformanceV2(results):\n",
    "    performance = {}\n",
    "    for key in results:       \n",
    "        precision = results[key]['tp'] / (results[key]['tp'] + results[key]['fp'])\n",
    "        recall = results[key]['tp'] / (results[key]['tp'] + results[key]['fn'])\n",
    "        if precision == 0 and recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "        performance[key] = {}\n",
    "        performance[key]['precision'] = round(precision,3)\n",
    "        performance[key]['recall'] = round(recall,3)\n",
    "        performance[key]['f1'] = round(f1,3)\n",
    "        print(key)\n",
    "        print(results[key])\n",
    "        print(performance[key])\n",
    "\n",
    "    return performance\n",
    "\n",
    "def calculatePerformanceFinal(version):\n",
    "    with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "        p = calcPerformanceV2(results)\n",
    "        \n",
    "    with open('..\\\\data\\\\results\\\\openstate performance' + version +'json', 'w') as f:\n",
    "        json.dump(p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "offensive-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfInventoryDocs():\n",
    "    basePDFPath = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "    nInventoryDocs = 0\n",
    "    for dir in os.listdir(basePDFPath):\n",
    "        for request in os.listdir(basePDFPath + dir):\n",
    "            if request == '.DS_Store':\n",
    "                continue\n",
    "            for file in os.listdir(basePDFPath + dir + '\\\\' + request):\n",
    "                if 'inventaris' in file.lower():\n",
    "                    print(file)\n",
    "                    nInventoryDocs += 1\n",
    "    return nInventoryDocs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "official-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinDataframe(number = None, col = None):\n",
    "    baseDFPath = '..\\\\data\\\\openstate data\\\\'\n",
    "    ministrie = 'ienw'\n",
    "\n",
    "    # get dir of ministry \n",
    "    for f in os.listdir(baseDFPath):\n",
    "        if ministrie in f.lower():\n",
    "            file = f\n",
    "            break\n",
    "        \n",
    "    testDf = pd.read_excel(baseDFPath + file)\n",
    "    if not number:\n",
    "        return testDf\n",
    "    testDf = testDf[testDf['WOB Verzoek'] == number]\n",
    "    if not col:\n",
    "        return testDf\n",
    "\n",
    "    return testDf[col].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "binary-bishop",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evalReasonJaccard():\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws']\n",
    "    docs = 0\n",
    "    results = {'tp':0, 'fn':0, 'fp':0}\n",
    "    while docs < 50:\n",
    "        while True:\n",
    "            ministry = random.choice(mins)\n",
    "            clear_output()\n",
    "            print(f'number of docs checked {docs}')\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(ministry)\n",
    "            \n",
    "            if pdfPath == None:\n",
    "                continue\n",
    "            \n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')   \n",
    "            \n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "            if text == None:\n",
    "                continue\n",
    "            print(text[:2000])\n",
    "            \n",
    "            # check if the text actually contains the decision document\n",
    "            goodText = input('Good text? ')\n",
    "            if goodText.lower() != 'n':\n",
    "                docs += 1\n",
    "                break\n",
    "            if goodText == 'q':\n",
    "                return results\n",
    "            \n",
    "        GT = groundTruth['Soort aanvraag'].values[0]\n",
    "        matches = getRequestReason(text)\n",
    "        if len(matches) == 0:\n",
    "            match = ''\n",
    "        else:\n",
    "            match = matches[0]\n",
    "\n",
    "        print(f'Ground truth description: {GT}')\n",
    "        print(f'Extracted description: {match}')\n",
    "\n",
    "\n",
    "        GT = GT.replace('\\n', ' ')\n",
    "        match = match.replace('\\n', ' ')\n",
    "\n",
    "        IoU = calculateIoU(set(GT), set(match))\n",
    "        print(IoU)\n",
    "        if IoU > .5:\n",
    "            print('Good')\n",
    "            results['tp'] += 1\n",
    "        else:\n",
    "            print('bad')\n",
    "            results['fp'] += 1\n",
    "            results['fn'] += 1\n",
    "\n",
    "    precision = results['tp'] / (results['tp'] + results['fp'])\n",
    "    recall = results['tp'] / (results['tp'] + results['fn'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "\n",
    "    print(f'true positve {results[\"tp\"]}')\n",
    "    print(f'False negative {results[\"fn\"]}')\n",
    "    print(f'False positve {results[\"fp\"]}')\n",
    "\n",
    "    print(f'precision {precision}')\n",
    "    print(f'recall {recall}')\n",
    "    print(f'f1 {f1}')\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "latter-platinum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reason\n",
      "{'tp': 91, 'fn': 9, 'fp': 9}\n",
      "{'precision': 0.91, 'recall': 0.91, 'f1': 0.91}\n",
      "received\n",
      "{'tp': 70, 'fn': 0, 'fp': 30}\n",
      "{'precision': 0.7, 'recall': 1.0, 'f1': 0.824}\n",
      "awnser\n",
      "{'tp': 68, 'fn': 0, 'fp': 32}\n",
      "{'precision': 0.68, 'recall': 1.0, 'f1': 0.81}\n",
      "days taken\n",
      "{'tp': 64, 'fn': 0, 'fp': 36}\n",
      "{'precision': 0.64, 'recall': 1.0, 'f1': 0.78}\n",
      "in time\n",
      "{'tp': 72, 'fn': 0, 'fp': 28}\n",
      "{'precision': 0.72, 'recall': 1.0, 'f1': 0.837}\n",
      "docs considered\n",
      "{'tp': 41, 'fn': 35, 'fp': 17}\n",
      "{'precision': 0.707, 'recall': 0.539, 'f1': 0.612}\n",
      "days per doc\n",
      "{'tp': 32, 'fn': 35, 'fp': 25}\n",
      "{'precision': 0.561, 'recall': 0.478, 'f1': 0.516}\n",
      "n pages\n",
      "{'tp': 91, 'fn': 0, 'fp': 9}\n",
      "{'precision': 0.91, 'recall': 1.0, 'f1': 0.953}\n",
      "public\n",
      "{'tp': 2, 'fn': 27, 'fp': 16}\n",
      "{'precision': 0.111, 'recall': 0.069, 'f1': 0.085}\n",
      "not public\n",
      "{'tp': 4, 'fn': 40, 'fp': 14}\n",
      "{'precision': 0.222, 'recall': 0.091, 'f1': 0.129}\n",
      "partial\n",
      "{'tp': 5, 'fn': 62, 'fp': 13}\n",
      "{'precision': 0.278, 'recall': 0.075, 'f1': 0.118}\n"
     ]
    }
   ],
   "source": [
    "def evaluateFinal(version):\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws']\n",
    "    \n",
    "    resultsDir = 'C:\\\\Users\\\\justin\\\\OneDrive - UvA\\\\Studie\\\\Data Science\\\\Thesis\\\\Knowledge extraction\\\\data\\\\results\\\\'\n",
    "    \n",
    "    # get results doc\n",
    "    if 'openstate' + version +'.json' in os.listdir(resultsDir):\n",
    "        with open(resultsDir + 'openstate' + version +'.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "            docsChecked = sum([results['received'][key] for key in results['received']])\n",
    "    # if theres no results doc yet, create new one\n",
    "    else:\n",
    "        docsChecked = 0\n",
    "        results = {}\n",
    "        cats = ['reason','received', 'awnser', 'days taken', 'in time', 'docs considered', 'days per doc', 'n pages', 'public', 'not public', 'partial']\n",
    "        for cat in cats:\n",
    "            results[cat] = {'tp' : 0, 'fn' : 0, 'fp' : 0}\n",
    "    \n",
    "\n",
    "    while docsChecked < 100:\n",
    "\n",
    "        # this selects a request with good text and retrieves the ground truth from the openstate files\n",
    "        while True:\n",
    "            ministry = random.choice(mins)\n",
    "            clear_output()\n",
    "            print(f'number of docs checked {docsChecked}')\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(ministry)\n",
    "            \n",
    "            if pdfPath == None:\n",
    "                continue\n",
    "            \n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')   \n",
    "            \n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "            if text == None:\n",
    "                continue\n",
    "            print(text[:2000])\n",
    "            \n",
    "            # check if the text actually contains the decision document\n",
    "            goodText = input('Good text? ')\n",
    "            if goodText.lower() != 'n':\n",
    "                break\n",
    "            if goodText == 'q':\n",
    "                with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "                return results\n",
    "            \n",
    "        docsChecked += 1\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "            \n",
    "        # extract metadata\n",
    "        reason = getRequestReason(text)\n",
    "        if len(reason) == 0:\n",
    "            reason = ''\n",
    "        else:\n",
    "            match = reason[0]\n",
    "        receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "        nDocuments = nDocs(text)\n",
    "        nPages = getNumberOfPages(pdfPath)\n",
    "        public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "        \n",
    "        # tabula has a lot of output, this clears it\n",
    "        clear_output()\n",
    "\n",
    "        # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "        try:\n",
    "            if receivedDate:\n",
    "                receivedDate = convertDate(receivedDate, True)\n",
    "        except:\n",
    "            receivedDate = None\n",
    "        try:\n",
    "            if completedDate:\n",
    "                completedDate = convertDate(completedDate, True)\n",
    "        except:\n",
    "            completedDate = None\n",
    "\n",
    "        # if number of documents was not found in text, use total docs from inventory list\n",
    "        if not nDocuments and total:\n",
    "            nDocuments = total        \n",
    "\n",
    "        # calculate days per doc\n",
    "        if daysTaken and nDocuments:\n",
    "            daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "        elif total and daysTaken:\n",
    "            daysPerDoc = round(daysTaken / total, 2)\n",
    "        else:\n",
    "            daysPerDoc = None\n",
    "        \n",
    "        # ground truth data\n",
    "        desc = groundTruth['Soort aanvraag'].values[0]\n",
    "        received = groundTruth['Datum van binnenkomst'].values[0]\n",
    "        awnser = groundTruth['Datum van antwoord'].values[0]\n",
    "        daysTakenGT = groundTruth['Aantal dagen in behandeling'].values[0]\n",
    "        inTimeGT = groundTruth['Binnen de termijn afgehandeld'].values[0]\n",
    "        nDocsConsidered = groundTruth['Aantal overwogen documenten'].values[0]\n",
    "        daysPerDocGT = groundTruth['Aantal dagen nodig gehad per document'].values[0]\n",
    "        nPagesGT = groundTruth[\"Omvang document (aantal pagina's)\"].values[0]\n",
    "        publicGT = groundTruth[\"Volledig verstrekte documenten\"].values[0]\n",
    "        notPublicGT = groundTruth[\"Niet verstrekte documenten\"].values[0]\n",
    "        partialPublicGT = groundTruth[\"Deels verstrekte documenten\"].values[0]\n",
    "\n",
    "        # extract n docs from ground truth values\n",
    "        publicGT = getSumOfNumbersFromString(publicGT)\n",
    "        notPublicGT = getSumOfNumbersFromString(notPublicGT)\n",
    "        partialPublicGT = getSumOfNumbersFromString(partialPublicGT)\n",
    "        \n",
    "        if nDocsConsidered == 0:\n",
    "            nDocsConsidered = None\n",
    "\n",
    "        # get inTimeGT to correct true/false format\n",
    "        if inTimeGT == 'Ja':\n",
    "            inTimeGT = True\n",
    "        elif inTimeGT == 'Nee':\n",
    "            inTimeGT = False\n",
    "        else:\n",
    "            inTimeGT = None\n",
    "\n",
    "\n",
    "        values = {\n",
    "            'received' : [pd.Timestamp(received), receivedDate], \n",
    "            'awnser' : [pd.Timestamp(awnser), completedDate], \n",
    "            'days taken' : [daysTakenGT, daysTaken], \n",
    "            'in time' : [inTimeGT, inTime], \n",
    "            'docs considered' : [nDocsConsidered, nDocuments], \n",
    "            'days per doc' : [round(daysPerDocGT, 2), daysPerDoc], \n",
    "            'n pages' : [nPagesGT, nPages], \n",
    "            'public' : [publicGT, public], \n",
    "            'not public' : [notPublicGT, notPublic], \n",
    "            'partial' : [partialPublicGT, partialPublic]\n",
    "        }\n",
    "        \n",
    "        for key in values:\n",
    "            if type(values[key][0]) == float  or type(values[key][0]) == np.float64:\n",
    "                if np.isnan(values[key][0]):\n",
    "                    values[key][0] = None\n",
    "        \n",
    "        # compare all extracted data to ground truth values\n",
    "        for cat in values:\n",
    "            \n",
    "            # if ground truth and extractor are equal but not both none, its tp else tn\n",
    "            if values[cat][1] == values[cat][0]:\n",
    "                if values[cat][1] != None and values[cat][0] != None:\n",
    "                    results[cat]['tp'] += 1\n",
    "            \n",
    "            # if extractor didnt find anything, its missing\n",
    "            elif values[cat][1] == None:\n",
    "                results[cat]['fn'] += 1\n",
    "                \n",
    "            # if they do not equal, its spurious\n",
    "            else:\n",
    "                results[cat]['fp'] += 1\n",
    "        \n",
    "\n",
    "        desc = desc.replace('\\n', ' ')\n",
    "        match = match.replace('\\n', ' ')\n",
    "        print(results)\n",
    "\n",
    "        IoU = calculateIoU(set(desc), set(match))\n",
    "        if IoU > .5:\n",
    "            results['reason']['tp'] += 1\n",
    "        else:\n",
    "            results['reason']['fp'] += 1\n",
    "            results['reason']['fn'] += 1\n",
    "        \n",
    "        with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "                \n",
    "# evaluateFinal('final')\n",
    "calculatePerformanceFinal('final')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
