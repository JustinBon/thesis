{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "import PyPDF2\n",
    "import tabula\n",
    "\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-iraqi",
   "metadata": {},
   "source": [
    "## Request metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-essence",
   "metadata": {},
   "source": [
    "Metadata about wob requests can be usefull to collect. Luckily, when the government agency to which the request was send completes the request, it also gives a decision document and a inventory list in addition to the documents that were requested. With these two documents the reason for a request, the date on which the request was received and completed, the number of documents concidered, the number of documents (partially) released, and the number of documents not released can be extracted.\n",
    "\n",
    "# Reason for request\n",
    "The reason for the wob request can be found in the decision document. In this document it states in one sentence a summary of what has been requested. This summary is what needs to be extracted. This summary is usually indicated by a keyword or keywords. These keywords come down to dutch versions of \"requested\", \"information about\", or \"publication of\". What follows is a list of all keywords used in dutch:\n",
    "- verzocht\n",
    "- u verzoekt\n",
    "- om informatie over\n",
    "- uw verzoek ziet\n",
    "- om openbaarmaking van\n",
    "- more to come\n",
    "When one of these keywords are found, all following text is extracted until a the next period occurs. To do this, a regular expression was used. \n",
    "\n",
    "keyword([^.]+?)\\\\.\n",
    "\n",
    "Where keyword is one of the words or phrases listed above. The expression first finds one of these keywords and then matches any alpha-numerical character until the first period is found. Before the regular expression can be used however, the text needs some preprocessing. First, exessive newlines are removed. Second, all letters are converted to lowercase letters. This is done so that the regular expression will match the keywords even though in the text the the keywords are written with capitalization. This has to be done as regular expressions are case sensitive. Last, for any word or abbreviation in the text that includes a period where the period does not indicate the end of a sentence, said period have to be removed otherwise it will trip up the regular expression. After this, the neccessary information can be extracted.\n",
    "\n",
    "After the regular expressions have been run, the matches need to be checked for duplicates. This can happen when the decision document states for example \"the request if for information about [...]\". In this case the same sentence would be matched for the keyword \"request\" and \"information about\". This would match the same sentence twice so only one of these is needed and the other is removed.\n",
    "\n",
    "To evaluate the extractor, a different method than the other extractors was used: the Intersection over Union (IoU). With this metric precision, recall, and F1 scores are still calculated, but with different method. It uses the overlap between the ground truth and the prediction to measure the performance of a model. First the intersect and union of the ground truth and prediction are calculated. The intersect is the overlap between the set of words in the extraction and the set of words in the ground truth. The union is the combination of the two. With this the IoU metric is calculated. Using the panoptic segmentation metric \\cite{Mechea_2019}, IoU can then be compared to a threshold value. If the IoU is higher then the treshold it counts as a true positive, if it is lower it counts as both a false negative and a false positive. Presicion, recall and F1 score can then be calculated.\n",
    "\n",
    "# Relevant dates\n",
    "There are two relevant dates that can be extracted from the decision document: the date on which the request was received and the date on which the request was completed. The decision document has these two dates at the beginning. The completion date is same date as when the document was made so that is always the very first date in the document. The date of request is always in the first sentence of the document as they all start with as follows: \"In your letter of 01 januari 2022\". This means that the first and second date that are found in the document are the relevant dates to extract. Sometimes the date a request was send is not the same as the date the request was received. This happens when the request is send by post. In this case the decision document states: \"in your letter of 01 januari 2022, received on 05 januari 2022\". The following regular expression was used to check if this is the case:\n",
    "\n",
    "'ontvangen op ([^.]+?)\\,'\n",
    "\n",
    " In this case the first and third date are the relevant dates. To actually extract the dates, the dates extractor described here TODO[REF HERE] was used. These dates can then also be used to check how long the request took to fulfull and if it was done in the time that they have. \n",
    "\n",
    "\n",
    "# Number of documents\n",
    "The inventory lists contain an overview of all documents that were found that fall in the scope of the request. The list also has information about which documents were made public, which were made partially public, which were not made public and also the documents that were already public. \"Openbaar\" or \"volledig openbaar\" for public, \"deels openbaar\" or \"gedeeltelijk openbaar\" for partially public, and \"niet openbaar\", \"reeds openbaar\", or \"geweigerd\" for not public. The sum of these can be used to find the total number of documents considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-parcel",
   "metadata": {},
   "source": [
    "## Results\n",
    "The request metadata extractors in \\hyperref[tab:table7]{table 7} show very varying results. All the extractors that had to do with dates (Date received, date fulfilled, number of days taken, and completed in time) all show good performance in the F\\textsubscript{1} score. However it is worth to mention that this is for the most part due to the recall for all of these being 1. This means that the extractor never found a match when there was no ground truth match to be found. The precision is a lot lower meaning that it wasn't correct all of the time. \n",
    "\n",
    "The extractors that had to do with the number of documents preformed really low. These are: Documents considered, days taken per document, Number of public documents, Number of partially public documents, and Number of not public documents. This is mostly because of the fact that the inventory lists are necessary to calculate these and of the 1045 requests in the dataset only 256 had an one. Besides that, the way the data was stored doesn't lend itself well to extraction. Its stored in tables within a PDF documents. There are methods to retrieve the tables with Python (like Tabula used in this thesis) however these methods are far from foolproof and don't always work. Even if the table is retrieved, there is no consistency between ministries on how to make these tables which adds another layer of complexity.\n",
    "\n",
    "The reason for request extractor does show good results with a precision of 0.722 it correctly identified almost three quarters of request reasons. The recall is higher at 0.878. These figures give an F\\textsubscript{1} score of 0.793\n",
    "\n",
    "\n",
    "One problem the used approach is sensetive to is mistakes made in the OCR process. The regular expressions look some keywords and, when those are found, the end of the sentence. If in the OCR process a mistake was made in on of the keywords or if the period at the end of the sentence wasn't recognized as a period, the regular expression won't match it even though it should. \n",
    "\n",
    "Another problem is the generalizibility. These extractors were made specificly for desicion documents coming from wob requests to dutch government ministries. They will not work for wob request to provinces or muninipalities. They use different standards which do not work with the current extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "steady-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the dirs id wob request as 1, 2,3 and some do it as 1.0, 2.0, 3.0\n",
    "# this makes all of the dirs use the first method\n",
    "def fixNamingSceme():\n",
    "    base = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "    for dir in os.listdir(base):\n",
    "        for folder in os.listdir(base + dir):\n",
    "            if os.path.isdir(base + dir + '\\\\' + folder):\n",
    "                os.rename(base + dir + '\\\\' + folder, base + dir + '\\\\' + folder.split('.')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-logistics",
   "metadata": {},
   "source": [
    "### Cells for dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "understood-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates matcher\n",
    "def getDates(text, nlp):\n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'okt']\n",
    "    days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "       'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "\n",
    "    datesPattern = [ \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = removeKenmerkDate(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    return [doc[start:end].text for match_id, start, end in matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "macro-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts dates to timestamp or yyyy-mm-dd\n",
    "def convertDate(date, timestamp=False):\n",
    "    months = {\n",
    "        'januari':'01','jan':'01',\n",
    "        'februari':'02','feb':'02',\n",
    "        'maart':'03','mrt':'03',\n",
    "        'april':'04','apr':'04',\n",
    "        'mei':'05','mei':'05',\n",
    "        'juni':'06','jun':'06',\n",
    "        'juli':'07','jul':'07',\n",
    "        'augustus':'08','aug':'08',\n",
    "        'september':'09','sep':'09',\n",
    "        'oktober':'10','okt':'10',\n",
    "        'november':'11','nov':'11',\n",
    "        'december':'12','dec':'12'\n",
    "    }\n",
    "\n",
    "\n",
    "    date = date.split(' ')\n",
    "    date[1] = months[date[1].lower()]\n",
    "\n",
    "    if timestamp:\n",
    "        return pd.Timestamp(year=int(date[2]), month=int(date[1]), day=int(date[0]))\n",
    "    if len(date[0]) == 1:\n",
    "        date[0] = '0' + date[0]\n",
    "    return date[2] + '-' + date[1] + '-' + date[0]\n",
    "\n",
    "# calculate days between two dates\n",
    "def days_between(d1, d2):\n",
    "    d1 = convertDate(d1)\n",
    "    d2 = convertDate(d2)\n",
    "    try:\n",
    "        d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "        d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "    return abs((d2 - d1).days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "broken-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts all fields for dates\n",
    "def dateInformation(text):        \n",
    "    # find dates in text with date extractor\n",
    "    matches = getDates(text, nlp)\n",
    "\n",
    "    # first found date is date when document was written\n",
    "    # the date the wob request was completed\n",
    "    if len(matches) < 3:\n",
    "        return None, None, None, None\n",
    "    completedDate = matches[0]\n",
    "\n",
    "    # check if request was received on a different date then when it was send\n",
    "    # this is the case if it states \"ontvangen op\"\n",
    "    receivedDate = re.findall('ontvangen op ([^.]+?)\\,', text)\n",
    "    if not receivedDate:\n",
    "        receivedDate = matches[1]\n",
    "    else:\n",
    "        receivedDate = matches[2]\n",
    "\n",
    "    # calculate days between request and completion\n",
    "    daysTaken = days_between(completedDate, receivedDate)\n",
    "    \n",
    "    # converts to yyyy-mm-dd\n",
    "    start = convertDate(receivedDate)\n",
    "    end = convertDate(completedDate)\n",
    "    print(start, end)\n",
    "    \n",
    "    # converts to np.datetime \n",
    "    try:\n",
    "        start = np.datetime64(start, format='Y%-%m-%d')\n",
    "        end = np.datetime64(end, format='Y%-%m-%d')\n",
    "        businessDaysTaken = np.busday_count(start, end)\n",
    "        inTime = businessDaysTaken <= 42\n",
    "    except:\n",
    "        inTime = None\n",
    "    \n",
    "    # check if request was fulfilled wihtin 42 business days\n",
    "\n",
    "    return receivedDate, completedDate, daysTaken, inTime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-tooth",
   "metadata": {},
   "source": [
    "### Cleaning cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stable-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the extractor used periods as indicators, abbreviations like N.V.T. need to be removed\n",
    "# this function converts it to N V T while keeping the periods at the end of sentences\n",
    "def removeAbbreviation(text):\n",
    "    sentence = ''\n",
    "\n",
    "    # split sentence in words\n",
    "    for word in text.split(' '):\n",
    "\n",
    "        # if there is no period in the word, add it back to sentence\n",
    "        if word.count('.') == 0 or '\\n' in word:\n",
    "            sentence += word + ' '\n",
    "            continue\n",
    "        \n",
    "        # when there is one period in word\n",
    "        elif word.count('.') == 1:\n",
    "\n",
    "            # if it is at the end, keep it and add word to sentence\n",
    "            if word[-1] == '.':\n",
    "                sentence += word + ' '\n",
    "            \n",
    "            # if its in the middle replace it with a space\n",
    "            else:\n",
    "                word = word.replace('.', ' ')\n",
    "                sentence += word + ' '\n",
    "        \n",
    "        # if there are  more than 1 periods, replace them all\n",
    "        else:\n",
    "            word = word.replace('.', ' ')\n",
    "            sentence += word + ' '\n",
    "            \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "derived-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge case when the sidebar of the doc is earlier than the heading\n",
    "# in that case we have to remove a date that is not used\n",
    "def removeKenmerkDate(text):\n",
    "    text = text.split('\\n')\n",
    "    \n",
    "    toRemove = None\n",
    "    for i in range(len(text)):\n",
    "        if '(kenmerk)' in text[i] and 'ons' not in text[i]:\n",
    "            toRemove = [i, i+1]\n",
    "            break\n",
    "            \n",
    "    if toRemove:\n",
    "        text.remove(text[toRemove[0]])\n",
    "        text.remove(text[toRemove[1]])\n",
    "\n",
    "    text = '\\n'.join(text)                        \n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-mentor",
   "metadata": {},
   "source": [
    "### request reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fewer-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to find the request reason from decision doc\n",
    "def getRequestReason(text, removeAbbr = True):\n",
    "\n",
    "    # text = rawText.replace('\\n', ' ')\n",
    "    text = text.lower()\n",
    "    if removeAbbr:\n",
    "        text = removeAbbreviation(text)\n",
    "\n",
    "    patterns = ['verzocht([^.]+?)\\.', \n",
    "                'u verzoekt([^.]+?)\\.', \n",
    "                'om informatie over([^.]+?)\\.', \n",
    "                'uw verzoek ziet([^.]+?)\\.', \n",
    "                'om openbaarmaking van([^.]+?)\\.']\n",
    "    matches = []\n",
    "\n",
    "    # get matches for all keywords\n",
    "    for pattern in patterns:\n",
    "        matches += re.findall(pattern, text)\n",
    "\n",
    "    uniqueMatches = []\n",
    "\n",
    "    # check all matches against eachother\n",
    "    for i in range(len(matches)):\n",
    "        for j in range(len(matches)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # if there is a duplicate we do not add it to uniqueMatches\n",
    "            if matches[i] in matches[j]:\n",
    "                break\n",
    "\n",
    "        # add match to uniqueMatches if the j loop completes\n",
    "        else:\n",
    "            uniqueMatches.append(matches[i])\n",
    "\n",
    "    return matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-gender",
   "metadata": {},
   "source": [
    "### Retrieving text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "norman-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets a random decision doc from a given ministry\n",
    "def getRandomDecisionDoc(ministrie):\n",
    "\n",
    "    # set paths \n",
    "    baseDFPath = '..\\\\data\\\\openstate data\\\\'\n",
    "    basePDFPath = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "\n",
    "    # get df of ministry\n",
    "    for f in os.listdir(baseDFPath):\n",
    "        if ministrie in f.lower():\n",
    "            file = f\n",
    "            break\n",
    "\n",
    "    # get dir of ministry \n",
    "    for d in os.listdir(basePDFPath):\n",
    "        if ministrie in d.lower():\n",
    "            dir = basePDFPath + d + '\\\\'\n",
    "            break\n",
    "    \n",
    "    requests = [x for x in os.listdir(dir) if x != '.DS_Store']\n",
    "    requestNr = int(random.choice(requests))\n",
    "    \n",
    "    # load in dataframe of ministry and get random sample\n",
    "    testDf = pd.read_excel(baseDFPath + file)\n",
    "    testDf.columns = [x.replace('\\n', '') for x in testDf.columns]    \n",
    "    s = testDf[testDf['WOB Verzoek'] == requestNr]\n",
    "\n",
    "    if not os.path.exists(dir + str(requestNr)):\n",
    "        return None, None, None\n",
    "\n",
    "    # find the desicion document\n",
    "    for p in os.listdir(dir + str(requestNr)):\n",
    "\n",
    "        # if found, save it in pdfPath\n",
    "        if 'besluit' in p.lower() and 'bijlage' not in p.lower() and p.endswith('.pdf'):\n",
    "            pdfPath = p\n",
    "            break\n",
    "    \n",
    "    # if there is no desicion document, try again\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "    return s, dir + str(requestNr) + '\\\\', pdfPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "theoretical-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text with pdftotext\n",
    "def textExtract(dir, name, startPage = 0, endPage = 10):\n",
    "    txtName = '.'.join(name.split('.')[0:-1])\n",
    "    txtName = dir + txtName + f'-pages{startPage}-{endPage}' + '.txt'\n",
    "\n",
    "    # extract text\n",
    "    os.system(f'pdftotext -f {startPage} -l {endPage} -raw \"{dir}{name}\" \"{txtName}\"')\n",
    "    \n",
    "    if not os.path.exists(txtName): \n",
    "        return None\n",
    "    \n",
    "    # open file and return content\n",
    "    with open(txtName, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-humanity",
   "metadata": {},
   "source": [
    "### Inventory document cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bored-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory(path, pdf):\n",
    "\n",
    "    # words to look for\n",
    "    rating = {'deels openbaar':0, 'niet openbaar':0, 'openbaar':0, 'reeds openbaar':0, \n",
    "            'geweigerd':0, 'gedeeltelijk openbaar':0,\n",
    "            'volledig openbaar': 0}\n",
    "    \n",
    "    # if an inventory document exists, use that\n",
    "    for file in os.listdir(path):\n",
    "        if 'inventaris' in file.lower():\n",
    "            rating = inventoryListToDataframe(path + file, rating)\n",
    "            break\n",
    "    \n",
    "    # else try to find inventory table in decision doc\n",
    "    if not rating:\n",
    "        rating = inventoryListToDataframe(path + pdf, rating)\n",
    "\n",
    "    if not rating:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # combine categories\n",
    "    notPublic = rating['niet openbaar'] + rating['geweigerd'] + rating['reeds openbaar']\n",
    "    partialPublic = rating['deels openbaar'] + rating['gedeeltelijk openbaar']\n",
    "    public = rating['openbaar'] - rating['niet openbaar'] - rating['reeds openbaar'] - partialPublic\n",
    "    total = public + notPublic + partialPublic\n",
    "    print(rating)\n",
    "    if total == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    return public, notPublic, partialPublic, public + notPublic + partialPublic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sunrise-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes dataframes from table in pdf\n",
    "# then counts occurences of \n",
    "def inventoryListToDataframe(pdf, rating):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(pdf, pages='all')\n",
    "\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            return None\n",
    "\n",
    "        for table in tables:\n",
    "            for col in table.columns:\n",
    "                col = list(table[col])\n",
    "                col = [str(x).lower() for x in col]\n",
    "                for key in rating:\n",
    "                    for value in col:\n",
    "                        if key in value:\n",
    "                            rating[key] += 1\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-resource",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "psychological-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts description of n documents to int\n",
    "def getSumOfNumbersFromString(string):\n",
    "    try:\n",
    "        if type(string) == str:\n",
    "            return sum([int(s) for s in string.split() if s.isdigit()])\n",
    "        elif type(string) == float or type(string) == int:\n",
    "            return int(string)\n",
    "    except:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "middle-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of pages in pdf documents\n",
    "def getNumberOfPages(path):\n",
    "    nPages = 0\n",
    "\n",
    "    # gets list of pdf files in a directory\n",
    "    pdfs = [x for x in os.listdir(path) if x.endswith('.pdf')]\n",
    "    \n",
    "    # open all files and count pages\n",
    "    for file in pdfs:\n",
    "        try:\n",
    "            with open(path + file, 'rb') as f:\n",
    "                pdf = PyPDF2.PdfFileReader(f, strict=False)\n",
    "                nPages += pdf.numPages\n",
    "        except:\n",
    "            return None\n",
    "    return nPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ultimate-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of considered documents in the decision doc\n",
    "def nDocs(text):\n",
    "\n",
    "    # look for the first mention of a number of documents\n",
    "    nDocuments = re.findall('[0-9]+? documenten[a-z]{2} aangetroffen', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        nDocuments = re.findall('[0-9]+? document[a-z]{2}', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        if len(re.findall('één document', text)) == 1:\n",
    "            return 1\n",
    "\n",
    "    # return the first if found, else return None\n",
    "    if type(nDocuments) == list:\n",
    "        if len(nDocuments) == 0:\n",
    "            return None\n",
    "        return int(nDocuments[0].split(' ')[0])\n",
    "    elif type(nDocuments) == int:\n",
    "        return nDocuments\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-arabic",
   "metadata": {},
   "source": [
    "### Performance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "psychological-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIoU(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "\n",
    "    intersection = 0\n",
    "\n",
    "    for value in a:\n",
    "        if value in b:\n",
    "            intersection += 1\n",
    "\n",
    "    union = len(a) + len(b) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "norwegian-camera",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calcPerformanceV2(results):\n",
    "    performance = {}\n",
    "    for key in results:       \n",
    "        precision = results[key]['tp'] / (results[key]['tp'] + results[key]['fp'])\n",
    "        recall = results[key]['tp'] / (results[key]['tp'] + results[key]['fn'])\n",
    "        if precision == 0 and recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "        performance[key] = {}\n",
    "        performance[key]['precision'] = round(precision,3)\n",
    "        performance[key]['recall'] = round(recall,3)\n",
    "        performance[key]['f1'] = round(f1,3)\n",
    "        print(key)\n",
    "        print(results[key])\n",
    "        print(performance[key])\n",
    "\n",
    "    return performance\n",
    "\n",
    "def calculatePerformanceFinal(version):\n",
    "    with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "        p = calcPerformanceV2(results)\n",
    "        \n",
    "    with open('..\\\\data\\\\results\\\\openstate performance' + version +'json', 'w') as f:\n",
    "        json.dump(p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incorporate-greece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': {'tp': 178, 'fn': 21, 'fp': 21}, 'received': {'tp': 135, 'fn': 0, 'fp': 65}, 'awnser': {'tp': 135, 'fn': 0, 'fp': 65}, 'days taken': {'tp': 117, 'fn': 0, 'fp': 83}, 'in time': {'tp': 134, 'fn': 0, 'fp': 66}, 'docs considered': {'tp': 71, 'fn': 72, 'fp': 36}, 'days per doc': {'tp': 48, 'fn': 76, 'fp': 54}, 'n pages': {'tp': 179, 'fn': 0, 'fp': 21}, 'public': {'tp': 5, 'fn': 53, 'fp': 32}, 'not public': {'tp': 8, 'fn': 81, 'fp': 29}, 'partial': {'tp': 9, 'fn': 117, 'fp': 28}}\n",
      "reason\n",
      "{'tp': 179, 'fn': 21, 'fp': 21}\n",
      "{'precision': 0.895, 'recall': 0.895, 'f1': 0.895}\n",
      "received\n",
      "{'tp': 135, 'fn': 0, 'fp': 65}\n",
      "{'precision': 0.675, 'recall': 1.0, 'f1': 0.806}\n",
      "awnser\n",
      "{'tp': 135, 'fn': 0, 'fp': 65}\n",
      "{'precision': 0.675, 'recall': 1.0, 'f1': 0.806}\n",
      "days taken\n",
      "{'tp': 117, 'fn': 0, 'fp': 83}\n",
      "{'precision': 0.585, 'recall': 1.0, 'f1': 0.738}\n",
      "in time\n",
      "{'tp': 134, 'fn': 0, 'fp': 66}\n",
      "{'precision': 0.67, 'recall': 1.0, 'f1': 0.802}\n",
      "docs considered\n",
      "{'tp': 71, 'fn': 72, 'fp': 36}\n",
      "{'precision': 0.664, 'recall': 0.497, 'f1': 0.568}\n",
      "days per doc\n",
      "{'tp': 48, 'fn': 76, 'fp': 54}\n",
      "{'precision': 0.471, 'recall': 0.387, 'f1': 0.425}\n",
      "n pages\n",
      "{'tp': 179, 'fn': 0, 'fp': 21}\n",
      "{'precision': 0.895, 'recall': 1.0, 'f1': 0.945}\n",
      "public\n",
      "{'tp': 5, 'fn': 53, 'fp': 32}\n",
      "{'precision': 0.135, 'recall': 0.086, 'f1': 0.105}\n",
      "not public\n",
      "{'tp': 8, 'fn': 81, 'fp': 29}\n",
      "{'precision': 0.216, 'recall': 0.09, 'f1': 0.127}\n",
      "partial\n",
      "{'tp': 9, 'fn': 117, 'fp': 28}\n",
      "{'precision': 0.243, 'recall': 0.071, 'f1': 0.11}\n"
     ]
    }
   ],
   "source": [
    "def evaluateFinal(version):\n",
    "    mins = ['lnv','az','buza','bzk','ezk','fin','ienw','jenv','ocw','szw','vws']\n",
    "    \n",
    "    resultsDir = 'C:\\\\Users\\\\justin\\\\OneDrive - UvA\\\\Studie\\\\Data Science\\\\Thesis\\\\Knowledge extraction\\\\data\\\\results\\\\'\n",
    "    \n",
    "    # get results doc\n",
    "    if 'openstate' + version +'.json' in os.listdir(resultsDir):\n",
    "        with open(resultsDir + 'openstate' + version +'.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "            docsChecked = sum([results['received'][key] for key in results['received']])\n",
    "    # if theres no results doc yet, create new one\n",
    "    else:\n",
    "        docsChecked = 0\n",
    "        results = {}\n",
    "        cats = ['reason','received', 'awnser', 'days taken', 'in time', 'docs considered', 'days per doc', 'n pages', 'public', 'not public', 'partial']\n",
    "        for cat in cats:\n",
    "            results[cat] = {'tp' : 0, 'fn' : 0, 'fp' : 0}\n",
    "    \n",
    "\n",
    "    while docsChecked < 200:\n",
    "\n",
    "        # this selects a request with good text and retrieves the ground truth from the openstate files\n",
    "        while True:\n",
    "            ministry = random.choice(mins)\n",
    "            clear_output()\n",
    "            print(f'number of docs checked {docsChecked}')\n",
    "            groundTruth, pdfPath, pdfName = getRandomDecisionDoc(ministry)\n",
    "            \n",
    "            if pdfPath == None:\n",
    "                continue\n",
    "            \n",
    "            print(f'Path: {pdfPath}{pdfName}')\n",
    "            print('\\n______________________________')   \n",
    "            \n",
    "            # extract text from document\n",
    "            text = textExtract(pdfPath, pdfName)\n",
    "            if text == None:\n",
    "                continue\n",
    "            print(text[:2000])\n",
    "            \n",
    "            # check if the text actually contains the decision document\n",
    "            goodText = input('Good text? ')\n",
    "            if goodText.lower() != 'n':\n",
    "                break\n",
    "            if goodText == 'q':\n",
    "                with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "                return results\n",
    "            \n",
    "        docsChecked += 1\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "            \n",
    "        # extract metadata\n",
    "        reason = getRequestReason(text)\n",
    "        if len(reason) == 0:\n",
    "            reason = ''\n",
    "        else:\n",
    "            match = reason[0]\n",
    "        receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "        nDocuments = nDocs(text)\n",
    "        nPages = getNumberOfPages(pdfPath)\n",
    "        public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "        \n",
    "        # tabula has a lot of output, this clears it\n",
    "        clear_output()\n",
    "\n",
    "        # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "        try:\n",
    "            if receivedDate:\n",
    "                receivedDate = convertDate(receivedDate, True)\n",
    "        except:\n",
    "            receivedDate = None\n",
    "        try:\n",
    "            if completedDate:\n",
    "                completedDate = convertDate(completedDate, True)\n",
    "        except:\n",
    "            completedDate = None\n",
    "\n",
    "        # if number of documents was not found in text, use total docs from inventory list\n",
    "        if not nDocuments and total:\n",
    "            nDocuments = total        \n",
    "\n",
    "        # calculate days per doc\n",
    "        if daysTaken and nDocuments:\n",
    "            daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "        elif total and daysTaken:\n",
    "            daysPerDoc = round(daysTaken / total, 2)\n",
    "        else:\n",
    "            daysPerDoc = None\n",
    "        \n",
    "        # ground truth data\n",
    "        desc = groundTruth['Soort aanvraag'].values[0]\n",
    "        received = groundTruth['Datum van binnenkomst'].values[0]\n",
    "        awnser = groundTruth['Datum van antwoord'].values[0]\n",
    "        daysTakenGT = groundTruth['Aantal dagen in behandeling'].values[0]\n",
    "        inTimeGT = groundTruth['Binnen de termijn afgehandeld'].values[0]\n",
    "        nDocsConsidered = groundTruth['Aantal overwogen documenten'].values[0]\n",
    "        daysPerDocGT = groundTruth['Aantal dagen nodig gehad per document'].values[0]\n",
    "        nPagesGT = groundTruth[\"Omvang document (aantal pagina's)\"].values[0]\n",
    "        publicGT = groundTruth[\"Volledig verstrekte documenten\"].values[0]\n",
    "        notPublicGT = groundTruth[\"Niet verstrekte documenten\"].values[0]\n",
    "        partialPublicGT = groundTruth[\"Deels verstrekte documenten\"].values[0]\n",
    "\n",
    "        # extract n docs from ground truth values\n",
    "        publicGT = getSumOfNumbersFromString(publicGT)\n",
    "        notPublicGT = getSumOfNumbersFromString(notPublicGT)\n",
    "        partialPublicGT = getSumOfNumbersFromString(partialPublicGT)\n",
    "        \n",
    "        if nDocsConsidered == 0:\n",
    "            nDocsConsidered = None\n",
    "\n",
    "        # get inTimeGT to correct true/false format\n",
    "        if inTimeGT == 'Ja':\n",
    "            inTimeGT = True\n",
    "        elif inTimeGT == 'Nee':\n",
    "            inTimeGT = False\n",
    "        else:\n",
    "            inTimeGT = None\n",
    "\n",
    "\n",
    "        values = {\n",
    "            'received' : [pd.Timestamp(received), receivedDate], \n",
    "            'awnser' : [pd.Timestamp(awnser), completedDate], \n",
    "            'days taken' : [daysTakenGT, daysTaken], \n",
    "            'in time' : [inTimeGT, inTime], \n",
    "            'docs considered' : [nDocsConsidered, nDocuments], \n",
    "            'days per doc' : [round(daysPerDocGT, 2), daysPerDoc], \n",
    "            'n pages' : [nPagesGT, nPages], \n",
    "            'public' : [publicGT, public], \n",
    "            'not public' : [notPublicGT, notPublic], \n",
    "            'partial' : [partialPublicGT, partialPublic]\n",
    "        }\n",
    "        \n",
    "        for key in values:\n",
    "            if type(values[key][0]) == float  or type(values[key][0]) == np.float64:\n",
    "                if np.isnan(values[key][0]):\n",
    "                    values[key][0] = None\n",
    "        \n",
    "        # compare all extracted data to ground truth values\n",
    "        for cat in values:\n",
    "            \n",
    "            # if ground truth and extractor are equal but not both none, its tp else tn\n",
    "            if values[cat][1] == values[cat][0]:\n",
    "                if values[cat][1] != None and values[cat][0] != None:\n",
    "                    results[cat]['tp'] += 1\n",
    "            \n",
    "            # if extractor didnt find anything, its missing\n",
    "            elif values[cat][1] == None:\n",
    "                results[cat]['fn'] += 1\n",
    "                \n",
    "            # if they do not equal, its spurious\n",
    "            else:\n",
    "                results[cat]['fp'] += 1\n",
    "        \n",
    "\n",
    "        desc = desc.replace('\\n', ' ')\n",
    "        match = match.replace('\\n', ' ')\n",
    "        print(results)\n",
    "\n",
    "        IoU = calculateIoU(set(desc), set(match))\n",
    "        if IoU > .5:\n",
    "            results['reason']['tp'] += 1\n",
    "        else:\n",
    "            results['reason']['fp'] += 1\n",
    "            results['reason']['fn'] += 1\n",
    "        \n",
    "        with open('..\\\\data\\\\results\\\\openstate' + version +'.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "                \n",
    "evaluateFinal('final')\n",
    "calculatePerformanceFinal('final')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b20df",
   "metadata": {},
   "source": [
    "## Test generalizability\n",
    "\n",
    "The following is to test the generalizability of the extractors\n",
    "I've used wob requests from the munincipality of Amsterdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "brave-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reason:  \n",
      "receivedDate:  None\n",
      "completedDate:  None\n",
      "daysTaken:  None\n",
      "inTime:  None\n",
      "nDocuments:  None\n",
      "nPages:  None\n",
      "daysPerDoc:  None\n",
      "public:  None\n",
      "notPublic:  None\n",
      "partialPublic:  None\n"
     ]
    }
   ],
   "source": [
    "fileName = r'C:\\Users\\justin\\OneDrive - UvA\\Studie\\Data Science\\Thesis\\Knowledge extraction\\data\\amsterdam csv\\amsterdam_files_df.csv'\n",
    "pdfs = r'F:\\Data files\\Master thesis\\pdfs amsterdam\\\\'\n",
    "dfAmstedam = pd.read_csv(fileName)\n",
    "dfAmstedam = dfAmstedam[['full_name', 'name', 'page', 'text']]\n",
    "\n",
    "requests = os.listdir(pdfs)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    request = random.choice(requests)\n",
    "    print(request)\n",
    "    pdfPath = pdfs + request\n",
    "    for file in os.listdir(pdfPath):\n",
    "        if 'besluit' in file.lower():\n",
    "            pdfName = pdfPath + '\\\\' + file\n",
    "            break\n",
    "    \n",
    "    dfRequest = dfAmstedam[dfAmstedam['full_name'].str.contains(request)]\n",
    "\n",
    "    dfRequest = dfRequest.sort_values(by=['page'])\n",
    "    pages = list(dfRequest.head(10).text.values)\n",
    "    pages = [str(x) for x in pages]\n",
    "    text = ' '.join(pages) \n",
    "\n",
    "    reason = getRequestReason(text)\n",
    "    if len(reason) == 0:\n",
    "        reason = ''\n",
    "    else:\n",
    "        match = reason[0]\n",
    "    receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "    nDocuments = nDocs(text)\n",
    "    nPages = getNumberOfPages(pdfPath)\n",
    "    public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "    \n",
    "    # tabula has a lot of output, this clears it\n",
    "    clear_output()\n",
    "\n",
    "    # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "    try:\n",
    "        if receivedDate:\n",
    "            receivedDate = convertDate(receivedDate, True)\n",
    "    except:\n",
    "        receivedDate = None\n",
    "    try:\n",
    "        if completedDate:\n",
    "            completedDate = convertDate(completedDate, True)\n",
    "    except:\n",
    "        completedDate = None\n",
    "\n",
    "    # if number of documents was not found in text, use total docs from inventory list\n",
    "    if not nDocuments and total:\n",
    "        nDocuments = total        \n",
    "\n",
    "    # calculate days per doc\n",
    "    if daysTaken and nDocuments:\n",
    "        daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "    elif total and daysTaken:\n",
    "        daysPerDoc = round(daysTaken / total, 2)\n",
    "    else:\n",
    "        daysPerDoc = None\n",
    "\n",
    "\n",
    "    print('reason: ', reason)\n",
    "    print('receivedDate: ', receivedDate)\n",
    "    print('completedDate: ', completedDate)\n",
    "    print('daysTaken: ', daysTaken)\n",
    "    print('inTime: ', inTime)\n",
    "    print('nDocuments: ', nDocuments)\n",
    "    print('nPages: ', nPages)\n",
    "    print('daysPerDoc: ', daysPerDoc)\n",
    "    print('public: ', public)\n",
    "    print('notPublic: ', notPublic)\n",
    "\n",
    "    \n",
    "    print('partialPublic: ', partialPublic)\n",
    "    time.sleep(1)\n",
    "    input()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eba8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
