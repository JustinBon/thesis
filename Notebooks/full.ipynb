{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "import PyPDF2\n",
    "import tabula\n",
    "\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import langid\n",
    "import voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'nl_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "nlp_ministries = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates\n",
    "\n",
    "TODO combine spacy dates and regex dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates matcher\n",
    "def getDatesBase(text, nlp):\n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    days = ['maandag', 'dinsdag', 'woensdag', 'donderdag', 'vrijdag', 'zaterdag', 'zondag',\n",
    "        'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    sent = ['datum', 'verzonden', 'sent', 'date', 'received']\n",
    "\n",
    "    datesPattern = [{\"LOWER\" : {\"IN\" : days}, \"OP\" : \"?\"}, \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True, \"OP\" : \"?\"}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    regexMatches = regexMatcher(text)\n",
    "    return [doc[start:end].text for match_id, start, end in matches], regexMatches\n",
    "    \n",
    "\n",
    "\n",
    "def getDates(text, nlp):\n",
    "    months = ['januari', 'februari', 'maart', 'april', 'mei', 'juni', 'juli', 'augustus', 'september', 'oktober', 'november', 'december',\n",
    "         'january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december',\n",
    "         'jan', 'feb', 'mrt', 'apr', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'okt']\n",
    "\n",
    "    datesPattern = [ \n",
    "           {\"IS_DIGIT\": True}, \n",
    "           {\"LOWER\" : {\"IN\" : months}},\n",
    "           {\"IS_PUNCT\" : True, \"OP\" : \"?\", \"TEXT\":'.'},\n",
    "           {\"IS_DIGIT\": True}]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Dates\", [datesPattern])\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = removeKenmerkDate(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    return [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "def validate(dates, sep, pat):\n",
    "    goodDates = []\n",
    "    for date in dates:\n",
    "        date = date.replace(' ', '')\n",
    "        try:\n",
    "            date = date.replace(sep, ' ')\n",
    "            datetime.strptime(date, pat)\n",
    "            goodDates.append(date.replace(' ', sep))\n",
    "        except:\n",
    "            try:\n",
    "                if len(date.split(' ')) == 3 and len(date.split(' ')[2]) == 2:\n",
    "                    datetime.strptime(date, '%d %m %y')\n",
    "                    goodDates.append(date.replace(' ', sep))\n",
    "            except:\n",
    "                pass     \n",
    "    return goodDates\n",
    "            \n",
    "\n",
    "def regexMatcher(text):\n",
    "    results = []\n",
    "    \n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]', text), '/', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]\\/[0-1]{0,1}[0-9]\\/[0-9]{2,4}', text), '/', '%d %m %Y')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]', text), '-', '%d %m')\n",
    "\n",
    "    results += validate(re.findall('[0-3]{0,1}[0-9]-[0-1]{0,1}[0-9]-[0-9]{2,4}', text), '-', '%d %m %Y')\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "# converts dates to timestamp or yyyy-mm-dd\n",
    "def convertDate(date, timestamp=False):\n",
    "    months = {\n",
    "        'januari':'01','jan':'01',\n",
    "        'februari':'02','feb':'02',\n",
    "        'maart':'03','mrt':'03',\n",
    "        'april':'04','apr':'04',\n",
    "        'mei':'05','mei':'05',\n",
    "        'juni':'06','jun':'06',\n",
    "        'juli':'07','jul':'07',\n",
    "        'augustus':'08','aug':'08',\n",
    "        'september':'09','sep':'09',\n",
    "        'oktober':'10','okt':'10',\n",
    "        'november':'11','nov':'11',\n",
    "        'december':'12','dec':'12'\n",
    "    }\n",
    "\n",
    "\n",
    "    date = date.split(' ')\n",
    "    date[1] = months[date[1].lower()]\n",
    "\n",
    "    if timestamp:\n",
    "        return pd.Timestamp(year=int(date[2]), month=int(date[1]), day=int(date[0]))\n",
    "    if len(date[0]) == 1:\n",
    "        date[0] = '0' + date[0]\n",
    "    return date[2] + '-' + date[1] + '-' + date[0]\n",
    "\n",
    "# calculate days between two dates\n",
    "def days_between(d1, d2):\n",
    "    d1 = convertDate(d1)\n",
    "    d2 = convertDate(d2)\n",
    "    try:\n",
    "        d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "        d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# extracts all fields for dates\n",
    "def dateInformation(text):        \n",
    "    # find dates in text with date extractor\n",
    "    matches = getDates(text, nlp)\n",
    "\n",
    "    # first found date is date when document was written\n",
    "    # the date the wob request was completed\n",
    "    if len(matches) < 3:\n",
    "        return None, None, None, None\n",
    "    completedDate = matches[0]\n",
    "\n",
    "    # check if request was received on a different date then when it was send\n",
    "    # this is the case if it states \"ontvangen op\"\n",
    "    receivedDate = re.findall('ontvangen op ([^.]+?)\\,', text)\n",
    "    if not receivedDate:\n",
    "        receivedDate = matches[1]\n",
    "    else:\n",
    "        receivedDate = matches[2]\n",
    "\n",
    "    # calculate days between request and completion\n",
    "    daysTaken = days_between(completedDate, receivedDate)\n",
    "    \n",
    "    # converts to yyyy-mm-dd\n",
    "    start = convertDate(receivedDate)\n",
    "    end = convertDate(completedDate)\n",
    "    \n",
    "    # converts to np.datetime \n",
    "\n",
    "    start = np.datetime64(start)\n",
    "    end = np.datetime64(end)\n",
    "    businessDaysTaken = np.busday_count(start, end)\n",
    "    inTime = businessDaysTaken <= 42\n",
    "\n",
    "    # check if request was fulfilled wihtin 42 business days\n",
    "\n",
    "    return receivedDate, completedDate, daysTaken, inTime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the extractor used periods as indicators, abbreviations like N.V.T. need to be removed\n",
    "# this function converts it to N V T while keeping the periods at the end of sentences\n",
    "def removeAbbreviation(text):\n",
    "    sentence = ''\n",
    "\n",
    "    # split sentence in words\n",
    "    for word in text.split(' '):\n",
    "\n",
    "        # if there is no period in the word, add it back to sentence\n",
    "        if word.count('.') == 0 or '\\n' in word:\n",
    "            sentence += word + ' '\n",
    "            continue\n",
    "        \n",
    "        # when there is one period in word\n",
    "        elif word.count('.') == 1:\n",
    "\n",
    "            # if it is at the end, keep it and add word to sentence\n",
    "            if word[-1] == '.':\n",
    "                sentence += word + ' '\n",
    "            \n",
    "            # if its in the middle replace it with a space\n",
    "            else:\n",
    "                word = word.replace('.', ' ')\n",
    "                sentence += word + ' '\n",
    "        \n",
    "        # if there are  more than 1 periods, replace them all\n",
    "        else:\n",
    "            word = word.replace('.', ' ')\n",
    "            sentence += word + ' '\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "# edge case when the sidebar of the doc is earlier than the heading\n",
    "# in that case we have to remove a date that is not used\n",
    "def removeKenmerkDate(text):\n",
    "    text = text.split('\\n')\n",
    "    \n",
    "    toRemove = None\n",
    "    for i in range(len(text)):\n",
    "        if '(kenmerk)' in text[i] and 'ons' not in text[i]:\n",
    "            toRemove = [i, i+1]\n",
    "            break\n",
    "            \n",
    "    if toRemove:\n",
    "        text.remove(text[toRemove[0]])\n",
    "        text.remove(text[toRemove[1]])\n",
    "\n",
    "    text = '\\n'.join(text)                        \n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subject of request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to find the request reason from decision doc\n",
    "def getRequestReason(text, removeAbbr = True):\n",
    "\n",
    "    # text = rawText.replace('\\n', ' ')\n",
    "    text = text.lower()\n",
    "    if removeAbbr:\n",
    "        text = removeAbbreviation(text)\n",
    "    text = text[:1500]\n",
    "    patterns = ['verzocht([^.]+?)\\.', \n",
    "                'u verzoekt([^.]+?)\\.', \n",
    "                'om informatie over([^.]+?)\\.', \n",
    "                'uw verzoek ziet([^.]+?)\\.', \n",
    "                'om openbaarmaking van([^.]+?)\\.',\n",
    "                'uw verzoek([^.]+?)\\.']\n",
    "    matches = []\n",
    "\n",
    "    # get matches for all keywords\n",
    "    for pattern in patterns:\n",
    "        matches += re.findall(pattern, text)\n",
    "\n",
    "    uniqueMatches = []\n",
    "\n",
    "    # check all matches against eachother\n",
    "    for i in range(len(matches)):\n",
    "        for j in range(len(matches)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # if there is a duplicate we do not add it to uniqueMatches\n",
    "            if matches[i] in matches[j]:\n",
    "                break\n",
    "\n",
    "        # add match to uniqueMatches if the j loop completes\n",
    "        else:\n",
    "            uniqueMatches.append(matches[i])\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retreiving text\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets a random decision doc from a given ministry\n",
    "def getRandomDecisionDoc(ministrie):\n",
    "\n",
    "    # set paths \n",
    "    baseDFPath = '..\\\\data\\\\openstate data\\\\'\n",
    "    basePDFPath = 'F:\\\\Data files\\\\Master thesis\\\\verzoeken\\\\'\n",
    "\n",
    "    # get df of ministry\n",
    "    for f in os.listdir(baseDFPath):\n",
    "        if ministrie in f.lower():\n",
    "            file = f\n",
    "            break\n",
    "\n",
    "    # get dir of ministry \n",
    "    for d in os.listdir(basePDFPath):\n",
    "        if ministrie in d.lower():\n",
    "            dir = basePDFPath + d + '\\\\'\n",
    "            break\n",
    "    \n",
    "    requests = [x for x in os.listdir(dir) if x != '.DS_Store']\n",
    "    requestNr = int(random.choice(requests))\n",
    "    \n",
    "    # load in dataframe of ministry and get random sample\n",
    "    testDf = pd.read_excel(baseDFPath + file)\n",
    "    testDf.columns = [x.replace('\\n', '') for x in testDf.columns]    \n",
    "    s = testDf[testDf['WOB Verzoek'] == requestNr]\n",
    "\n",
    "    if not os.path.exists(dir + str(requestNr)):\n",
    "        return None, None, None\n",
    "\n",
    "    # find the desicion document\n",
    "    for p in os.listdir(dir + str(requestNr)):\n",
    "\n",
    "        # if found, save it in pdfPath\n",
    "        if 'besluit' in p.lower() and 'bijlage' not in p.lower() and p.endswith('.pdf'):\n",
    "            pdfPath = p\n",
    "            break\n",
    "    \n",
    "    # if there is no desicion document, try again\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "    return s, dir + str(requestNr) + '\\\\', pdfPath\n",
    "\n",
    "\n",
    "# extract text with pdftotext\n",
    "def textExtract(dir, name):\n",
    "    txtName = '.'.join(name.split('.')[0:-1])\n",
    "    txtName = dir + txtName  + '.txt'\n",
    "\n",
    "    # extract text\n",
    "    os.system(f'pdftotext -raw \"{dir}{name}\" \"{txtName}\"')\n",
    "    \n",
    "    if not os.path.exists(txtName): \n",
    "        return None\n",
    "    \n",
    "    # open file and return content\n",
    "    with open(txtName, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def textExtractPyPdf(name, nPages = 400):\n",
    "    reader = PyPDF2.PdfFileReader(name)\n",
    "    text = []\n",
    "    pageCounter = 0\n",
    "    for page in reader.pages:\n",
    "        page = page.extract_text()\n",
    "        print(page)\n",
    "        page = page.replace('\\n', ' ')\n",
    "\n",
    "        text.append(page)\n",
    "        if pageCounter == nPages:\n",
    "            break\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"path/to/file.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    print(first_page.chars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['                                                                                                          ',\n",
       " '                                                                         ',\n",
       " '                                                                     ',\n",
       " '                                                                   ',\n",
       " '                                                                                 ',\n",
       " '                                                                                      ',\n",
       " '                                                                                  ',\n",
       " '                                                                                               ',\n",
       " '                                                       ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textExtractPyPdf(r'F:\\Data files\\Master thesis\\verzoeken\\Wob verzoeken LNV\\202\\\\Bes-Besluit+op+Wob-verzoek+over+gesubsidieerde+sanering+van+varkenshouderijen.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inventory docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory(path, pdf):\n",
    "\n",
    "    # words to look for\n",
    "    rating = {'deels openbaar':0, 'niet openbaar':0, 'openbaar':0, 'reeds openbaar':0, \n",
    "            'geweigerd':0, 'gedeeltelijk openbaar':0,\n",
    "            'volledig openbaar': 0}\n",
    "    \n",
    "    # if an inventory document exists, use that\n",
    "    for file in os.listdir(path):\n",
    "        if 'inventaris' in file.lower():\n",
    "            rating = inventoryListToDataframe(path + file, rating)\n",
    "            break\n",
    "    \n",
    "    # else try to find inventory table in decision doc\n",
    "    if not rating:\n",
    "        rating = inventoryListToDataframe(path + pdf, rating)\n",
    "\n",
    "    if not rating:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # combine categories\n",
    "    notPublic = rating['niet openbaar'] + rating['geweigerd'] + rating['reeds openbaar']\n",
    "    partialPublic = rating['deels openbaar'] + rating['gedeeltelijk openbaar']\n",
    "    public = rating['openbaar'] - rating['niet openbaar'] - rating['reeds openbaar'] - partialPublic\n",
    "    total = public + notPublic + partialPublic\n",
    "\n",
    "    if total == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    return public, notPublic, partialPublic, public + notPublic + partialPublic\n",
    "\n",
    "\n",
    "# makes dataframes from table in pdf\n",
    "# then counts occurences of \n",
    "def inventoryListToDataframe(pdf, rating):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(pdf, pages='all')\n",
    "\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            return None\n",
    "\n",
    "        for table in tables:\n",
    "            for col in table.columns:\n",
    "                col = list(table[col])\n",
    "                col = [str(x).lower() for x in col]\n",
    "                for key in rating:\n",
    "                    for value in col:\n",
    "                        if key in value:\n",
    "                            rating[key] += 1\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other request metadata functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of pages in pdf documents\n",
    "def getNumberOfPages(path):\n",
    "    nPages = 0\n",
    "\n",
    "    # gets list of pdf files in a directory\n",
    "    pdfs = [x for x in os.listdir(path) if x.endswith('.pdf')]\n",
    "    \n",
    "    # open all files and count pages\n",
    "    for file in pdfs:\n",
    "        try:\n",
    "            with open(path + file, 'rb') as f:\n",
    "                pdf = PyPDF2.PdfFileReader(f, strict=False)\n",
    "                nPages += pdf.numPages\n",
    "        except:\n",
    "            return None\n",
    "    return nPages\n",
    "\n",
    "def nPagesOfPdf(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            pdf = PyPDF2.PdfFileReader(f, strict = False)\n",
    "            nPages = pdf.numPages\n",
    "        return nPages\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "\n",
    "# finds the number of considered documents in the decision doc\n",
    "def nDocs(text):\n",
    "\n",
    "    # look for the first mention of a number of documents\n",
    "    nDocuments = re.findall('[0-9]+? documenten[a-z]{2} aangetroffen', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        nDocuments = re.findall('[0-9]+? document[a-z]{2}', text)\n",
    "    \n",
    "    if not nDocuments:\n",
    "        if len(re.findall('één document', text)) == 1:\n",
    "            return 1\n",
    "\n",
    "    # return the first if found, else return None\n",
    "    if type(nDocuments) == list:\n",
    "        if len(nDocuments) == 0:\n",
    "            return None\n",
    "        return int(nDocuments[0].split(' ')[0])\n",
    "    elif type(nDocuments) == int:\n",
    "        return nDocuments\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatches(doc):\n",
    "\n",
    "    # list of entities to include\n",
    "    wantedTypes = ['FAC','GPE','LOC', 'ORG', 'PERSON']\n",
    "\n",
    "    bannedWords = ['sent', 'cc', 'for', 'te', 'we', 'to']\n",
    "\n",
    "    wantedEnts = []\n",
    "\n",
    "    # for all entities, check if to include and clean up the string\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in wantedTypes:\n",
    "            text = str(ent.text)\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'\\n', ' ', text)\n",
    "            text = text.strip()\n",
    "\n",
    "            if text.endswith('2e'):\n",
    "                continue\n",
    "            if text in bannedWords:\n",
    "                continue\n",
    "            if len(text) == 1:\n",
    "                continue\n",
    "                \n",
    "\n",
    "            wantedEnts.append(text)    \n",
    "    \n",
    "    # retrun list of entities \n",
    "    return wantedEnts\n",
    "\n",
    "# remove all entities that only occur in one document\n",
    "def goodEnts(entities):\n",
    "    ents = {}\n",
    "\n",
    "    # get a dict of entities as keys and a list of documents in which the entity occurs as values\n",
    "    for document in entities:\n",
    "        for ent in entities[document]:\n",
    "            if ent in ents:\n",
    "                ents[ent].append(document)\n",
    "            else:\n",
    "                ents[ent] = [document]\n",
    "\n",
    "    entsToDelete = []\n",
    "\n",
    "    # find and deletes entities that only occur in one document\n",
    "    for ent in ents:\n",
    "        language = langid.classify(ent)\n",
    "        if len(ents[ent]) <= 1:\n",
    "            entsToDelete.append(ent)\n",
    "        \n",
    "        # checks if the ent is english\n",
    "        if language[0] =='en' and language[1] < 0:\n",
    "            entsToDelete.append(ent)\n",
    "\n",
    "    for ent in set(entsToDelete):\n",
    "        del ents[ent]\n",
    "\n",
    "    documents = {}\n",
    "\n",
    "    # revert back to original format\n",
    "    # documents as keys, entities in said document as values\n",
    "    for ent in ents:\n",
    "        for doc in ents[ent]:\n",
    "            if doc in documents:\n",
    "                documents[doc].append(ent)\n",
    "            else:\n",
    "                documents[doc] = [ent]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def getEntities(text, name):\n",
    "    try:\n",
    "        text = re.sub('\\n+', '\\n', text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "\n",
    "        if langid.classify(text)[0] != 'nl':\n",
    "            return {name:[]}\n",
    "\n",
    "        NERdoc = nlp(text)\n",
    "        NERmatches = getMatches(NERdoc)\n",
    "        \n",
    "        # MinDoc = nlp_ministries(text)\n",
    "        # MinMatches = getMatches(MinDoc)\n",
    "    except:\n",
    "        return {name:[]}\n",
    "\n",
    "    return {name: list(set(NERmatches))}\n",
    "\n",
    "def orderCheck(x, y):\n",
    "    ordered = [x, y]\n",
    "    ordered.sort()\n",
    "    return ordered[0], ordered[1]\n",
    "\n",
    "def getEdges(entities):\n",
    "    entitiesDict = {}\n",
    "\n",
    "    for d in entities:\n",
    "        key = list(d.keys())[0]\n",
    "        entitiesDict[key] = d[key]\n",
    "\n",
    "    entitiesDict = goodEnts(entitiesDict)\n",
    "\n",
    "    edges = {}\n",
    "\n",
    "    for document in entitiesDict:\n",
    "        if len(entitiesDict[document]) <= 1:\n",
    "            continue\n",
    "        \n",
    "        for edge in itertools.combinations(entitiesDict[document], 2):\n",
    "            node1, node2 = orderCheck(edge[0], edge[1])\n",
    "            key = node1 + '\\t' + node2 \n",
    "            if key in edges:\n",
    "                edges[key].append(document)\n",
    "            else:\n",
    "                edges[key] = [document]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def makeGraph(pages):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    for i in range(len(pages)):\n",
    "        entities.append(getEntities(pages[i], i))\n",
    "\n",
    "    edges = getEdges(entities)\n",
    "    for edge in edges:\n",
    "        nodes = edge.split('\\t')\n",
    "        # G.add_edge(nodes[0], nodes[1])\n",
    "        G.add_edge(nodes[0], nodes[1], weight = len(edges[edge]))\n",
    "\n",
    "    if len(list(nx.connected_components(G))) > 1:\n",
    "        x = [len(c) for c in list(nx.connected_components(G))]\n",
    "        x.sort(reverse=True)\n",
    "        limit = x[1]\n",
    "        del x\n",
    "        for component in list(nx.connected_components(G)):\n",
    "            if len(component)<=limit:\n",
    "                for node in component:\n",
    "                    G.remove_node(node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostConnectedNodes(G, n = 10):\n",
    "    nodeNeighbors = [(node, len(list(G.neighbors(node)))) for node in G.nodes]\n",
    "    sortedNodes = sorted(nodeNeighbors, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return sortedNodes[:n]\n",
    "\n",
    "def mostCooccurences(G, n = 10):\n",
    "    weightedEdges =  G.edges(data='weight')\n",
    "    sortedEdges = sorted(weightedEdges, key=lambda tup: tup[2], reverse=True)\n",
    "    return sortedEdges[:n]\n",
    "\n",
    "def calculateBetweenness(G, n = 10):\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    betweenness = [(x, betweenness[x]) for x in betweenness]\n",
    "    betweenness = sorted(betweenness, key=lambda tup: tup[1], reverse=True)\n",
    "    return betweenness[:n]\n",
    "\n",
    "def addNodeWeight(d, node, weight):\n",
    "    if node in d:\n",
    "        d[node] += weight\n",
    "    else:\n",
    "        d[node] = weight\n",
    "    return d\n",
    "\n",
    "def calculateStrength(G, n = 10):\n",
    "    nodeStrenghts = {}\n",
    "\n",
    "    for edge in G.edges(data=True):\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        weight = edge[2]['weight']    \n",
    "\n",
    "        nodeStrenghts = addNodeWeight(nodeStrenghts, node1, weight)\n",
    "        nodeStrenghts = addNodeWeight(nodeStrenghts, node2, weight)\n",
    "    nodeStrenghts = [(x, nodeStrenghts[x]) for x in nodeStrenghts]\n",
    "    nodeStrenghts = sorted(nodeStrenghts, key=lambda tup: tup[1], reverse=True)\n",
    "    return nodeStrenghts[:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRequestMetadata(text, pdfPath, pdfName):\n",
    "    reason = getRequestReason(text)\n",
    "    if len(reason) == 0:\n",
    "        reason = ''\n",
    "    else:\n",
    "        match = reason[0]\n",
    "    receivedDate, completedDate, daysTaken, inTime = dateInformation(text)       \n",
    "    nDocuments = nDocs(text)\n",
    "    nPages = getNumberOfPages(pdfPath)\n",
    "    public, notPublic, partialPublic, total = inventory(pdfPath, pdfName)\n",
    "    \n",
    "    # tabula has a lot of output, this clears it\n",
    "    clear_output()\n",
    "\n",
    "    # convert dates to pd Timestamp dates to compare to ground truth      \n",
    "    try:\n",
    "        if receivedDate:\n",
    "            receivedDate = convertDate(receivedDate, True)\n",
    "    except:\n",
    "        receivedDate = None\n",
    "    try:\n",
    "        if completedDate:\n",
    "            completedDate = convertDate(completedDate, True)\n",
    "    except:\n",
    "        completedDate = None\n",
    "\n",
    "    # if number of documents was not found in text, use total docs from inventory list\n",
    "    if not nDocuments and total:\n",
    "        nDocuments = total        \n",
    "\n",
    "    # calculate days per doc\n",
    "    if daysTaken and nDocuments:\n",
    "        daysPerDoc = round(daysTaken / nDocuments, 2)\n",
    "    elif total and daysTaken:\n",
    "        daysPerDoc = round(daysTaken / total, 2)\n",
    "    else:\n",
    "        daysPerDoc = None\n",
    "\n",
    "    return (reason, receivedDate, completedDate, daysTaken, inTime, nPages, public, notPublic, partialPublic, nDocuments, daysPerDoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [_reader.py:1453]\n"
     ]
    }
   ],
   "source": [
    "def getText(requestDir):\n",
    "    pageLimit = 400\n",
    "    totalPages = 0\n",
    "    text = []\n",
    "\n",
    "    for file in os.listdir(requestDir):\n",
    "        if file.startswith('Bes') and file.endswith('.pdf'):\n",
    "            decisionDocPath = requestDir + file\n",
    "            decisionDocName = file\n",
    "            decisionDocPages = textExtractPyPdf(decisionDocPath)\n",
    "            decisionDocText = ''\n",
    "            for page in decisionDocPages:\n",
    "                decisionDocText += page + '\\n'\n",
    "            totalPages = nPagesOfPdf(decisionDocPath)\n",
    "\n",
    "        if file.endswith('.pdf') and not file.startswith('Bes'):\n",
    "            fileName = requestDir + file\n",
    "            numberOfPages = nPagesOfPdf(fileName)\n",
    "            totalPages += nPagesOfPdf(fileName)\n",
    "            if numberOfPages + totalPages > pageLimit:\n",
    "                test =  textExtractPyPdf(fileName, nPages = numberOfPages - (totalPages - pageLimit))\n",
    "                text += test\n",
    "                break\n",
    "            test =  textExtractPyPdf(fileName, nPages = numberOfPages - (totalPages - pageLimit))\n",
    "            text += test\n",
    "\n",
    "    text = [x.replace('\\n', ' ') for x in text]\n",
    "    return decisionDocName, decisionDocText, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictonaryCounter(d):\n",
    "    result = {}\n",
    "    for item in d:\n",
    "        if item in result:\n",
    "            result[item] += 1\n",
    "        else:\n",
    "            result[item] = 1\n",
    "\n",
    "    result = [(x, result[x]) for x in result]\n",
    "    result = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requestDir = r'F:\\Data files\\Master thesis\\verzoeken\\WOB-verzoeken BZK\\71\\\\'\n",
    "\n",
    "requestDir = r'F:\\Data files\\Master thesis\\verzoeken\\Wob verzoeken LNV\\202\\\\Bes-Besluit+op+Wob-verzoek+over+gesubsidieerde+sanering+van+varkenshouderijen.pdf'\n",
    "# gets text\n",
    "decisionDocName, decisionDocText, text = getText(requestDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None None None None 933 92 14 211 317 None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gets metadata from request\n",
    "match, receivedDate, completedDate, daysTaken, inTime, nPages, public, notPublic, partialPublic, nDocuments, daysPerDoc = extractRequestMetadata(decisionDocText, requestDir, decisionDocName)\n",
    "print(match, receivedDate, completedDate, daysTaken, inTime, nPages, public, notPublic, partialPublic, nDocuments, daysPerDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('31 december', 73), ('31 december 2019', 55), ('31-12', 41), ('31-12-2019', 21), ('18-2', 15), ('9 november', 14), ('31-12-2018', 14), ('9 februari', 11), ('14 december', 10), ('1 januari', 10)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extract dates\n",
    "spacyDates, regexDates = getDatesBase(' '.join(text), nlp)\n",
    "dates = spacyDates + regexDates\n",
    "dates = dictonaryCounter(dates) \n",
    "print(dates[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ministerie van binnenlandse zaken', 18),\n",
       " ('bzk', 5),\n",
       " ('defensie', 4),\n",
       " ('ministerie van buitenlandse zaken', 2),\n",
       " ('ministerie van bzk', 2),\n",
       " ('ministerie van defensie', 1),\n",
       " ('ministerie van vws', 1),\n",
       " ('vws', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# extract ministries\n",
    "doc = nlp_ministries(' '.join(text))\n",
    "mins = dictonaryCounter([ent.text.lower() for ent in doc.ents])\n",
    "mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cda', 0.18618709241713685), ('christenunie', 0.1276681274734423), ('nederland', 0.10214585013255557), ('cda activiteitenverslag', 0.06522559836927351), ('den haag', 0.06475944577503105), ('europees parlement', 0.050019829612994525), ('wi', 0.045982560642835), ('cdja', 0.04500487555616356), ('europa', 0.04259117707101891), ('tweede kamer', 0.038336822838858996)]\n",
      "[('cda', 674), ('cda activiteitenverslag', 359), ('christenunie', 323), ('nederland', 281), ('wi', 262), ('cdja', 262), ('europa', 252), ('europees parlement', 234), ('den haag', 191), ('wetenschappelijk instituut', 189)]\n",
      "[('cda', 263), ('cda activiteitenverslag', 189), ('nederland', 165), ('cdja', 158), ('europa', 143), ('europees parlement', 141), ('christenunie', 140), ('wi', 130), ('tweede kamer', 120), ('den haag', 114)]\n",
      "[('cda', 'cda activiteitenverslag', 23), ('cda', 'cdja', 19), ('amersfoort', 'christenunie', 18), ('cda', 'europa', 17), ('christenunie', 'wetenschappelijk instituut', 16), ('wi', 'wetenschappelijk instituut', 15), ('cda', 'nederland', 14), ('wi', 'christenunie', 13), ('europees parlement', 'cda', 12), ('den haag', 'cda', 11)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# named entites with graph\n",
    "G = makeGraph(text)\n",
    "between = calculateBetweenness(G)\n",
    "strength = calculateStrength(G)\n",
    "connected = mostConnectedNodes(G)\n",
    "cooccur = mostCooccurences(G)\n",
    "print(between)\n",
    "print(strength)\n",
    "print(connected)\n",
    "print(cooccur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
