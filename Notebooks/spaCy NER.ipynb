{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sticky-unemployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "familiar-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-reading",
   "metadata": {},
   "source": [
    "This notebook is about testing the accuracy of the spaCy NER model. This was done by using a pre-exisiting labeled dataset. The code in this notebook is to answer 2.1.\n",
    "\n",
    "\n",
    "The first part is all of the data handeling. It took quite some effort to get the data in the correct format to actually be  able to calculate the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "falling-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGroundTruth():\n",
    "    with open('..\\\\data\\\\ner labeled data\\\\test.conllu', 'r', encoding='utf8') as f:\n",
    "        ground = f.read()\n",
    "        ground = ground.split('\\n')\n",
    "        ground = [x.split('\\t') for x in ground]\n",
    "    \n",
    "    text = []\n",
    "    for word in ground:\n",
    "        try:\n",
    "            text.append(word[1])\n",
    "        except IndexError:\n",
    "            text.append('\\n')\n",
    "    \n",
    "\n",
    "    text = ' '.join(text)\n",
    "    return text, ground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "saved-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function makes sure that an index refers to the same token in both the ground truth and the doc\n",
    "# its not a robust approach but it works\n",
    "def sync(ground, doc):\n",
    "    \n",
    "    # newlines in ground truth are given as an empty string in a list\n",
    "    # spacy doesnt do tokens for newlines so these can be removed\n",
    "    ground = [x for x in ground if x != ['']]\n",
    "    \n",
    "    groundIndex, docIndex = 0, 0\n",
    "    docNew, groundNew = [], []\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex+2 > len(ground) or docIndex+2 > len(doc):\n",
    "            return groundNew, docNew\n",
    "        \n",
    "        \n",
    "        if str(doc[docIndex].text) != ground[groundIndex][1]:\n",
    "            # see if next token in ground truth equals current doc token\n",
    "            if str(doc[docIndex].text) == ground[groundIndex + 1][1]:\n",
    "                groundIndex +=1\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex + 2][1]:\n",
    "                groundIndex +=2\n",
    "\n",
    "            # see if next token in doc equals current ground truth token\n",
    "            elif str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=1\n",
    "            elif str(doc[docIndex + 2].text) == ground[groundIndex ][1]:\n",
    "                docIndex+=2\n",
    "\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex + 1].text) == ground[groundIndex ][1]:\n",
    "                docIndex += 2\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex ][1] + ground[groundIndex + 1][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 2\n",
    "\n",
    "            # checks if doc split a token that ground truth didnt\n",
    "            elif str(doc[docIndex].text) + str(doc[docIndex+1].text)+ str(doc[docIndex+2].text) == ground[groundIndex][1]:\n",
    "                docIndex += 3\n",
    "                groundIndex += 1\n",
    "\n",
    "            # checks if ground split a token that doc didnt\n",
    "            elif str(doc[docIndex].text) == ground[groundIndex][1] + ground[groundIndex+1][1] + ground[groundIndex+2][1]:\n",
    "                docIndex += 1 \n",
    "                groundIndex += 3\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(str(doc[docIndex].text), str(doc[docIndex+1].text),str(doc[docIndex+2].text)) \n",
    "                print(ground[groundIndex][1],ground[groundIndex+1][1],ground[groundIndex+2][1])\n",
    "                return groundNew, docNew\n",
    "        \n",
    "        # add good tokens to new lists\n",
    "        groundNew.append((ground[groundIndex][1], ground[groundIndex][2]))\n",
    "        docNew.append((str(doc[docIndex].text), doc[docIndex].ent_type_))\n",
    "            \n",
    "        groundIndex += 1\n",
    "        docIndex += 1\n",
    "\n",
    "def testSync(ground, doc):\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i][0] != ground[i][0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mental-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets the span of all entities in ground truth\n",
    "def getSpans(ground):\n",
    "    span = []\n",
    "    i = 0\n",
    "    flag = False\n",
    "    begin = 0\n",
    "    \n",
    "    while i < len(ground):\n",
    "        \n",
    "        # base case: no continuation of entiy or start of entity\n",
    "        if ground[i][1] == 'O' and flag == False:\n",
    "            pass\n",
    "        \n",
    "        # end of entity span no new entity, reset flag and add entity\n",
    "        elif ground[i][1] == 'O' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            flag = False\n",
    "        \n",
    "        # end of entity span, new entity starts. Reset flag, add entity, and start new entity\n",
    "        elif ground[i][1][0] == 'B' and flag == True:\n",
    "            span.append(((begin, i), ground[begin][1]))\n",
    "            begin = i\n",
    "        \n",
    "        # start of new entity, set flag and begin\n",
    "        elif ground[i][1][0] == 'B' and flag == False:\n",
    "            flag = True\n",
    "            begin = i\n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heard-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "def savePredictions(predictions):\n",
    "    begin = [x[0][0] for x in predictions]\n",
    "    end = [x[0][1] for x in predictions]\n",
    "    entType = [x[1] for x in predictions]\n",
    "    ent = [x[2] for x in predictions]\n",
    "    pred = pd.DataFrame.from_dict({'begin':begin, 'end':end, 'entType':entType, 'ent':ent})\n",
    "    pred.to_csv('..\\\\data\\\\predictions.csv')\n",
    "\n",
    "# get entities found by spacy in correct format\n",
    "def getPredictionSpans(doc, tokens):\n",
    "    spans = []\n",
    "    \n",
    "    i = 0\n",
    "    entityIndex = 0\n",
    "    \n",
    "    # check for every entity\n",
    "    while i < len(tokens):\n",
    "        try:\n",
    "            \n",
    "            # if current token has an entity label\n",
    "            if tokens[i][1] != '':\n",
    "                \n",
    "                # get entity type and string representation\n",
    "                entity = doc.ents[entityIndex]\n",
    "                entityType = doc[entity.start].ent_type_\n",
    "                entity = str(entity)\n",
    "                \n",
    "                # get number of tokens in entity and add to list\n",
    "                nTokens = len(entity.split(' '))\n",
    "                spans.append(((i, i + nTokens), entityType, entity))\n",
    "                \n",
    "                # increase token index by number of tokens in current entity\n",
    "                i += nTokens\n",
    "                \n",
    "                # set entity index to next\n",
    "                entityIndex += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "        except IndexError:\n",
    "            savePredictions(spans)\n",
    "            return\n",
    "        \n",
    "    \n",
    "    savePredictions(spans)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "informal-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions from csv\n",
    "def getPredictions():\n",
    "    predictions = pd.read_csv('..\\\\data\\\\predictions.csv')\n",
    "    begin = list(predictions.begin)\n",
    "    end = list(predictions.end)\n",
    "    entType = list(predictions.entType)\n",
    "    ent = list(predictions.ent)\n",
    "    \n",
    "    predictionsList = []\n",
    "    for i in range(len(begin)):\n",
    "        predictionsList.append(((begin[i], end[i]), entType[i], ent[i]))\n",
    "        \n",
    "    return predictionsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "silver-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = full text as string\n",
    "# groundOld = tokens with ground truth labels\n",
    "# doc = spacy doc of full text\n",
    "# ground = list of tokens with ground truth labels in sync with docList\n",
    "# docList = list of tokens from spacy with predicted labels in sync with ground\n",
    "# predictions = list of predicted entities by spacy with begin, end, type and text\n",
    "# span = list of ground truth entites with begin, end, type\n",
    "\n",
    "\n",
    "text, groundOld = loadGroundTruth()\n",
    "doc = nlp(text[:1000000])\n",
    "ground, docList = sync(groundOld, doc)\n",
    "predictions = getPredictions()\n",
    "span = getSpans(ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-mouth",
   "metadata": {},
   "source": [
    "The last 4 cells contian the code that does the actual evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "enclosed-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1Strict(cor, inc, spu, mis):\n",
    "    print(cor,inc,spu,mis)\n",
    "    recall = cor / (cor+inc+mis)\n",
    "    precision = cor / (cor+inc+spu)\n",
    "    f1 = 2 * ((recall * precision) / (recall + precision))\n",
    "    print('recall', recall)\n",
    "    print('precision', precision)\n",
    "    print('f1', f1)\n",
    "    results = {'total':cor + inc + spu + mis,\n",
    "              'correct':cor,\n",
    "              'incorrect':inc,\n",
    "              'missing':mis,\n",
    "              'spurious':spu,\n",
    "              'precision':precision,\n",
    "              'recall':recall,\n",
    "              'f1':f1}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNER(ground, pred, method):\n",
    "    correct, incorrect, spurious, missing = 0, 0, 0, 0\n",
    "    \n",
    "    spacyBanList = ['CARDINAL','DATE','LAW','MONEY','ORDINAL','PERCENT','QUANTITY','TIME']\n",
    "    cats = {'B-ORG': ['ORG'],\n",
    "           'B-PER': ['PERSON'],\n",
    "           'B-LOC': ['FAC', 'GPE', 'LOC'],\n",
    "           'B-ORG': ['ORG']}\n",
    "    \n",
    "    groundIndex = 0\n",
    "    predIndex = 0\n",
    "    \n",
    "    while True:\n",
    "        if groundIndex >= len(ground) or predIndex >= len(pred):\n",
    "            return calcF1Strict(correct, incorrect, spurious, missing)\n",
    "        \n",
    "        # set current tokens \n",
    "        groundEnt = ground[groundIndex]\n",
    "        predEnt = pred[predIndex]\n",
    "        \n",
    "        # correct span\n",
    "        if groundEnt[0] == predEnt[0]:\n",
    "            if method == 'exact':\n",
    "                correct += 1\n",
    "            \n",
    "            else:\n",
    "                # correct type\n",
    "                if groundEnt[1] == 'B-MISC':\n",
    "                    correct += 1\n",
    "\n",
    "                # also correct type\n",
    "                elif predEnt[1] in cats[groundEnt[1]]:\n",
    "                    correct += 1\n",
    "\n",
    "                # not correct type\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                \n",
    "            groundIndex += 1\n",
    "            predIndex += 1\n",
    "                \n",
    "        # no overlap between spans\n",
    "        elif groundEnt[0][0] > predEnt[0][1]:\n",
    "            # ground is higher, increase predEnt\n",
    "            # spurious\n",
    "            # check ents: some do not count\n",
    "            if predEnt[1] not in spacyBanList:\n",
    "                spurious += 1\n",
    "            \n",
    "            predIndex += 1\n",
    "\n",
    "        elif groundEnt[0][1] < predEnt[0][0]:\n",
    "            # ground is lower, increase ground\n",
    "            # missing\n",
    "            groundIndex += 1\n",
    "            missing += 1\n",
    "        \n",
    "        # overlap between spans\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            groundIndex += 1\n",
    "            predIndex += 1   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-glory",
   "metadata": {},
   "source": [
    "## Strict evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "necessary-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10359 1935 1093 1029\n",
      "recall 0.7775275838775051\n",
      "precision 0.7738104130873236\n",
      "f1 0.7756645451141895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10359,\n",
       " 'incorrect': 1935,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.7738104130873236,\n",
       " 'recall': 0.7775275838775051,\n",
       " 'f1': 0.7756645451141895}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'strict')\n",
    "with open('..\\\\data\\\\results\\\\ner_strict_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-southwest",
   "metadata": {},
   "source": [
    "## Excact evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "gross-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976 1318 1093 1029\n",
      "recall 0.8238384748179839\n",
      "precision 0.8198999028908642\n",
      "f1 0.8218644702358667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 14416,\n",
       " 'correct': 10976,\n",
       " 'incorrect': 1318,\n",
       " 'missing': 1029,\n",
       " 'spurious': 1093,\n",
       " 'precision': 0.8198999028908642,\n",
       " 'recall': 0.8238384748179839,\n",
       " 'f1': 0.8218644702358667}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evalNER(span, predictions, 'exact')\n",
    "with open('..\\\\data\\\\results\\\\ner_exact_results,json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-antique",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These are really good scores for the model, especially the exact one. But we need to keep in mind that the permorance of a NER model can depend on the sort of text. It could be that the spaCy model was trained on documents that have a very high resemblance to this test set. The performance on the WOB document can be lower than these results suggest.\n",
    "\n",
    "For the evaluation, spacy has a lot more things it looks for, like monotary values or percentages. These were skipped, so if spacy found an entity that the ground truth didnt have and the label was one of the banned labels, it was not considered a spurious match. The law label was also ignored because that will be done better by hand (hopefully)\n",
    "\n",
    "There are also some limitations concerning the language. The spaCy model uses the dutch language pack as most docs are in dutch, but there are some documents in other languages, mostly english."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-aside",
   "metadata": {},
   "source": [
    "## Manual eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "similar-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string):\n",
    "    print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    \n",
    "def showMatches(doc, entities):   \n",
    "    indexOfMatches = []\n",
    "    for ent in entities:\n",
    "        for i in range(int(ent.start), int(ent.end)):\n",
    "            indexOfMatches.append(i)\n",
    "\n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "            \n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "\n",
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'q':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, q or nothing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indoor-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResults(r):\n",
    "    precision = r['correct'] / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = r['correct'] / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mature-graduation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n",
      "Met vriendelijke groet , \n",
      " 10.2e \n",
      " 10.2.e \n",
      " VNO-NCW en \u001b[1;31mMKB-Nederland \u001b[0m\u001b[1;31m\n",
      " \u001b[0m\u001b[1;31mo7010:2e//06102e \u001b[0m\u001b[1;31m\n",
      " \u001b[0m\u001b[1;31m[ \u001b[0m0'2-e@vnoncwâ€”mkb.nl \n",
      " WWWw.vno-ncw.nl en www.mkb.nl \n",
      " \u001b[1;31mBezuidenhoutseweg \u001b[0m\u001b[1;31m12 \u001b[0m, \u001b[1;31m2594 \u001b[0m\u001b[1;31mAV \u001b[0m\u001b[1;31mDEN \u001b[0m\u001b[1;31mHAAG \u001b[0m\u001b[1;31m\n",
      " \u001b[0m\u001b[1;31mPostbus \u001b[0m93002 , 2509 \u001b[1;31mAA \u001b[0m\u001b[1;31mDEN \u001b[0mHAAG \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "MKB-Nederland o7010:2e//06102e [ ORG\n",
      "Bezuidenhoutseweg 12 FAC\n",
      "2594 ORG\n",
      "AV DEN ORG\n",
      "HAAG Postbus ORG\n",
      "AA DEN ORG\n",
      "6\n",
      "correct\n",
      "incorrect5\n",
      "missing\n",
      "spurious1\n",
      "precision = 0.5375302663438256, recall = 0.556390977443609, f1 = 0.5467980295566501\n"
     ]
    }
   ],
   "source": [
    "def evalSpacy():    \n",
    "    resultsDir = 'C:\\\\Users\\\\justin\\\\OneDrive - UvA\\\\Studie\\\\Data Science\\\\Thesis\\\\Knowledge extraction\\\\data\\\\results\\\\'\n",
    "    \n",
    "    if 'spacyManualStrict.json' in os.listdir(resultsDir):\n",
    "        with open(resultsDir + 'spacyManualStrict.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "            \n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'emails' :0,\n",
    "            'ocr':0,\n",
    "            'context':0,\n",
    "            'other':0\n",
    "        }\n",
    "\n",
    "    total = results['correct'] + results['incorrect'] + results['spurious'] + results['missing']\n",
    "    print(total)\n",
    "    while total < 500:\n",
    "        \n",
    "        # clear previous page\n",
    "        clear_output()\n",
    "        print(total)\n",
    "        \n",
    "        # select new page, clean it a bit, and create a spacy doc\n",
    "        sample = df.sample(1)\n",
    "        text = sample.text.values[0]\n",
    "        text = re.sub('\\n+', '\\n', text)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # select only entities of person , location and organisation\n",
    "        entities = [ent for ent in doc.ents if ent.label_ in [\"ORG\", 'FAC', 'GPE', 'LOC', 'PERSON']]\n",
    "        \n",
    "        # print text with ents highlited\n",
    "        showMatches(doc, entities)\n",
    "        \n",
    "        print('\\n\\n')\n",
    "        \n",
    "        # print entities with label\n",
    "        for ent in entities:\n",
    "            print(ent.text.replace('\\n', ' '), ent.label_)\n",
    "        print(len(entities))\n",
    "        \n",
    "        # handle inputs\n",
    "        for key in ['correct','incorrect','missing','spurious']:\n",
    "            value = inputHandling(key)\n",
    "            if value == -1:\n",
    "                return results\n",
    "            results[key] += value\n",
    "        \n",
    "        # calculate total entitites checked\n",
    "        total = results['correct'] + results['incorrect'] + results['spurious'] + results['missing']\n",
    "        \n",
    "        # save results\n",
    "        with open(resultsDir + 'spacyManualStrict.json', 'w') as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "    return results\n",
    "\n",
    "results = evalSpacy()\n",
    "results = calcResults(results)\n",
    "with open('..\\\\data\\\\results\\\\spacyManualStrict.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
