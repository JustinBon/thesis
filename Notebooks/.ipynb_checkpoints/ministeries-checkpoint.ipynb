{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greater-sessions",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n",
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cupy\\_environment.py:214: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  'CUDA path could not be detected.'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functioning-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "close-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ministries = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")\n",
    "df = pd.read_csv('..\\\\data\\\\ocred\\\\files_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-removal",
   "metadata": {},
   "source": [
    "This notebook is for rq 2.2.2\n",
    "\n",
    "There were a lot of itterations for this extractor. These will be first. At the end, the extractor that was used is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cubic-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHilight(string, extraSpace = True):\n",
    "    if extraSpace:\n",
    "        print('\\x1b[1;31m'+string + ' ' +'\\x1b[0m', end='')\n",
    "    else:\n",
    "        print('\\x1b[1;31m'+string + '\\x1b[0m', end='')\n",
    "\n",
    "def printHilightUnderline(string):\n",
    "    print('\\033[4m\\033[96m'+string  +'\\033[96m\\x1b[0m' + ' ', end='')\n",
    "    \n",
    "def inputHandling(message):\n",
    "    while(True):\n",
    "        i = input(message)\n",
    "        if i == 'q':\n",
    "            return -1\n",
    "        \n",
    "        elif i == '':\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            i = int(i)\n",
    "            return i\n",
    "        \n",
    "        except:\n",
    "            print(\"input number, q or nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates f1, precision and recall with the partial method\n",
    "def calcResultsMinistries(r):\n",
    "    precision = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['spurious'])\n",
    "    recall = (r['correct'] + (0.5 * r['partial'])) / (r['correct']+r['incorrect']+r['missing'])\n",
    "    f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "    r['precision'] = precision\n",
    "    r['recall'] = recall\n",
    "    r['f1'] = f1 \n",
    "    print(f'precision = {precision}, recall = {recall}, f1 = {f1}')\n",
    "    print(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overhead-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of current dutch ministries and their abbriviations\n",
    "def getMinisteries(getList):\n",
    "    \n",
    "    # get html of the wiki page of dutch ministries\n",
    "    page = requests.get('https://nl.wikipedia.org/wiki/Lijst_van_Nederlandse_ministeries')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"td\")[-1]\n",
    "    \n",
    "    # get all links in the html\n",
    "    results.find_all('a', href = True)\n",
    "    wikis = {}\n",
    "    \n",
    "    abrr = []\n",
    "    for item in str(results.find_all('p')[0]).split('\\n')[:-1] + str(results.find_all('p')[1]).split('\\n')[1:-1]:\n",
    "        temp = re.findall('(?<=\\()(.*?)(?=\\))', item)\n",
    "        if temp == []:\n",
    "            abrr.append(None)\n",
    "        elif temp[-1] == 'Nederland':\n",
    "            abrr.append(None)\n",
    "            if 'Overzeese Gebiedsdelen' in item:\n",
    "                abrr.append(None)\n",
    "        else:\n",
    "            abrr.append(temp[-1].replace('&amp;', '&'))\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for ministerie in results.find_all('a')[:12]:\n",
    "        wikis[ministerie.text] = {'Link': 'https://nl.wikipedia.org' + ministerie['href'], 'Abbriviation' : abrr[counter]}\n",
    "        counter += 1\n",
    "    \n",
    "    if getList:\n",
    "        minList = list(wikis.keys()) + [wikis[x]['Abbriviation'] for x in wikis.keys()]\n",
    "        for x in minList:\n",
    "            temp+=x.replace(',', '').split(' ')\n",
    "\n",
    "        return [x.lower() for x in temp if x != 'en' and x != ''] + ['ezk']\n",
    "    else:\n",
    "        return wikis\n",
    "    \n",
    "minList = getMinisteries(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "obvious-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showMatchesOld(text, matches, minList):   \n",
    "    charIndex = 0\n",
    "    mIndex = 0\n",
    "\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.split(' ')\n",
    "    flag = False\n",
    "\n",
    "\n",
    "    for word in text:\n",
    "        startMatch = matches[mIndex][0]\n",
    "        endMatch = matches[mIndex][1]\n",
    "        # continuation of match\n",
    "        if flag:\n",
    "\n",
    "            # if end of match\n",
    "            if endMatch <= charIndex + len(word):\n",
    "                printHilight(word[:endMatch - charIndex], False)\n",
    "                print(word[endMatch - charIndex:], end=' ')\n",
    "\n",
    "                flag = False\n",
    "                mIndex += 1\n",
    "            # if end of match\n",
    "            else:\n",
    "                printHilight(word)\n",
    "\n",
    "        # match\n",
    "        elif startMatch == charIndex and endMatch == charIndex + len(word):\n",
    "            printHilight(word)\n",
    "            mIndex += 1\n",
    "\n",
    "        # first part of a match\n",
    "        elif startMatch == charIndex:\n",
    "            printHilight(word)\n",
    "            flag = True\n",
    "\n",
    "        # partial match\n",
    "        elif startMatch < charIndex + len(word):\n",
    "\n",
    "            print(word[:startMatch - charIndex], end='')\n",
    "            if endMatch <= charIndex + len(word):\n",
    "                printHilight(word[startMatch - charIndex:endMatch - charIndex], False)\n",
    "                print(word[endMatch - charIndex:], end=' ')\n",
    "                mIndex += 1\n",
    "            else:\n",
    "                printHilight(word[startMatch - charIndex:endMatch - charIndex])\n",
    "                flag = True\n",
    "\n",
    "\n",
    "        # potential uncaught match\n",
    "        elif word.lower() in minList:\n",
    "            printHilightUnderline(word)\n",
    "\n",
    "        # normal word\n",
    "        else:\n",
    "            print(word, end=' ')\n",
    "\n",
    "        charIndex += len(word) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "iraqi-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinisteries(minList):\n",
    "    \n",
    "    # gets gazetteers for ministries\n",
    "    ministeries = getMinisteries(False)\n",
    "    abrr = [ministeries[x]['Abbriviation'].lower() for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    mins = [x.lower() for x in list(ministeries.keys())]\n",
    "    print(mins)\n",
    "    \n",
    "    # variants of the extractor\n",
    "    variants = [('full', True, True), ('broad', True, False), ('no_prefix', False, False)]\n",
    "    output = {}\n",
    "\n",
    "    # if results already exist, this will load them\n",
    "    # otherwise it will create a new results set\n",
    "    for v in variants:\n",
    "        if os.path.isfile('..\\\\data\\\\results\\\\min_old_' + v[0] + '.json'):\n",
    "            with open('..\\\\data\\\\results\\\\min_old_' + v[0] + '.json') as f:\n",
    "                output[v[0]] = json.load(f)\n",
    "            print('Loaded previous output for ', v[0])\n",
    "        else:\n",
    "            print('new output for ' + v[0])\n",
    "            output[v[0]] = {\n",
    "                'abbr':v[1],\n",
    "                'lookForMin':v[2],\n",
    "                'correct':0,\n",
    "                'partial':0,\n",
    "                'missing':0,\n",
    "                'spurious':0,\n",
    "            }\n",
    "    \n",
    "\n",
    "    minList = set(getMinisteries(True))\n",
    "    while(True):\n",
    "        try:\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for i in output:\n",
    "                results = output[i]\n",
    "                print('version', i)\n",
    "                print(sum(list(results.values())[2:6] ))\n",
    "                print(sample.name.values[0], sample.page.values[0])\n",
    "\n",
    "                if results['abbr']:\n",
    "                    allMinisteries = mins + abrr\n",
    "                else:\n",
    "                    allMinisteries = mins\n",
    "\n",
    "                matches = []\n",
    "                for ministerie in allMinisteries:\n",
    "\n",
    "                    if results['lookForMin']:        \n",
    "                        matches += [(x.start(), x.end()) for x in re.finditer('ministerie van ' + ministerie.lower(),text.lower())]       \n",
    "                    else:\n",
    "                        matches += [(x.start(), x.end()) for x in re.finditer(ministerie.lower(),text.lower())]    \n",
    "\n",
    "                print('n matches: ', len(matches), '\\n\\n')\n",
    "                matches = sorted(matches, key=lambda tup: tup[0])\n",
    "                matches.append((len(text), len(text) + 1))\n",
    "                showMatchesOld(text, matches, minList)\n",
    "\n",
    "\n",
    "                for case in list(results.keys())[2:6]:\n",
    "                    result = inputHandling(case)\n",
    "                    if result == -1:\n",
    "                        output[i] = results\n",
    "                        return output\n",
    "                    else:\n",
    "                        results[case] += result\n",
    "\n",
    "                clear_output()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "through-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version full\n",
      "39\n",
      "0272cdb141e62321341591f0959794a2_wob-documenten 400\n",
      "n matches:  0 \n",
      "\n",
      "\n",
      "32480014 Wij zijn verheugd dat de leden van de D66-fractie de doelstellingen van dit wetsvoorstel onderschrijven en positief gestemd zijn over aanpassingen die naar aanleiding van de consultatiefase zijn aangebracht. Tegelijkertijd is ons duidelijk dat naar het oordeel van de leden van de D66-fractie nog aanpassingen noodzakelijk zijn. Daarop gaan wij graag in in de beant- woording van de specifieke vragen die daarover zijn gesteld. 7 - GroenLinks De leden van de GroenLinks-fractie hebben kennisgenomen van het voorliggende wetsvoorstel. Zij hebben over het wetsvoorstej een behoorlijk aantal vragen die zij graag aan de regering voorleggen. Tevens willen zij op deze plek aan de regering vragen of en zo ja, wanneer de maatregelen die afgelopen week door het kabinet werden aangekondigd over mogelijke quarantaineverplichtingen, het verplicht meewerken aan een test en het verplicht meewerken aan het bron- en contactonderzoek concreet aan de Kamer worden voorgelegd. Worden eventuele aanvul- lende wettelijke bepalingen via cen nota van wijziging bij dit wetsvoorstel aan de Kamer voorgelegd? Zo nee, op welke wijze en wanneer dan wel? Dit wetsvoorstel heeft hierop geen betrekking. Wij verwijzen naar antwoord 3. De leden van de SP-fractie zien dat het Corona-virus al veel slachtoffers heeft gemaakt en de samenleving ernstig heeft ontwricht. De bestrijding van het virus is dan ook van groot belang. Daarbij is het echter ook essentieel dat de democratie niet buiten spel gezet wordt. Het virus indammen gaat immers niet zonder dat er draagvlak voor de maatregelen is in de samenleving. Deze leden hebben daarom twijfels over het voorgestelde wetsvoorstel en zullen deze hieronder bespreken en voorleggen. Wij hopen met onze beantwoording de twijfels van de leden van de SP-fractie te kunnen wegnemen en hen ervan te kunnen overtuigen dat dit wetsvoorstel het tegendeel beoogt van het buitenspel zetten van de democratie. Met deze leden onderschrijven wij het belang van draagvlak in de samenleving voor maatregelen die er inderdaad op zijn gericht ontwrichting van die samenleving door verspreiding van het virus te voorkomen. De leden van de PvdA-fractie hebben met belangstelling van het onder- havige wetsvoorstel kennisgenomen. Zij delen de mening van de regering dat noodverordeningen niet langer als juridische basis voor soms vergaande coronamaatregelen zouden moeten gelden. Om die reden achten deze leden een wettelijke basis een verbetering. Dat neemt echter niet weg dat zij nog vragen en opmerkingen hebben. Wij zijn verheugd dat de leden van de PvdA-fractie de formeelwettelijke basis die met dit wetsvoorstel wordt gecreëerd als een verbetering beschouwen. Hun vragen en opmerkingen hopen wij op een voor hen tot tevredenheid stemmende wijze te hebben beantwoord in deze nota. De leden van de ChristenUnie-fractie hebben met belangstelling kennisge- nomen van het wetsvoorstel. Zij constateren dat de pandemie van COVID-19, en de maatregelen die ook in Nederland worden getroffen het virus te bestrijden, tot uitzonderlijke situaties leiden. Veel \u001b[4m\u001b[96mzaken\u001b[96m\u001b[0m die we als samenleving normaal beschouwen, zijn niet mogelijk in het tijdelijk abnormaal van de bestrijding van COVID-19. Deze leden vinden het verdedigbaar maatregelen te treffen teneinde kwetsbare mensen te beschermen en de druk op de zorg te verlichten. Daarbij is het wel van Tweede Kamer, vergaderjaar 2020-2021, 35 526, nr. 23 8 0070 \u001b[1;31m \u001b[0mcorrectq\n",
      "precision = 1.0, recall = 0.10256410256410256, f1 = 0.18604651162790695\n",
      "{'abbr': True, 'lookForMin': True, 'correct': 4, 'partial': 0, 'missing': 35, 'spurious': 0, 'incorrect': 0, 'precision': 1.0, 'recall': 0.10256410256410256, 'f1': 0.18604651162790695}\n",
      "______________________________\n",
      "precision = 0.19383259911894274, recall = 0.9166666666666666, f1 = 0.32\n",
      "{'abbr': True, 'lookForMin': False, 'correct': 44, 'partial': 0, 'missing': 4, 'spurious': 183, 'incorrect': 0, 'precision': 0.19383259911894274, 'recall': 0.9166666666666666, 'f1': 0.32}\n",
      "______________________________\n",
      "precision = 0.5, recall = 0.07894736842105263, f1 = 0.13636363636363635\n",
      "{'abbr': False, 'lookForMin': False, 'correct': 3, 'partial': 0, 'missing': 35, 'spurious': 3, 'incorrect': 0, 'precision': 0.5, 'recall': 0.07894736842105263, 'f1': 0.13636363636363635}\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "results = findMinisteries(minList)\n",
    "\n",
    "for variant in results:\n",
    "    output = calcResultsMinistries(results[variant])\n",
    "    with open('..\\\\data\\\\results\\\\min_old_' + variant + '.json', 'w') as f:\n",
    "        json.dump(output, f)\n",
    "    print('______________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "filled-focus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wvc',\n",
       " 'algemene',\n",
       " 'zaken',\n",
       " 'binnenlandse',\n",
       " 'zaken',\n",
       " 'koninkrijksrelaties',\n",
       " 'buitenlandse',\n",
       " 'zaken',\n",
       " 'defensie',\n",
       " 'economische',\n",
       " 'zaken',\n",
       " 'klimaat',\n",
       " 'financiën',\n",
       " 'infrastructuur',\n",
       " 'waterstaat',\n",
       " 'justitie',\n",
       " 'veiligheid',\n",
       " 'landbouw',\n",
       " 'natuur',\n",
       " 'voedselkwaliteit',\n",
       " 'onderwijs',\n",
       " 'cultuur',\n",
       " 'wetenschap',\n",
       " 'sociale',\n",
       " 'zaken',\n",
       " 'werkgelegenheid',\n",
       " 'volksgezondheid',\n",
       " 'welzijn',\n",
       " 'sport',\n",
       " 'az',\n",
       " 'bzk',\n",
       " 'bz',\n",
       " 'def',\n",
       " 'ez',\n",
       " 'fin',\n",
       " 'i&w',\n",
       " 'j&v',\n",
       " 'lnv',\n",
       " 'ocw',\n",
       " 'szw',\n",
       " 'vws',\n",
       " 'ezk']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-recruitment",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It is easy to find correct matches for ministeries when using gazetteers. The precision of this test is 0.882. Most all of the matches that were found are actually ministeries. However the recall of the matcher is very bad at 0.377. The matcher doesnt even find half of all the ministeries that were in the text. The reason for this is clear. It has to do with summations as abbriviations. \n",
    "\n",
    "Within the texts, a lot of summations of ministeries are used. For example: \"de ministeries van VWSS, JenV, en BZK\". In this case none of these will abbreviations of ministeries will be matched. The matcher looks for \"ministerie van {name of ministerie}\". VWSS will not be matched because ministeries is plural in stead of singular and JenV and BZK will not be matched because these are not preceded by \"ministerie van \". A solution for this could be to not look for the preceding \"ministerie van \", however, if this is done, all of those abbriviations will be matched outside of context. The abbriviation of the ministery of defence for example, is \"def\" so the matcher will find all occurences of the three letters \"def\". With this modification the recall will actually increase to a whopping 0.458 but the precision will decrease to a meager 0.122. So this is not a solution. \n",
    "\n",
    "I also tested if the it would help to at least find the VWSS in the example by also looking at the plural of ministerie but that decreased the precision more than it increased the recall. \n",
    "\n",
    "The last solution is the \"best\". This completely ignores the abbriviations and just looks for the names if the ministeries without the preceding \"ministerie van \". This actually increases the recall to 0.435 but it also decreases the precision to 0.732. This is because it will also find random mentions of the words \"financien\" and \"defensie\" and others. I don't think that that is worth it. In this case I would rather have more confidance in what I what I find to be correct than finding everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-currency",
   "metadata": {},
   "source": [
    "Now for the best extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-leisure",
   "metadata": {},
   "source": [
    "## Labeling new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floppy-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function allows a user to label new data\n",
    "def showTokens(text, allMinisteries):\n",
    "\n",
    "    try:\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "    except:\n",
    "        pass\n",
    "    doc = nlp(str(text))\n",
    "    # get list of entities as strings\n",
    "    listTokens = [str(ent.text) for ent in doc.ents]\n",
    "\n",
    "    # control variables\n",
    "    nToken = 0\n",
    "    tempTokens = []\n",
    "    lenTemp = 0\n",
    "\n",
    "    # loop over tokens\n",
    "    for i in range(len(doc)):\n",
    "\n",
    "        # get string representation of token and add to a temporary list with token index\n",
    "        t = str(doc[i])\n",
    "        tempTokens.append((t, nToken))\n",
    "\n",
    "        # increase n tokens and length of all strings in tempTokens\n",
    "        nToken += 1\n",
    "        lenTemp += (len(t) + 1)\n",
    "\n",
    "        # once lenght of all strings in tempTokens is 100 or larger, start printing the line\n",
    "        if lenTemp >= 100 or i >= len(doc) - 1:\n",
    "\n",
    "            # print the index for every token\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # if index number is more chars than the token, dont print the index number\n",
    "                if len(word[0]) < len(str(word[1])):\n",
    "                    print(' ' * (len(word[0]) + 1), end='')\n",
    "\n",
    "                # else just print index number plus a number of spaces\n",
    "                else:\n",
    "                    print(word[1], ' ' * (len(word[0]) - len(str(word[1]))), end='')\n",
    "            print('')\n",
    "\n",
    "            # print all tokens\n",
    "            for word in tempTokens:\n",
    "\n",
    "                # highlight tokens in red if named entity\n",
    "                if word[0].lower() in allMinisteries + ['ministerie', 'ministeries']:\n",
    "                    printHilight(word[0])\n",
    "                else:\n",
    "                    print(word[0] + ' ', end='')\n",
    "\n",
    "            # add some space between lines\n",
    "            print('\\n\\n')\n",
    "\n",
    "            # reset control variables\n",
    "            tempTokens = []\n",
    "            lenTemp = 0\n",
    "    \n",
    "    # inputs:\n",
    "    # no input: next page\n",
    "    # q: quit \n",
    "    # remove: remove last label\n",
    "    # number1 number2: adds a label from span number1 to number2\n",
    "    # everything else is invalid, you will be prompted again\n",
    "    \n",
    "    \n",
    "    newEnts = []\n",
    "    while True:\n",
    "        x = input()\n",
    "        if x == '':\n",
    "            break\n",
    "        elif x == 'q':\n",
    "            raise Exception(\"Stopped the program\")\n",
    "        elif x == 'remove':\n",
    "            del newEnts[-1]\n",
    "            print('Removed')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            x = x.split()\n",
    "            newEnts.append(Span(doc, int(x[0]), int(x[1]), label='MINISTERIE'))\n",
    "            print(doc[int(x[0]):int(x[1])])\n",
    "        except:\n",
    "            print('Two numbers seperated by a space')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        doc.ents = newEnts\n",
    "        clear_output()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showText(df, n):\n",
    "    \n",
    "    ministeries = getMinisteries()\n",
    "    abrr = [ministeries[x]['Abbriviation'] for x in ministeries if ministeries[x]['Abbriviation'] != None]\n",
    "    allMinisteries = list(ministeries.keys()) + abrr\n",
    "    allMinisteries = [x.lower() for x in allMinisteries]\n",
    "    \n",
    "    samples = df.sample(n)\n",
    "    docs = list(samples.text.apply(lambda x: showTokens(x, allMinisteries)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for i in range(30):\n",
    "    docs += showText(df, 10)\n",
    "    print(f'done {i + 1}0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\train2.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"..\\\\data\\\\spacy labeled\\\\dev2.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floating-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp1 = spacy.load(\"..\\\\data\\\\spacy labeled\\\\output\\\\model-last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-window",
   "metadata": {},
   "source": [
    "## Evaluating new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMatchesMinistries(doc, minList):   \n",
    "    indexOfMatches = []\n",
    "    for ent in doc.ents:\n",
    "        for i in range(int(ent.start), int(ent.end)):\n",
    "            indexOfMatches.append(i)\n",
    "\n",
    "    indexOfMatches = set(indexOfMatches)\n",
    "\n",
    "    for token in doc:\n",
    "        flag = False\n",
    "        for mini in minList:\n",
    "            if token.text.lower() == mini:\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if token.i in indexOfMatches:\n",
    "            printHilight(str(token.text))\n",
    "            \n",
    "        elif flag:\n",
    "            printHilightUnderline(str(token.text))\n",
    "            \n",
    "        else:\n",
    "            print(token, end=' ')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hybrid-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateMinistries(nlp):\n",
    "    # cor = exact match, inc = match is wrong (wrong bounds or label), mis = missing match, spu = found something that\n",
    "    # isnt a mactch, \n",
    "    if os.path.isfile('..\\\\data\\\\results\\\\ministries_results,json'):\n",
    "        with open('..\\\\data\\\\results\\\\ministries_results,json') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        results = {\n",
    "            'correct':0,\n",
    "            'incorrect':0,\n",
    "            'partial':0,\n",
    "            'missing':0,\n",
    "            'spurious':0,\n",
    "            'abbr' :0,\n",
    "            'ocr':0,\n",
    "            'whitespace':0,\n",
    "            'minister':0,\n",
    "            'other':0\n",
    "        }\n",
    "    \n",
    "    minList = set(getMinisteries(True))\n",
    "    while(True):\n",
    "        try:\n",
    "            sample = df.sample(1)\n",
    "            text = sample.text.values[0]\n",
    "            text = re.sub('\\n+', '\\n', text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            for m in minList:\n",
    "                if m in text.split(' '):\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            print(sum(list(results.values())[:4] ))\n",
    "            print(sample.name.values[0], sample.page.values[0], '\\n\\n')\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            showMatchesMinistries(doc, minList)\n",
    "\n",
    "            for case in list(results.keys())[:10]:\n",
    "                result = inputHandling(case)\n",
    "                if result == -1:\n",
    "                    return results\n",
    "                else:\n",
    "                    results[case] += result\n",
    "\n",
    "            clear_output()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "czech-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "306d4b337a5d407eb00c8845884c82fd_rivm-maart-2020-documenten 2674 \n",
      "\n",
      "\n",
      "( instabiliteit , onveiligheid , \u001b[4m\u001b[96meconomische\u001b[96m\u001b[0m verstrengeling met elkaar , toegang tot markten in de toekomst ) moeten we ons ook \n",
      " bezig houden met een internationale aanpak . We moeten dit nu internationaal regelen met het IMF , Wereldbank en de EU , \n",
      " dan zullen we nog heel lang de gevolgen hiervan plukken . \n",
      " * “ Binnen het budget dat ik heb , kijken we naar andere middelen . We kijken ook juist naar hoe het op een Marshall-achtige \n",
      " fund aangepakt kan worden door het IMF . ” \n",
      " « Social distancing is een basisconcept dat in vluchtelingenkampen en favela's wegvalt . Structurele armoede en slecht bestuur \n",
      " is een giftige mengeling . \n",
      " s De cyclus van een aanpak van corona , morgen misschien een ander virus , is ook afhankelijk van de aanpak in de zwakste \n",
      " schakel in de keten . Wervenheid van reizen , werken , denken en mensen , moet je ook aan de zwakste schakel werken . Groot \n",
      " punt van zorg . Ook delen van lessen en ervaringen met ebola is belangrijk . Ergens in de uitzending noemde ze ook het belang \n",
      " van delen van data en informatie . \n",
      " « Wat kan NL doen ? Gaat om primaire gezondheidszorg : moet je denken aan verplegers , gemeenschapsinformatie ( veel \n",
      " goede maatschappelijke organisaties uit NL doen aan bevolkingsvoorlichting - was je handen etc ) en blijven kijken naar andere \n",
      " noden . Groot punt van zorg is dat de hiv-patiënten aan de kant worden geschoven doordat de financiering er alleen maar naar \n",
      " bestrijding van corona gaat . Dus we moeten blijven kijken naar de kwetsbaarheid van de mens en zorg dat de economieën niet \n",
      " alleen draaiende blijven , maar armoede aanpakt . \n",
      " Wie beslist er ? \n",
      " Dan nog korte uitleg over de organen die de adviezen voorbereiden en nemen . Of kijk dit filmpje . ‘ De Lijn ’ volgens het Nationaal \n",
      " Handboek Crisisbesluitvorming is daarin leidend . De rolverdeling is namelijk : deskundigen adviseren , ambtenaren formuleren \n",
      " mogelijke maatregelen en politici besluiten welke genomen worden . Het is een drietrapsraket , zodat artsen niet gaan over beleid en \n",
      " politici niet gaan shoppen in wetenschappelijke gegevens . \n",
      " Volgorde : Outbreak Management Team ( medisch advies ) 3 Interdepartementale Commissie Crisisbeheersing ( maatregelen en \n",
      " beleidsadvies ) > Ministeriële Commissie Crisisbeheersing ( knopen doorhakken ) \n",
      " 1 . Het Outbreak Management Team \n",
      " « Het OMT komt bij elkaar op het moment dat ‘ bestaande draaiboeken te weinig houvast bieden ’ bij de uitbraak van een \n",
      " infectieziekte . De vorige keer dat dat gebeurde , was in 2009 bij de Mexicaanse griep . Vanwege Covid-19 riep \n",
      " de directeur van het Centrum Infectieziektebestrijding van het RIVM , het OMT op 24 januari voor het eerst bij elkaar . \n",
      " « Het overleg is vertrouwelijk , leden mogen niet uit de school klappen . Of er pittige discussies waren , of er leden zijn die \n",
      " twijfelen aan het advies : dat blijft binnenskamers . Hoewel de leden zijn uitgenodigd omdat ze bestuurder of expert zijn bij een \n",
      " medische vereniging of wetenschappelijk instituut , spreken ze op persoonlijke titel . Zo kunnen ze snel beslissen . Alleen het \n",
      " advies wordt openbaar . \n",
      " * Slechts twee van de leden van het team zijn veel in de media te zien : Van Dissel is voorzitter . Secretaris is UJES , hoofd \n",
      " van een belangrijk onderdeel van dat centrum . \n",
      " * Rondom de andere leden is enige geheimzinnigheid . Hun namen ontbreken op openbare stukken . Een van vaste leden is in \n",
      " elk geval viroloog van het Leids Universitair Medisch Centrum , en voorzitter van de Nederlandse Vereniging voor \n",
      " Medische Microbiologie . Zij komt ook op tv : ze schoof al eens aan bij Jinek om uitleg te geven over het virus , en deed mee aan \n",
      " een van de live-uitzendingen van de NOS . Ze doet onder andere onderzoek naar het herpesvirus . \n",
      " * Annelies Verbon , hoogleraar infectieziekten bij Erasmus MC , is lid namens de Nederlandse Vereniging voor Internist- \n",
      " Infectiologen . Zij is onder meer gespecialiseerd in het hiv-virus en het gebruik van antibiotica , en treedt weinig op in de media . \n",
      " * Van andere vaste deelnemers is alleen bekend namens welke organisatie ze aanschuiven . Volgens een woordvoerder van \n",
      " het Nederlands Huisartsen Genootschap is het ‘ vertrouwelijk ’ wie namens hen in het OMT zit . Andere vaste deelnemers \n",
      " komen van het Nederlands Centrum voor Beroepsziekten van het Amsterdam UMC en het Landelijk Overleg \n",
      " Infectieziektebestrijding van de GGD's . \n",
      " * Naast deze vaste leden doen bij iedere vergadering andere specialisten mee , afhankelijk van het onderwerp . Hoe verder de \n",
      " crisis zich ontwikkelt , des te groter wordt de groep , zo blijkt uit de adviezen die het OMT uitbrengt . Bij het vorige overleg zaten \n",
      " naast de vaste leden ook longartsen , intensive-care-artsen , ouderengeneeskundigen , een arts-microbioloog , iemand van een \n",
      " laboratorium en verschillende andere GGD'ers , RIVM-medewerkers en specialisten . Dat overleg vindt nu grotendeels via \n",
      " inbellen plaats . Zo kunnen ook de leden van het OMT voldoen aan het eigen advies : houdt minstens 1,5 meter afstand van \n",
      " elkaar . \n",
      " « Het RIVM krijgt haar info van huisartsenpraktijken en zorgverleners die verplicht zijn om alle coronagevallen te melden in \n",
      " het systeem . \n",
      " 2 . Interdepartementale Commissie Crisisbeheersing \n",
      " 371919 \n",
      " precision = 0.96875, recall = 0.6549295774647887, f1 = 0.7815126050420168\n",
      "{'correct': 178, 'incorrect': 0, 'partial': 16, 'missing': 106, 'spurious': 14, 'abbr': 18, 'ocr': 0, 'whitespace': 5, 'minister': 6, 'other': 91, 'precision': 0.96875, 'recall': 0.6549295774647887, 'f1': 0.7815126050420168}\n"
     ]
    }
   ],
   "source": [
    "results = evaluateMinistries(nlp_ministries)\n",
    "\n",
    "results = calcResultsMinistries(results)\n",
    "\n",
    "with open('..\\\\data\\\\results\\\\ministries_results,json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-access",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Missing things in situations like this: RVO/LNV. LNV should be have been caught here. This might be caused by the fact that spacy works with tokens/words in stead of individual letters. Places with a lot of extra newlines or spaces within the name of a ministry will also trip up the model. No context also doesnt help\n",
    "\n",
    "when training I specificly excluded the minister variants of the ministries. Therefore these aren't counted as missing if the model doesn't find them but will be counted as spurious if the the model does find them.\n",
    "\n",
    "The data the model was trained on means that it easily recoginizes ministries that have a lot to do with covid like volksgezondheid, but less so for others like landbouw. EZK (economics and climate) specifically doesn't want to be captured\n",
    "\n",
    "The type of text doesnt lend itself very well to NER. A lot of times the abbriviations of the ministries are listed without any contex to what it is. NER uses information about the surrounding words to predict what the word is. If there are no surrounding words, no context, predictions are going to be difficult\n",
    "\n",
    "ministerie SZW en EZK was counted as two partially correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-venice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0826c092e7a36555626e841d0508686b74d54b11c575d1ce95f2ca83d0f405ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
